{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d0ae1d3-39ce-4723-aa6a-ad2bf245c345",
   "metadata": {},
   "outputs": [],
   "source": [
    "## similar to github.com/Michaelvll/DeepCCA main\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from linear_gcca import linear_gcca\n",
    "from synth_data import create_synthData,create_synthData_new\n",
    "from torch.utils.data import BatchSampler, SequentialSampler, RandomSampler\n",
    "from models import DeepGCCA\n",
    "# from utils import load_data, svm_classify\n",
    "import time\n",
    "import logging\n",
    "try:\n",
    "    import cPickle as thepickle\n",
    "except ImportError:\n",
    "    import _pickle as thepickle\n",
    "\n",
    "import gzip\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "from main import Solver\n",
    "from loss_objectives import new_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8079b9d6-353d-47c0-8f36-a6fac3503ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 0 GPUs\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# Parameters Section\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "\n",
    "# the path to save the final learned features\n",
    "save_name = './DGCCA.model'\n",
    "\n",
    "# the size of the new space learned by the model (number of the new features)\n",
    "outdim_size = 1\n",
    "\n",
    "# number of layers with nodes in each one\n",
    "layer_sizes1 = [256, 512, 128, outdim_size]\n",
    "layer_sizes2 = [256, 512, 128, outdim_size]\n",
    "layer_sizes3 = [256, 512, 128, outdim_size]\n",
    "layer_sizes_list = [layer_sizes1, layer_sizes2, layer_sizes3] \n",
    "\n",
    "# the parameters for training the network\n",
    "learning_rate = 5*1e-2\n",
    "epoch_num = 150\n",
    "batch_size = 400\n",
    "\n",
    "# the regularization parameter of the network\n",
    "# seems necessary to avoid the gradient exploding especially when non-saturating activations are used\n",
    "reg_par = 1e-5\n",
    "\n",
    "# specifies if all the singular values should get used to calculate the correlation or just the top outdim_size ones\n",
    "# if one option does not work for a network or dataset, try the other one\n",
    "use_all_singular_values = False\n",
    "\n",
    "\n",
    "apply_linear_gcca = True\n",
    "# end of parameters section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a0f4d4f-14a1-489c-acb6-e59a9e1cee26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2023-05-31 15:42:25,254 ] - DeepGCCA(\n",
      "  (model_list): ModuleList(\n",
      "    (0-2): 3 x MlpNet(\n",
      "      (layers): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
      "          (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[ INFO : 2023-05-31 15:42:25,259 ] - Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.05\n",
      "    lr: 0.05\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "[ INFO : 2023-05-31 15:42:25,328 ] - Epoch 1/150 - time: 0.06 - training_loss: -2.7923\n",
      "[ INFO : 2023-05-31 15:42:25,386 ] - Epoch 2/150 - time: 0.06 - training_loss: -2.5237\n",
      "[ INFO : 2023-05-31 15:42:25,443 ] - Epoch 3/150 - time: 0.06 - training_loss: -3.6121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input views shape :\n",
      "view_0 :  torch.Size([400, 20])\n",
      "view_1 :  torch.Size([400, 20])\n",
      "view_2 :  torch.Size([400, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2023-05-31 15:42:25,503 ] - Epoch 4/150 - time: 0.06 - training_loss: -4.6827\n",
      "[ INFO : 2023-05-31 15:42:25,566 ] - Epoch 5/150 - time: 0.06 - training_loss: -5.3919\n",
      "[ INFO : 2023-05-31 15:42:25,625 ] - Epoch 6/150 - time: 0.06 - training_loss: -5.8859\n",
      "[ INFO : 2023-05-31 15:42:25,686 ] - Epoch 7/150 - time: 0.06 - training_loss: -6.2652\n",
      "[ INFO : 2023-05-31 15:42:25,748 ] - Epoch 8/150 - time: 0.06 - training_loss: -6.5596\n",
      "[ INFO : 2023-05-31 15:42:25,809 ] - Epoch 9/150 - time: 0.06 - training_loss: -6.7904\n",
      "[ INFO : 2023-05-31 15:42:25,868 ] - Epoch 10/150 - time: 0.06 - training_loss: -6.9782\n",
      "[ INFO : 2023-05-31 15:42:25,926 ] - Epoch 11/150 - time: 0.06 - training_loss: -7.1354\n",
      "[ INFO : 2023-05-31 15:42:25,985 ] - Epoch 12/150 - time: 0.06 - training_loss: -7.2686\n",
      "[ INFO : 2023-05-31 15:42:26,044 ] - Epoch 13/150 - time: 0.06 - training_loss: -7.3825\n",
      "[ INFO : 2023-05-31 15:42:26,103 ] - Epoch 14/150 - time: 0.06 - training_loss: -7.4809\n",
      "[ INFO : 2023-05-31 15:42:26,164 ] - Epoch 15/150 - time: 0.06 - training_loss: -7.5672\n",
      "[ INFO : 2023-05-31 15:42:26,226 ] - Epoch 16/150 - time: 0.06 - training_loss: -7.6436\n",
      "[ INFO : 2023-05-31 15:42:26,288 ] - Epoch 17/150 - time: 0.06 - training_loss: -7.7118\n",
      "[ INFO : 2023-05-31 15:42:26,352 ] - Epoch 18/150 - time: 0.06 - training_loss: -7.7729\n",
      "[ INFO : 2023-05-31 15:42:26,414 ] - Epoch 19/150 - time: 0.06 - training_loss: -7.8279\n",
      "[ INFO : 2023-05-31 15:42:26,476 ] - Epoch 20/150 - time: 0.06 - training_loss: -7.8776\n",
      "[ INFO : 2023-05-31 15:42:26,537 ] - Epoch 21/150 - time: 0.06 - training_loss: -7.9227\n",
      "[ INFO : 2023-05-31 15:42:26,597 ] - Epoch 22/150 - time: 0.06 - training_loss: -7.9638\n",
      "[ INFO : 2023-05-31 15:42:26,657 ] - Epoch 23/150 - time: 0.06 - training_loss: -8.0014\n",
      "[ INFO : 2023-05-31 15:42:26,719 ] - Epoch 24/150 - time: 0.06 - training_loss: -8.0359\n",
      "[ INFO : 2023-05-31 15:42:26,780 ] - Epoch 25/150 - time: 0.06 - training_loss: -8.0678\n",
      "[ INFO : 2023-05-31 15:42:26,842 ] - Epoch 26/150 - time: 0.06 - training_loss: -8.0973\n",
      "[ INFO : 2023-05-31 15:42:26,900 ] - Epoch 27/150 - time: 0.06 - training_loss: -8.1248\n",
      "[ INFO : 2023-05-31 15:42:26,958 ] - Epoch 28/150 - time: 0.06 - training_loss: -8.1504\n",
      "[ INFO : 2023-05-31 15:42:27,015 ] - Epoch 29/150 - time: 0.06 - training_loss: -8.1744\n",
      "[ INFO : 2023-05-31 15:42:27,075 ] - Epoch 30/150 - time: 0.06 - training_loss: -8.1970\n",
      "[ INFO : 2023-05-31 15:42:27,133 ] - Epoch 31/150 - time: 0.06 - training_loss: -8.2181\n",
      "[ INFO : 2023-05-31 15:42:27,195 ] - Epoch 32/150 - time: 0.06 - training_loss: -8.2379\n",
      "[ INFO : 2023-05-31 15:42:27,256 ] - Epoch 33/150 - time: 0.06 - training_loss: -8.2566\n",
      "[ INFO : 2023-05-31 15:42:27,316 ] - Epoch 34/150 - time: 0.06 - training_loss: -8.2741\n",
      "[ INFO : 2023-05-31 15:42:27,375 ] - Epoch 35/150 - time: 0.06 - training_loss: -8.2906\n",
      "[ INFO : 2023-05-31 15:42:27,437 ] - Epoch 36/150 - time: 0.06 - training_loss: -8.3061\n",
      "[ INFO : 2023-05-31 15:42:27,496 ] - Epoch 37/150 - time: 0.06 - training_loss: -8.3209\n",
      "[ INFO : 2023-05-31 15:42:27,559 ] - Epoch 38/150 - time: 0.06 - training_loss: -8.3349\n",
      "[ INFO : 2023-05-31 15:42:27,619 ] - Epoch 39/150 - time: 0.06 - training_loss: -8.3482\n",
      "[ INFO : 2023-05-31 15:42:27,679 ] - Epoch 40/150 - time: 0.06 - training_loss: -8.3609\n",
      "[ INFO : 2023-05-31 15:42:27,739 ] - Epoch 41/150 - time: 0.06 - training_loss: -8.3730\n",
      "[ INFO : 2023-05-31 15:42:27,802 ] - Epoch 42/150 - time: 0.06 - training_loss: -8.3846\n",
      "[ INFO : 2023-05-31 15:42:27,862 ] - Epoch 43/150 - time: 0.06 - training_loss: -8.3956\n",
      "[ INFO : 2023-05-31 15:42:27,921 ] - Epoch 44/150 - time: 0.06 - training_loss: -8.4061\n",
      "[ INFO : 2023-05-31 15:42:27,979 ] - Epoch 45/150 - time: 0.06 - training_loss: -8.4161\n",
      "[ INFO : 2023-05-31 15:42:28,037 ] - Epoch 46/150 - time: 0.06 - training_loss: -8.4257\n",
      "[ INFO : 2023-05-31 15:42:28,096 ] - Epoch 47/150 - time: 0.06 - training_loss: -8.4349\n",
      "[ INFO : 2023-05-31 15:42:28,155 ] - Epoch 48/150 - time: 0.06 - training_loss: -8.4437\n",
      "[ INFO : 2023-05-31 15:42:28,216 ] - Epoch 49/150 - time: 0.06 - training_loss: -8.4522\n",
      "[ INFO : 2023-05-31 15:42:28,278 ] - Epoch 50/150 - time: 0.06 - training_loss: -8.4603\n",
      "[ INFO : 2023-05-31 15:42:28,339 ] - Epoch 51/150 - time: 0.06 - training_loss: -8.4681\n",
      "[ INFO : 2023-05-31 15:42:28,399 ] - Epoch 52/150 - time: 0.06 - training_loss: -8.4756\n",
      "[ INFO : 2023-05-31 15:42:28,459 ] - Epoch 53/150 - time: 0.06 - training_loss: -8.4829\n",
      "[ INFO : 2023-05-31 15:42:28,522 ] - Epoch 54/150 - time: 0.06 - training_loss: -8.4899\n",
      "[ INFO : 2023-05-31 15:42:28,583 ] - Epoch 55/150 - time: 0.06 - training_loss: -8.4966\n",
      "[ INFO : 2023-05-31 15:42:28,645 ] - Epoch 56/150 - time: 0.06 - training_loss: -8.5031\n",
      "[ INFO : 2023-05-31 15:42:28,705 ] - Epoch 57/150 - time: 0.06 - training_loss: -8.5093\n",
      "[ INFO : 2023-05-31 15:42:28,765 ] - Epoch 58/150 - time: 0.06 - training_loss: -8.5154\n",
      "[ INFO : 2023-05-31 15:42:28,827 ] - Epoch 59/150 - time: 0.06 - training_loss: -8.5212\n",
      "[ INFO : 2023-05-31 15:42:28,887 ] - Epoch 60/150 - time: 0.06 - training_loss: -8.5268\n",
      "[ INFO : 2023-05-31 15:42:28,945 ] - Epoch 61/150 - time: 0.06 - training_loss: -8.5323\n",
      "[ INFO : 2023-05-31 15:42:29,004 ] - Epoch 62/150 - time: 0.06 - training_loss: -8.5376\n",
      "[ INFO : 2023-05-31 15:42:29,063 ] - Epoch 63/150 - time: 0.06 - training_loss: -8.5427\n",
      "[ INFO : 2023-05-31 15:42:29,121 ] - Epoch 64/150 - time: 0.06 - training_loss: -8.5476\n",
      "[ INFO : 2023-05-31 15:42:29,179 ] - Epoch 65/150 - time: 0.06 - training_loss: -8.5525\n",
      "[ INFO : 2023-05-31 15:42:29,242 ] - Epoch 66/150 - time: 0.06 - training_loss: -8.5571\n",
      "[ INFO : 2023-05-31 15:42:29,301 ] - Epoch 67/150 - time: 0.06 - training_loss: -8.5616\n",
      "[ INFO : 2023-05-31 15:42:29,363 ] - Epoch 68/150 - time: 0.06 - training_loss: -8.5660\n",
      "[ INFO : 2023-05-31 15:42:29,423 ] - Epoch 69/150 - time: 0.06 - training_loss: -8.5703\n",
      "[ INFO : 2023-05-31 15:42:29,483 ] - Epoch 70/150 - time: 0.06 - training_loss: -8.5744\n",
      "[ INFO : 2023-05-31 15:42:29,542 ] - Epoch 71/150 - time: 0.06 - training_loss: -8.5785\n",
      "[ INFO : 2023-05-31 15:42:29,604 ] - Epoch 72/150 - time: 0.06 - training_loss: -8.5824\n",
      "[ INFO : 2023-05-31 15:42:29,667 ] - Epoch 73/150 - time: 0.06 - training_loss: -8.5862\n",
      "[ INFO : 2023-05-31 15:42:29,729 ] - Epoch 74/150 - time: 0.06 - training_loss: -8.5899\n",
      "[ INFO : 2023-05-31 15:42:29,789 ] - Epoch 75/150 - time: 0.06 - training_loss: -8.5935\n",
      "[ INFO : 2023-05-31 15:42:29,848 ] - Epoch 76/150 - time: 0.06 - training_loss: -8.5970\n",
      "[ INFO : 2023-05-31 15:42:29,907 ] - Epoch 77/150 - time: 0.06 - training_loss: -8.6004\n",
      "[ INFO : 2023-05-31 15:42:29,970 ] - Epoch 78/150 - time: 0.06 - training_loss: -8.6037\n",
      "[ INFO : 2023-05-31 15:42:30,029 ] - Epoch 79/150 - time: 0.06 - training_loss: -8.6070\n",
      "[ INFO : 2023-05-31 15:42:30,090 ] - Epoch 80/150 - time: 0.06 - training_loss: -8.6102\n",
      "[ INFO : 2023-05-31 15:42:30,161 ] - Epoch 81/150 - time: 0.07 - training_loss: -8.6133\n",
      "[ INFO : 2023-05-31 15:42:30,220 ] - Epoch 82/150 - time: 0.06 - training_loss: -8.6163\n",
      "[ INFO : 2023-05-31 15:42:30,284 ] - Epoch 83/150 - time: 0.06 - training_loss: -8.6192\n",
      "[ INFO : 2023-05-31 15:42:30,346 ] - Epoch 84/150 - time: 0.06 - training_loss: -8.6221\n",
      "[ INFO : 2023-05-31 15:42:30,411 ] - Epoch 85/150 - time: 0.06 - training_loss: -8.6249\n",
      "[ INFO : 2023-05-31 15:42:30,474 ] - Epoch 86/150 - time: 0.06 - training_loss: -8.6276\n",
      "[ INFO : 2023-05-31 15:42:30,544 ] - Epoch 87/150 - time: 0.07 - training_loss: -8.6303\n",
      "[ INFO : 2023-05-31 15:42:30,605 ] - Epoch 88/150 - time: 0.06 - training_loss: -8.6329\n",
      "[ INFO : 2023-05-31 15:42:30,664 ] - Epoch 89/150 - time: 0.06 - training_loss: -8.6355\n",
      "[ INFO : 2023-05-31 15:42:30,725 ] - Epoch 90/150 - time: 0.06 - training_loss: -8.6380\n",
      "[ INFO : 2023-05-31 15:42:30,786 ] - Epoch 91/150 - time: 0.06 - training_loss: -8.6404\n",
      "[ INFO : 2023-05-31 15:42:30,848 ] - Epoch 92/150 - time: 0.06 - training_loss: -8.6428\n",
      "[ INFO : 2023-05-31 15:42:30,906 ] - Epoch 93/150 - time: 0.06 - training_loss: -8.6451\n",
      "[ INFO : 2023-05-31 15:42:30,964 ] - Epoch 94/150 - time: 0.06 - training_loss: -8.6474\n",
      "[ INFO : 2023-05-31 15:42:31,021 ] - Epoch 95/150 - time: 0.06 - training_loss: -8.6497\n",
      "[ INFO : 2023-05-31 15:42:31,080 ] - Epoch 96/150 - time: 0.06 - training_loss: -8.6519\n",
      "[ INFO : 2023-05-31 15:42:31,137 ] - Epoch 97/150 - time: 0.06 - training_loss: -8.6540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2023-05-31 15:42:31,198 ] - Epoch 98/150 - time: 0.06 - training_loss: -8.6561\n",
      "[ INFO : 2023-05-31 15:42:31,257 ] - Epoch 99/150 - time: 0.06 - training_loss: -8.6582\n",
      "[ INFO : 2023-05-31 15:42:31,318 ] - Epoch 100/150 - time: 0.06 - training_loss: -8.6602\n",
      "[ INFO : 2023-05-31 15:42:31,376 ] - Epoch 101/150 - time: 0.06 - training_loss: -8.6622\n",
      "[ INFO : 2023-05-31 15:42:31,438 ] - Epoch 102/150 - time: 0.06 - training_loss: -8.6641\n",
      "[ INFO : 2023-05-31 15:42:31,499 ] - Epoch 103/150 - time: 0.06 - training_loss: -8.6660\n",
      "[ INFO : 2023-05-31 15:42:31,561 ] - Epoch 104/150 - time: 0.06 - training_loss: -8.6679\n",
      "[ INFO : 2023-05-31 15:42:31,621 ] - Epoch 105/150 - time: 0.06 - training_loss: -8.6697\n",
      "[ INFO : 2023-05-31 15:42:31,682 ] - Epoch 106/150 - time: 0.06 - training_loss: -8.6715\n",
      "[ INFO : 2023-05-31 15:42:31,742 ] - Epoch 107/150 - time: 0.06 - training_loss: -8.6733\n",
      "[ INFO : 2023-05-31 15:42:31,804 ] - Epoch 108/150 - time: 0.06 - training_loss: -8.6750\n",
      "[ INFO : 2023-05-31 15:42:31,864 ] - Epoch 109/150 - time: 0.06 - training_loss: -8.6767\n",
      "[ INFO : 2023-05-31 15:42:31,923 ] - Epoch 110/150 - time: 0.06 - training_loss: -8.6784\n",
      "[ INFO : 2023-05-31 15:42:31,981 ] - Epoch 111/150 - time: 0.06 - training_loss: -8.6800\n",
      "[ INFO : 2023-05-31 15:42:32,038 ] - Epoch 112/150 - time: 0.06 - training_loss: -8.6817\n",
      "[ INFO : 2023-05-31 15:42:32,096 ] - Epoch 113/150 - time: 0.06 - training_loss: -8.6832\n",
      "[ INFO : 2023-05-31 15:42:32,156 ] - Epoch 114/150 - time: 0.06 - training_loss: -8.6848\n",
      "[ INFO : 2023-05-31 15:42:32,215 ] - Epoch 115/150 - time: 0.06 - training_loss: -8.6863\n",
      "[ INFO : 2023-05-31 15:42:32,278 ] - Epoch 116/150 - time: 0.06 - training_loss: -8.6878\n",
      "[ INFO : 2023-05-31 15:42:32,339 ] - Epoch 117/150 - time: 0.06 - training_loss: -8.6893\n",
      "[ INFO : 2023-05-31 15:42:32,400 ] - Epoch 118/150 - time: 0.06 - training_loss: -8.6907\n",
      "[ INFO : 2023-05-31 15:42:32,461 ] - Epoch 119/150 - time: 0.06 - training_loss: -8.6922\n",
      "[ INFO : 2023-05-31 15:42:32,523 ] - Epoch 120/150 - time: 0.06 - training_loss: -8.6936\n",
      "[ INFO : 2023-05-31 15:42:32,584 ] - Epoch 121/150 - time: 0.06 - training_loss: -8.6950\n",
      "[ INFO : 2023-05-31 15:42:32,647 ] - Epoch 122/150 - time: 0.06 - training_loss: -8.6963\n",
      "[ INFO : 2023-05-31 15:42:32,706 ] - Epoch 123/150 - time: 0.06 - training_loss: -8.6976\n",
      "[ INFO : 2023-05-31 15:42:32,766 ] - Epoch 124/150 - time: 0.06 - training_loss: -8.6990\n",
      "[ INFO : 2023-05-31 15:42:32,826 ] - Epoch 125/150 - time: 0.06 - training_loss: -8.7002\n",
      "[ INFO : 2023-05-31 15:42:32,887 ] - Epoch 126/150 - time: 0.06 - training_loss: -8.7015\n",
      "[ INFO : 2023-05-31 15:42:32,944 ] - Epoch 127/150 - time: 0.06 - training_loss: -8.7028\n",
      "[ INFO : 2023-05-31 15:42:33,003 ] - Epoch 128/150 - time: 0.06 - training_loss: -8.7040\n",
      "[ INFO : 2023-05-31 15:42:33,060 ] - Epoch 129/150 - time: 0.06 - training_loss: -8.7052\n",
      "[ INFO : 2023-05-31 15:42:33,120 ] - Epoch 130/150 - time: 0.06 - training_loss: -8.7064\n",
      "[ INFO : 2023-05-31 15:42:33,179 ] - Epoch 131/150 - time: 0.06 - training_loss: -8.7076\n",
      "[ INFO : 2023-05-31 15:42:33,240 ] - Epoch 132/150 - time: 0.06 - training_loss: -8.7087\n",
      "[ INFO : 2023-05-31 15:42:33,301 ] - Epoch 133/150 - time: 0.06 - training_loss: -8.7099\n",
      "[ INFO : 2023-05-31 15:42:33,364 ] - Epoch 134/150 - time: 0.06 - training_loss: -8.7110\n",
      "[ INFO : 2023-05-31 15:42:33,426 ] - Epoch 135/150 - time: 0.06 - training_loss: -8.7121\n",
      "[ INFO : 2023-05-31 15:42:33,485 ] - Epoch 136/150 - time: 0.06 - training_loss: -8.7132\n",
      "[ INFO : 2023-05-31 15:42:33,544 ] - Epoch 137/150 - time: 0.06 - training_loss: -8.7143\n",
      "[ INFO : 2023-05-31 15:42:33,607 ] - Epoch 138/150 - time: 0.06 - training_loss: -8.7153\n",
      "[ INFO : 2023-05-31 15:42:33,668 ] - Epoch 139/150 - time: 0.06 - training_loss: -8.7164\n",
      "[ INFO : 2023-05-31 15:42:33,730 ] - Epoch 140/150 - time: 0.06 - training_loss: -8.7174\n",
      "[ INFO : 2023-05-31 15:42:33,790 ] - Epoch 141/150 - time: 0.06 - training_loss: -8.7184\n",
      "[ INFO : 2023-05-31 15:42:33,851 ] - Epoch 142/150 - time: 0.06 - training_loss: -8.7194\n",
      "[ INFO : 2023-05-31 15:42:33,909 ] - Epoch 143/150 - time: 0.06 - training_loss: -8.7204\n",
      "[ INFO : 2023-05-31 15:42:33,968 ] - Epoch 144/150 - time: 0.06 - training_loss: -8.7214\n",
      "[ INFO : 2023-05-31 15:42:34,025 ] - Epoch 145/150 - time: 0.06 - training_loss: -8.7223\n",
      "[ INFO : 2023-05-31 15:42:34,085 ] - Epoch 146/150 - time: 0.06 - training_loss: -8.7233\n",
      "[ INFO : 2023-05-31 15:42:34,144 ] - Epoch 147/150 - time: 0.06 - training_loss: -8.7242\n",
      "[ INFO : 2023-05-31 15:42:34,203 ] - Epoch 148/150 - time: 0.06 - training_loss: -8.7251\n",
      "[ INFO : 2023-05-31 15:42:34,264 ] - Epoch 149/150 - time: 0.06 - training_loss: -8.7260\n",
      "[ INFO : 2023-05-31 15:42:34,324 ] - Epoch 150/150 - time: 0.06 - training_loss: -8.7269\n",
      "[ INFO : 2023-05-31 15:42:34,369 ] - DeepGCCA(\n",
      "  (model_list): ModuleList(\n",
      "    (0-2): 3 x MlpNet(\n",
      "      (layers): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
      "          (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[ INFO : 2023-05-31 15:42:34,374 ] - Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.05\n",
      "    lr: 0.05\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "[ INFO : 2023-05-31 15:42:34,440 ] - Epoch 1/150 - time: 0.06 - training_loss: -2.9535\n",
      "[ INFO : 2023-05-31 15:42:34,499 ] - Epoch 2/150 - time: 0.06 - training_loss: -2.4255\n",
      "[ INFO : 2023-05-31 15:42:34,558 ] - Epoch 3/150 - time: 0.06 - training_loss: -3.6312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear CCA started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2023-05-31 15:42:34,621 ] - Epoch 4/150 - time: 0.06 - training_loss: -4.5478\n",
      "[ INFO : 2023-05-31 15:42:34,680 ] - Epoch 5/150 - time: 0.06 - training_loss: -5.3271\n",
      "[ INFO : 2023-05-31 15:42:34,740 ] - Epoch 6/150 - time: 0.06 - training_loss: -5.8537\n",
      "[ INFO : 2023-05-31 15:42:34,802 ] - Epoch 7/150 - time: 0.06 - training_loss: -6.2339\n",
      "[ INFO : 2023-05-31 15:42:34,861 ] - Epoch 8/150 - time: 0.06 - training_loss: -6.5285\n",
      "[ INFO : 2023-05-31 15:42:34,919 ] - Epoch 9/150 - time: 0.06 - training_loss: -6.7631\n",
      "[ INFO : 2023-05-31 15:42:34,978 ] - Epoch 10/150 - time: 0.06 - training_loss: -6.9545\n",
      "[ INFO : 2023-05-31 15:42:35,035 ] - Epoch 11/150 - time: 0.06 - training_loss: -7.1154\n",
      "[ INFO : 2023-05-31 15:42:35,093 ] - Epoch 12/150 - time: 0.06 - training_loss: -7.2532\n",
      "[ INFO : 2023-05-31 15:42:35,153 ] - Epoch 13/150 - time: 0.06 - training_loss: -7.3718\n",
      "[ INFO : 2023-05-31 15:42:35,213 ] - Epoch 14/150 - time: 0.06 - training_loss: -7.4739\n",
      "[ INFO : 2023-05-31 15:42:35,279 ] - Epoch 15/150 - time: 0.06 - training_loss: -7.5623\n",
      "[ INFO : 2023-05-31 15:42:35,340 ] - Epoch 16/150 - time: 0.06 - training_loss: -7.6396\n",
      "[ INFO : 2023-05-31 15:42:35,400 ] - Epoch 17/150 - time: 0.06 - training_loss: -7.7080\n",
      "[ INFO : 2023-05-31 15:42:35,459 ] - Epoch 18/150 - time: 0.06 - training_loss: -7.7693\n",
      "[ INFO : 2023-05-31 15:42:35,521 ] - Epoch 19/150 - time: 0.06 - training_loss: -7.8246\n",
      "[ INFO : 2023-05-31 15:42:35,581 ] - Epoch 20/150 - time: 0.06 - training_loss: -7.8747\n",
      "[ INFO : 2023-05-31 15:42:35,640 ] - Epoch 21/150 - time: 0.06 - training_loss: -7.9203\n",
      "[ INFO : 2023-05-31 15:42:35,703 ] - Epoch 22/150 - time: 0.06 - training_loss: -7.9618\n",
      "[ INFO : 2023-05-31 15:42:35,761 ] - Epoch 23/150 - time: 0.06 - training_loss: -7.9998\n",
      "[ INFO : 2023-05-31 15:42:35,821 ] - Epoch 24/150 - time: 0.06 - training_loss: -8.0346\n",
      "[ INFO : 2023-05-31 15:42:35,882 ] - Epoch 25/150 - time: 0.06 - training_loss: -8.0668\n",
      "[ INFO : 2023-05-31 15:42:35,939 ] - Epoch 26/150 - time: 0.06 - training_loss: -8.0966\n",
      "[ INFO : 2023-05-31 15:42:35,998 ] - Epoch 27/150 - time: 0.06 - training_loss: -8.1243\n",
      "[ INFO : 2023-05-31 15:42:36,056 ] - Epoch 28/150 - time: 0.06 - training_loss: -8.1501\n",
      "[ INFO : 2023-05-31 15:42:36,114 ] - Epoch 29/150 - time: 0.06 - training_loss: -8.1743\n",
      "[ INFO : 2023-05-31 15:42:36,173 ] - Epoch 30/150 - time: 0.06 - training_loss: -8.1969\n",
      "[ INFO : 2023-05-31 15:42:36,235 ] - Epoch 31/150 - time: 0.06 - training_loss: -8.2180\n",
      "[ INFO : 2023-05-31 15:42:36,294 ] - Epoch 32/150 - time: 0.06 - training_loss: -8.2378\n",
      "[ INFO : 2023-05-31 15:42:36,354 ] - Epoch 33/150 - time: 0.06 - training_loss: -8.2565\n",
      "[ INFO : 2023-05-31 15:42:36,415 ] - Epoch 34/150 - time: 0.06 - training_loss: -8.2740\n",
      "[ INFO : 2023-05-31 15:42:36,476 ] - Epoch 35/150 - time: 0.06 - training_loss: -8.2905\n",
      "[ INFO : 2023-05-31 15:42:36,535 ] - Epoch 36/150 - time: 0.06 - training_loss: -8.3062\n",
      "[ INFO : 2023-05-31 15:42:36,597 ] - Epoch 37/150 - time: 0.06 - training_loss: -8.3210\n",
      "[ INFO : 2023-05-31 15:42:36,657 ] - Epoch 38/150 - time: 0.06 - training_loss: -8.3351\n",
      "[ INFO : 2023-05-31 15:42:36,717 ] - Epoch 39/150 - time: 0.06 - training_loss: -8.3484\n",
      "[ INFO : 2023-05-31 15:42:36,778 ] - Epoch 40/150 - time: 0.06 - training_loss: -8.3611\n",
      "[ INFO : 2023-05-31 15:42:36,839 ] - Epoch 41/150 - time: 0.06 - training_loss: -8.3732\n",
      "[ INFO : 2023-05-31 15:42:36,896 ] - Epoch 42/150 - time: 0.06 - training_loss: -8.3848\n",
      "[ INFO : 2023-05-31 15:42:36,955 ] - Epoch 43/150 - time: 0.06 - training_loss: -8.3958\n",
      "[ INFO : 2023-05-31 15:42:37,013 ] - Epoch 44/150 - time: 0.06 - training_loss: -8.4063\n",
      "[ INFO : 2023-05-31 15:42:37,070 ] - Epoch 45/150 - time: 0.06 - training_loss: -8.4163\n",
      "[ INFO : 2023-05-31 15:42:37,129 ] - Epoch 46/150 - time: 0.06 - training_loss: -8.4259\n",
      "[ INFO : 2023-05-31 15:42:37,187 ] - Epoch 47/150 - time: 0.06 - training_loss: -8.4351\n",
      "[ INFO : 2023-05-31 15:42:37,246 ] - Epoch 48/150 - time: 0.06 - training_loss: -8.4440\n",
      "[ INFO : 2023-05-31 15:42:37,309 ] - Epoch 49/150 - time: 0.06 - training_loss: -8.4524\n",
      "[ INFO : 2023-05-31 15:42:37,367 ] - Epoch 50/150 - time: 0.06 - training_loss: -8.4606\n",
      "[ INFO : 2023-05-31 15:42:37,426 ] - Epoch 51/150 - time: 0.06 - training_loss: -8.4684\n",
      "[ INFO : 2023-05-31 15:42:37,489 ] - Epoch 52/150 - time: 0.06 - training_loss: -8.4759\n",
      "[ INFO : 2023-05-31 15:42:37,548 ] - Epoch 53/150 - time: 0.06 - training_loss: -8.4831\n",
      "[ INFO : 2023-05-31 15:42:37,619 ] - Epoch 54/150 - time: 0.07 - training_loss: -8.4901\n",
      "[ INFO : 2023-05-31 15:42:37,681 ] - Epoch 55/150 - time: 0.06 - training_loss: -8.4968\n",
      "[ INFO : 2023-05-31 15:42:37,740 ] - Epoch 56/150 - time: 0.06 - training_loss: -8.5033\n",
      "[ INFO : 2023-05-31 15:42:37,799 ] - Epoch 57/150 - time: 0.06 - training_loss: -8.5095\n",
      "[ INFO : 2023-05-31 15:42:37,861 ] - Epoch 58/150 - time: 0.06 - training_loss: -8.5156\n",
      "[ INFO : 2023-05-31 15:42:37,919 ] - Epoch 59/150 - time: 0.06 - training_loss: -8.5214\n",
      "[ INFO : 2023-05-31 15:42:37,977 ] - Epoch 60/150 - time: 0.06 - training_loss: -8.5271\n",
      "[ INFO : 2023-05-31 15:42:38,036 ] - Epoch 61/150 - time: 0.06 - training_loss: -8.5325\n",
      "[ INFO : 2023-05-31 15:42:38,094 ] - Epoch 62/150 - time: 0.06 - training_loss: -8.5378\n",
      "[ INFO : 2023-05-31 15:42:38,150 ] - Epoch 63/150 - time: 0.05 - training_loss: -8.5429\n",
      "[ INFO : 2023-05-31 15:42:38,213 ] - Epoch 64/150 - time: 0.06 - training_loss: -8.5479\n",
      "[ INFO : 2023-05-31 15:42:38,272 ] - Epoch 65/150 - time: 0.06 - training_loss: -8.5527\n",
      "[ INFO : 2023-05-31 15:42:38,331 ] - Epoch 66/150 - time: 0.06 - training_loss: -8.5573\n",
      "[ INFO : 2023-05-31 15:42:38,394 ] - Epoch 67/150 - time: 0.06 - training_loss: -8.5619\n",
      "[ INFO : 2023-05-31 15:42:38,453 ] - Epoch 68/150 - time: 0.06 - training_loss: -8.5662\n",
      "[ INFO : 2023-05-31 15:42:38,512 ] - Epoch 69/150 - time: 0.06 - training_loss: -8.5705\n",
      "[ INFO : 2023-05-31 15:42:38,574 ] - Epoch 70/150 - time: 0.06 - training_loss: -8.5746\n",
      "[ INFO : 2023-05-31 15:42:38,652 ] - Epoch 71/150 - time: 0.08 - training_loss: -8.5787\n",
      "[ INFO : 2023-05-31 15:42:38,712 ] - Epoch 72/150 - time: 0.06 - training_loss: -8.5826\n",
      "[ INFO : 2023-05-31 15:42:38,774 ] - Epoch 73/150 - time: 0.06 - training_loss: -8.5864\n",
      "[ INFO : 2023-05-31 15:42:38,833 ] - Epoch 74/150 - time: 0.06 - training_loss: -8.5901\n",
      "[ INFO : 2023-05-31 15:42:38,892 ] - Epoch 75/150 - time: 0.06 - training_loss: -8.5937\n",
      "[ INFO : 2023-05-31 15:42:38,951 ] - Epoch 76/150 - time: 0.06 - training_loss: -8.5972\n",
      "[ INFO : 2023-05-31 15:42:39,008 ] - Epoch 77/150 - time: 0.06 - training_loss: -8.6006\n",
      "[ INFO : 2023-05-31 15:42:39,065 ] - Epoch 78/150 - time: 0.06 - training_loss: -8.6039\n",
      "[ INFO : 2023-05-31 15:42:39,125 ] - Epoch 79/150 - time: 0.06 - training_loss: -8.6072\n",
      "[ INFO : 2023-05-31 15:42:39,184 ] - Epoch 80/150 - time: 0.06 - training_loss: -8.6103\n",
      "[ INFO : 2023-05-31 15:42:39,241 ] - Epoch 81/150 - time: 0.06 - training_loss: -8.6134\n",
      "[ INFO : 2023-05-31 15:42:39,303 ] - Epoch 82/150 - time: 0.06 - training_loss: -8.6164\n",
      "[ INFO : 2023-05-31 15:42:39,363 ] - Epoch 83/150 - time: 0.06 - training_loss: -8.6194\n",
      "[ INFO : 2023-05-31 15:42:39,421 ] - Epoch 84/150 - time: 0.06 - training_loss: -8.6223\n",
      "[ INFO : 2023-05-31 15:42:39,484 ] - Epoch 85/150 - time: 0.06 - training_loss: -8.6251\n",
      "[ INFO : 2023-05-31 15:42:39,543 ] - Epoch 86/150 - time: 0.06 - training_loss: -8.6278\n",
      "[ INFO : 2023-05-31 15:42:39,602 ] - Epoch 87/150 - time: 0.06 - training_loss: -8.6305\n",
      "[ INFO : 2023-05-31 15:42:39,665 ] - Epoch 88/150 - time: 0.06 - training_loss: -8.6331\n",
      "[ INFO : 2023-05-31 15:42:39,723 ] - Epoch 89/150 - time: 0.06 - training_loss: -8.6356\n",
      "[ INFO : 2023-05-31 15:42:39,783 ] - Epoch 90/150 - time: 0.06 - training_loss: -8.6381\n",
      "[ INFO : 2023-05-31 15:42:39,847 ] - Epoch 91/150 - time: 0.06 - training_loss: -8.6406\n",
      "[ INFO : 2023-05-31 15:42:39,908 ] - Epoch 92/150 - time: 0.06 - training_loss: -8.6430\n",
      "[ INFO : 2023-05-31 15:42:39,967 ] - Epoch 93/150 - time: 0.06 - training_loss: -8.6453\n",
      "[ INFO : 2023-05-31 15:42:40,026 ] - Epoch 94/150 - time: 0.06 - training_loss: -8.6476\n",
      "[ INFO : 2023-05-31 15:42:40,082 ] - Epoch 95/150 - time: 0.05 - training_loss: -8.6498\n",
      "[ INFO : 2023-05-31 15:42:40,141 ] - Epoch 96/150 - time: 0.06 - training_loss: -8.6520\n",
      "[ INFO : 2023-05-31 15:42:40,201 ] - Epoch 97/150 - time: 0.06 - training_loss: -8.6542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2023-05-31 15:42:40,259 ] - Epoch 98/150 - time: 0.06 - training_loss: -8.6563\n",
      "[ INFO : 2023-05-31 15:42:40,318 ] - Epoch 99/150 - time: 0.06 - training_loss: -8.6583\n",
      "[ INFO : 2023-05-31 15:42:40,380 ] - Epoch 100/150 - time: 0.06 - training_loss: -8.6604\n",
      "[ INFO : 2023-05-31 15:42:40,439 ] - Epoch 101/150 - time: 0.06 - training_loss: -8.6623\n",
      "[ INFO : 2023-05-31 15:42:40,498 ] - Epoch 102/150 - time: 0.06 - training_loss: -8.6643\n",
      "[ INFO : 2023-05-31 15:42:40,560 ] - Epoch 103/150 - time: 0.06 - training_loss: -8.6662\n",
      "[ INFO : 2023-05-31 15:42:40,620 ] - Epoch 104/150 - time: 0.06 - training_loss: -8.6680\n",
      "[ INFO : 2023-05-31 15:42:40,678 ] - Epoch 105/150 - time: 0.06 - training_loss: -8.6699\n",
      "[ INFO : 2023-05-31 15:42:40,741 ] - Epoch 106/150 - time: 0.06 - training_loss: -8.6717\n",
      "[ INFO : 2023-05-31 15:42:40,799 ] - Epoch 107/150 - time: 0.06 - training_loss: -8.6734\n",
      "[ INFO : 2023-05-31 15:42:40,859 ] - Epoch 108/150 - time: 0.06 - training_loss: -8.6752\n",
      "[ INFO : 2023-05-31 15:42:40,918 ] - Epoch 109/150 - time: 0.06 - training_loss: -8.6769\n",
      "[ INFO : 2023-05-31 15:42:40,975 ] - Epoch 110/150 - time: 0.06 - training_loss: -8.6785\n",
      "[ INFO : 2023-05-31 15:42:41,033 ] - Epoch 111/150 - time: 0.06 - training_loss: -8.6802\n",
      "[ INFO : 2023-05-31 15:42:41,093 ] - Epoch 112/150 - time: 0.06 - training_loss: -8.6818\n",
      "[ INFO : 2023-05-31 15:42:41,150 ] - Epoch 113/150 - time: 0.06 - training_loss: -8.6834\n",
      "[ INFO : 2023-05-31 15:42:41,208 ] - Epoch 114/150 - time: 0.06 - training_loss: -8.6849\n",
      "[ INFO : 2023-05-31 15:42:41,270 ] - Epoch 115/150 - time: 0.06 - training_loss: -8.6864\n",
      "[ INFO : 2023-05-31 15:42:41,329 ] - Epoch 116/150 - time: 0.06 - training_loss: -8.6879\n",
      "[ INFO : 2023-05-31 15:42:41,388 ] - Epoch 117/150 - time: 0.06 - training_loss: -8.6894\n",
      "[ INFO : 2023-05-31 15:42:41,451 ] - Epoch 118/150 - time: 0.06 - training_loss: -8.6909\n",
      "[ INFO : 2023-05-31 15:42:41,509 ] - Epoch 119/150 - time: 0.06 - training_loss: -8.6923\n",
      "[ INFO : 2023-05-31 15:42:41,568 ] - Epoch 120/150 - time: 0.06 - training_loss: -8.6937\n",
      "[ INFO : 2023-05-31 15:42:41,631 ] - Epoch 121/150 - time: 0.06 - training_loss: -8.6951\n",
      "[ INFO : 2023-05-31 15:42:41,691 ] - Epoch 122/150 - time: 0.06 - training_loss: -8.6964\n",
      "[ INFO : 2023-05-31 15:42:41,750 ] - Epoch 123/150 - time: 0.06 - training_loss: -8.6978\n",
      "[ INFO : 2023-05-31 15:42:41,812 ] - Epoch 124/150 - time: 0.06 - training_loss: -8.6991\n",
      "[ INFO : 2023-05-31 15:42:41,870 ] - Epoch 125/150 - time: 0.06 - training_loss: -8.7004\n",
      "[ INFO : 2023-05-31 15:42:41,928 ] - Epoch 126/150 - time: 0.06 - training_loss: -8.7016\n",
      "[ INFO : 2023-05-31 15:42:41,987 ] - Epoch 127/150 - time: 0.06 - training_loss: -8.7029\n",
      "[ INFO : 2023-05-31 15:42:42,045 ] - Epoch 128/150 - time: 0.06 - training_loss: -8.7041\n",
      "[ INFO : 2023-05-31 15:42:42,102 ] - Epoch 129/150 - time: 0.06 - training_loss: -8.7053\n",
      "[ INFO : 2023-05-31 15:42:42,162 ] - Epoch 130/150 - time: 0.06 - training_loss: -8.7065\n",
      "[ INFO : 2023-05-31 15:42:42,220 ] - Epoch 131/150 - time: 0.06 - training_loss: -8.7077\n",
      "[ INFO : 2023-05-31 15:42:42,279 ] - Epoch 132/150 - time: 0.06 - training_loss: -8.7089\n",
      "[ INFO : 2023-05-31 15:42:42,340 ] - Epoch 133/150 - time: 0.06 - training_loss: -8.7100\n",
      "[ INFO : 2023-05-31 15:42:42,400 ] - Epoch 134/150 - time: 0.06 - training_loss: -8.7111\n",
      "[ INFO : 2023-05-31 15:42:42,460 ] - Epoch 135/150 - time: 0.06 - training_loss: -8.7122\n",
      "[ INFO : 2023-05-31 15:42:42,521 ] - Epoch 136/150 - time: 0.06 - training_loss: -8.7133\n",
      "[ INFO : 2023-05-31 15:42:42,579 ] - Epoch 137/150 - time: 0.06 - training_loss: -8.7144\n",
      "[ INFO : 2023-05-31 15:42:42,640 ] - Epoch 138/150 - time: 0.06 - training_loss: -8.7154\n",
      "[ INFO : 2023-05-31 15:42:42,702 ] - Epoch 139/150 - time: 0.06 - training_loss: -8.7165\n",
      "[ INFO : 2023-05-31 15:42:42,761 ] - Epoch 140/150 - time: 0.06 - training_loss: -8.7175\n",
      "[ INFO : 2023-05-31 15:42:42,820 ] - Epoch 141/150 - time: 0.06 - training_loss: -8.7185\n",
      "[ INFO : 2023-05-31 15:42:42,881 ] - Epoch 142/150 - time: 0.06 - training_loss: -8.7195\n",
      "[ INFO : 2023-05-31 15:42:42,939 ] - Epoch 143/150 - time: 0.06 - training_loss: -8.7205\n",
      "[ INFO : 2023-05-31 15:42:42,997 ] - Epoch 144/150 - time: 0.06 - training_loss: -8.7215\n",
      "[ INFO : 2023-05-31 15:42:43,055 ] - Epoch 145/150 - time: 0.06 - training_loss: -8.7224\n",
      "[ INFO : 2023-05-31 15:42:43,114 ] - Epoch 146/150 - time: 0.06 - training_loss: -8.7234\n",
      "[ INFO : 2023-05-31 15:42:43,171 ] - Epoch 147/150 - time: 0.06 - training_loss: -8.7243\n",
      "[ INFO : 2023-05-31 15:42:43,233 ] - Epoch 148/150 - time: 0.06 - training_loss: -8.7252\n",
      "[ INFO : 2023-05-31 15:42:43,291 ] - Epoch 149/150 - time: 0.06 - training_loss: -8.7261\n",
      "[ INFO : 2023-05-31 15:42:43,350 ] - Epoch 150/150 - time: 0.06 - training_loss: -8.7270\n",
      "[ INFO : 2023-05-31 15:42:43,398 ] - DeepGCCA(\n",
      "  (model_list): ModuleList(\n",
      "    (0-2): 3 x MlpNet(\n",
      "      (layers): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
      "          (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[ INFO : 2023-05-31 15:42:43,403 ] - Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.05\n",
      "    lr: 0.05\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "[ INFO : 2023-05-31 15:42:43,469 ] - Epoch 1/150 - time: 0.06 - training_loss: -3.0018\n",
      "[ INFO : 2023-05-31 15:42:43,530 ] - Epoch 2/150 - time: 0.06 - training_loss: -3.6211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear CCA started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2023-05-31 15:42:43,592 ] - Epoch 3/150 - time: 0.06 - training_loss: -4.8620\n",
      "[ INFO : 2023-05-31 15:42:43,653 ] - Epoch 4/150 - time: 0.06 - training_loss: -5.6921\n",
      "[ INFO : 2023-05-31 15:42:43,714 ] - Epoch 5/150 - time: 0.06 - training_loss: -6.2409\n",
      "[ INFO : 2023-05-31 15:42:43,776 ] - Epoch 6/150 - time: 0.06 - training_loss: -6.6429\n",
      "[ INFO : 2023-05-31 15:42:43,836 ] - Epoch 7/150 - time: 0.06 - training_loss: -6.9435\n",
      "[ INFO : 2023-05-31 15:42:43,894 ] - Epoch 8/150 - time: 0.06 - training_loss: -7.1687\n",
      "[ INFO : 2023-05-31 15:42:43,953 ] - Epoch 9/150 - time: 0.06 - training_loss: -7.3402\n",
      "[ INFO : 2023-05-31 15:42:44,011 ] - Epoch 10/150 - time: 0.06 - training_loss: -7.4767\n",
      "[ INFO : 2023-05-31 15:42:44,069 ] - Epoch 11/150 - time: 0.06 - training_loss: -7.5907\n",
      "[ INFO : 2023-05-31 15:42:44,128 ] - Epoch 12/150 - time: 0.06 - training_loss: -7.6888\n",
      "[ INFO : 2023-05-31 15:42:44,187 ] - Epoch 13/150 - time: 0.06 - training_loss: -7.7741\n",
      "[ INFO : 2023-05-31 15:42:44,247 ] - Epoch 14/150 - time: 0.06 - training_loss: -7.8483\n",
      "[ INFO : 2023-05-31 15:42:44,309 ] - Epoch 15/150 - time: 0.06 - training_loss: -7.9131\n",
      "[ INFO : 2023-05-31 15:42:44,368 ] - Epoch 16/150 - time: 0.06 - training_loss: -7.9699\n",
      "[ INFO : 2023-05-31 15:42:44,427 ] - Epoch 17/150 - time: 0.06 - training_loss: -8.0202\n",
      "[ INFO : 2023-05-31 15:42:44,489 ] - Epoch 18/150 - time: 0.06 - training_loss: -8.0650\n",
      "[ INFO : 2023-05-31 15:42:44,548 ] - Epoch 19/150 - time: 0.06 - training_loss: -8.1050\n",
      "[ INFO : 2023-05-31 15:42:44,608 ] - Epoch 20/150 - time: 0.06 - training_loss: -8.1410\n",
      "[ INFO : 2023-05-31 15:42:44,670 ] - Epoch 21/150 - time: 0.06 - training_loss: -8.1736\n",
      "[ INFO : 2023-05-31 15:42:44,730 ] - Epoch 22/150 - time: 0.06 - training_loss: -8.2034\n",
      "[ INFO : 2023-05-31 15:42:44,789 ] - Epoch 23/150 - time: 0.06 - training_loss: -8.2310\n",
      "[ INFO : 2023-05-31 15:42:44,850 ] - Epoch 24/150 - time: 0.06 - training_loss: -8.2565\n",
      "[ INFO : 2023-05-31 15:42:44,909 ] - Epoch 25/150 - time: 0.06 - training_loss: -8.2801\n",
      "[ INFO : 2023-05-31 15:42:44,966 ] - Epoch 26/150 - time: 0.06 - training_loss: -8.3020\n",
      "[ INFO : 2023-05-31 15:42:45,024 ] - Epoch 27/150 - time: 0.06 - training_loss: -8.3223\n",
      "[ INFO : 2023-05-31 15:42:45,083 ] - Epoch 28/150 - time: 0.06 - training_loss: -8.3412\n",
      "[ INFO : 2023-05-31 15:42:45,143 ] - Epoch 29/150 - time: 0.06 - training_loss: -8.3587\n",
      "[ INFO : 2023-05-31 15:42:45,202 ] - Epoch 30/150 - time: 0.06 - training_loss: -8.3751\n",
      "[ INFO : 2023-05-31 15:42:45,262 ] - Epoch 31/150 - time: 0.06 - training_loss: -8.3905\n",
      "[ INFO : 2023-05-31 15:42:45,322 ] - Epoch 32/150 - time: 0.06 - training_loss: -8.4049\n",
      "[ INFO : 2023-05-31 15:42:45,384 ] - Epoch 33/150 - time: 0.06 - training_loss: -8.4185\n",
      "[ INFO : 2023-05-31 15:42:45,444 ] - Epoch 34/150 - time: 0.06 - training_loss: -8.4313\n",
      "[ INFO : 2023-05-31 15:42:45,503 ] - Epoch 35/150 - time: 0.06 - training_loss: -8.4434\n",
      "[ INFO : 2023-05-31 15:42:45,566 ] - Epoch 36/150 - time: 0.06 - training_loss: -8.4548\n",
      "[ INFO : 2023-05-31 15:42:45,626 ] - Epoch 37/150 - time: 0.06 - training_loss: -8.4657\n",
      "[ INFO : 2023-05-31 15:42:45,686 ] - Epoch 38/150 - time: 0.06 - training_loss: -8.4760\n",
      "[ INFO : 2023-05-31 15:42:45,747 ] - Epoch 39/150 - time: 0.06 - training_loss: -8.4857\n",
      "[ INFO : 2023-05-31 15:42:45,807 ] - Epoch 40/150 - time: 0.06 - training_loss: -8.4950\n",
      "[ INFO : 2023-05-31 15:42:45,867 ] - Epoch 41/150 - time: 0.06 - training_loss: -8.5039\n",
      "[ INFO : 2023-05-31 15:42:45,926 ] - Epoch 42/150 - time: 0.06 - training_loss: -8.5123\n",
      "[ INFO : 2023-05-31 15:42:45,983 ] - Epoch 43/150 - time: 0.06 - training_loss: -8.5203\n",
      "[ INFO : 2023-05-31 15:42:46,041 ] - Epoch 44/150 - time: 0.06 - training_loss: -8.5280\n",
      "[ INFO : 2023-05-31 15:42:46,109 ] - Epoch 45/150 - time: 0.07 - training_loss: -8.5354\n",
      "[ INFO : 2023-05-31 15:42:46,168 ] - Epoch 46/150 - time: 0.06 - training_loss: -8.5424\n",
      "[ INFO : 2023-05-31 15:42:46,228 ] - Epoch 47/150 - time: 0.06 - training_loss: -8.5491\n",
      "[ INFO : 2023-05-31 15:42:46,291 ] - Epoch 48/150 - time: 0.06 - training_loss: -8.5556\n",
      "[ INFO : 2023-05-31 15:42:46,351 ] - Epoch 49/150 - time: 0.06 - training_loss: -8.5618\n",
      "[ INFO : 2023-05-31 15:42:46,411 ] - Epoch 50/150 - time: 0.06 - training_loss: -8.5677\n",
      "[ INFO : 2023-05-31 15:42:46,473 ] - Epoch 51/150 - time: 0.06 - training_loss: -8.5734\n",
      "[ INFO : 2023-05-31 15:42:46,531 ] - Epoch 52/150 - time: 0.06 - training_loss: -8.5789\n",
      "[ INFO : 2023-05-31 15:42:46,591 ] - Epoch 53/150 - time: 0.06 - training_loss: -8.5842\n",
      "[ INFO : 2023-05-31 15:42:46,653 ] - Epoch 54/150 - time: 0.06 - training_loss: -8.5893\n",
      "[ INFO : 2023-05-31 15:42:46,712 ] - Epoch 55/150 - time: 0.06 - training_loss: -8.5942\n",
      "[ INFO : 2023-05-31 15:42:46,774 ] - Epoch 56/150 - time: 0.06 - training_loss: -8.5990\n",
      "[ INFO : 2023-05-31 15:42:46,837 ] - Epoch 57/150 - time: 0.06 - training_loss: -8.6036\n",
      "[ INFO : 2023-05-31 15:42:46,896 ] - Epoch 58/150 - time: 0.06 - training_loss: -8.6080\n",
      "[ INFO : 2023-05-31 15:42:46,954 ] - Epoch 59/150 - time: 0.06 - training_loss: -8.6122\n",
      "[ INFO : 2023-05-31 15:42:47,013 ] - Epoch 60/150 - time: 0.06 - training_loss: -8.6164\n",
      "[ INFO : 2023-05-31 15:42:47,071 ] - Epoch 61/150 - time: 0.06 - training_loss: -8.6204\n",
      "[ INFO : 2023-05-31 15:42:47,128 ] - Epoch 62/150 - time: 0.06 - training_loss: -8.6242\n",
      "[ INFO : 2023-05-31 15:42:47,188 ] - Epoch 63/150 - time: 0.06 - training_loss: -8.6280\n",
      "[ INFO : 2023-05-31 15:42:47,248 ] - Epoch 64/150 - time: 0.06 - training_loss: -8.6316\n",
      "[ INFO : 2023-05-31 15:42:47,307 ] - Epoch 65/150 - time: 0.06 - training_loss: -8.6351\n",
      "[ INFO : 2023-05-31 15:42:47,369 ] - Epoch 66/150 - time: 0.06 - training_loss: -8.6385\n",
      "[ INFO : 2023-05-31 15:42:47,430 ] - Epoch 67/150 - time: 0.06 - training_loss: -8.6418\n",
      "[ INFO : 2023-05-31 15:42:47,489 ] - Epoch 68/150 - time: 0.06 - training_loss: -8.6450\n",
      "[ INFO : 2023-05-31 15:42:47,551 ] - Epoch 69/150 - time: 0.06 - training_loss: -8.6482\n",
      "[ INFO : 2023-05-31 15:42:47,611 ] - Epoch 70/150 - time: 0.06 - training_loss: -8.6512\n",
      "[ INFO : 2023-05-31 15:42:47,670 ] - Epoch 71/150 - time: 0.06 - training_loss: -8.6541\n",
      "[ INFO : 2023-05-31 15:42:47,735 ] - Epoch 72/150 - time: 0.06 - training_loss: -8.6570\n",
      "[ INFO : 2023-05-31 15:42:47,794 ] - Epoch 73/150 - time: 0.06 - training_loss: -8.6598\n",
      "[ INFO : 2023-05-31 15:42:47,854 ] - Epoch 74/150 - time: 0.06 - training_loss: -8.6625\n",
      "[ INFO : 2023-05-31 15:42:47,914 ] - Epoch 75/150 - time: 0.06 - training_loss: -8.6651\n",
      "[ INFO : 2023-05-31 15:42:47,973 ] - Epoch 76/150 - time: 0.06 - training_loss: -8.6677\n",
      "[ INFO : 2023-05-31 15:42:48,031 ] - Epoch 77/150 - time: 0.06 - training_loss: -8.6702\n",
      "[ INFO : 2023-05-31 15:42:48,089 ] - Epoch 78/150 - time: 0.06 - training_loss: -8.6726\n",
      "[ INFO : 2023-05-31 15:42:48,147 ] - Epoch 79/150 - time: 0.06 - training_loss: -8.6750\n",
      "[ INFO : 2023-05-31 15:42:48,206 ] - Epoch 80/150 - time: 0.06 - training_loss: -8.6773\n",
      "[ INFO : 2023-05-31 15:42:48,268 ] - Epoch 81/150 - time: 0.06 - training_loss: -8.6796\n",
      "[ INFO : 2023-05-31 15:42:48,327 ] - Epoch 82/150 - time: 0.06 - training_loss: -8.6818\n",
      "[ INFO : 2023-05-31 15:42:48,387 ] - Epoch 83/150 - time: 0.06 - training_loss: -8.6840\n",
      "[ INFO : 2023-05-31 15:42:48,449 ] - Epoch 84/150 - time: 0.06 - training_loss: -8.6861\n",
      "[ INFO : 2023-05-31 15:42:48,510 ] - Epoch 85/150 - time: 0.06 - training_loss: -8.6881\n",
      "[ INFO : 2023-05-31 15:42:48,569 ] - Epoch 86/150 - time: 0.06 - training_loss: -8.6901\n",
      "[ INFO : 2023-05-31 15:42:48,631 ] - Epoch 87/150 - time: 0.06 - training_loss: -8.6921\n",
      "[ INFO : 2023-05-31 15:42:48,691 ] - Epoch 88/150 - time: 0.06 - training_loss: -8.6940\n",
      "[ INFO : 2023-05-31 15:42:48,750 ] - Epoch 89/150 - time: 0.06 - training_loss: -8.6958\n",
      "[ INFO : 2023-05-31 15:42:48,811 ] - Epoch 90/150 - time: 0.06 - training_loss: -8.6977\n",
      "[ INFO : 2023-05-31 15:42:48,870 ] - Epoch 91/150 - time: 0.06 - training_loss: -8.6995\n",
      "[ INFO : 2023-05-31 15:42:48,930 ] - Epoch 92/150 - time: 0.06 - training_loss: -8.7012\n",
      "[ INFO : 2023-05-31 15:42:48,989 ] - Epoch 93/150 - time: 0.06 - training_loss: -8.7029\n",
      "[ INFO : 2023-05-31 15:42:49,047 ] - Epoch 94/150 - time: 0.06 - training_loss: -8.7046\n",
      "[ INFO : 2023-05-31 15:42:49,105 ] - Epoch 95/150 - time: 0.06 - training_loss: -8.7062\n",
      "[ INFO : 2023-05-31 15:42:49,165 ] - Epoch 96/150 - time: 0.06 - training_loss: -8.7078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2023-05-31 15:42:49,224 ] - Epoch 97/150 - time: 0.06 - training_loss: -8.7094\n",
      "[ INFO : 2023-05-31 15:42:49,283 ] - Epoch 98/150 - time: 0.06 - training_loss: -8.7110\n",
      "[ INFO : 2023-05-31 15:42:49,345 ] - Epoch 99/150 - time: 0.06 - training_loss: -8.7125\n",
      "[ INFO : 2023-05-31 15:42:49,404 ] - Epoch 100/150 - time: 0.06 - training_loss: -8.7139\n",
      "[ INFO : 2023-05-31 15:42:49,464 ] - Epoch 101/150 - time: 0.06 - training_loss: -8.7154\n",
      "[ INFO : 2023-05-31 15:42:49,527 ] - Epoch 102/150 - time: 0.06 - training_loss: -8.7168\n",
      "[ INFO : 2023-05-31 15:42:49,586 ] - Epoch 103/150 - time: 0.06 - training_loss: -8.7182\n",
      "[ INFO : 2023-05-31 15:42:49,645 ] - Epoch 104/150 - time: 0.06 - training_loss: -8.7196\n",
      "[ INFO : 2023-05-31 15:42:49,707 ] - Epoch 105/150 - time: 0.06 - training_loss: -8.7209\n",
      "[ INFO : 2023-05-31 15:42:49,767 ] - Epoch 106/150 - time: 0.06 - training_loss: -8.7222\n",
      "[ INFO : 2023-05-31 15:42:49,828 ] - Epoch 107/150 - time: 0.06 - training_loss: -8.7235\n",
      "[ INFO : 2023-05-31 15:42:49,887 ] - Epoch 108/150 - time: 0.06 - training_loss: -8.7248\n",
      "[ INFO : 2023-05-31 15:42:49,945 ] - Epoch 109/150 - time: 0.06 - training_loss: -8.7260\n",
      "[ INFO : 2023-05-31 15:42:50,003 ] - Epoch 110/150 - time: 0.06 - training_loss: -8.7273\n",
      "[ INFO : 2023-05-31 15:42:50,062 ] - Epoch 111/150 - time: 0.06 - training_loss: -8.7285\n",
      "[ INFO : 2023-05-31 15:42:50,120 ] - Epoch 112/150 - time: 0.06 - training_loss: -8.7296\n",
      "[ INFO : 2023-05-31 15:42:50,178 ] - Epoch 113/150 - time: 0.06 - training_loss: -8.7308\n",
      "[ INFO : 2023-05-31 15:42:50,240 ] - Epoch 114/150 - time: 0.06 - training_loss: -8.7319\n",
      "[ INFO : 2023-05-31 15:42:50,300 ] - Epoch 115/150 - time: 0.06 - training_loss: -8.7331\n",
      "[ INFO : 2023-05-31 15:42:50,359 ] - Epoch 116/150 - time: 0.06 - training_loss: -8.7341\n",
      "[ INFO : 2023-05-31 15:42:50,421 ] - Epoch 117/150 - time: 0.06 - training_loss: -8.7352\n",
      "[ INFO : 2023-05-31 15:42:50,481 ] - Epoch 118/150 - time: 0.06 - training_loss: -8.7363\n",
      "[ INFO : 2023-05-31 15:42:50,541 ] - Epoch 119/150 - time: 0.06 - training_loss: -8.7373\n",
      "[ INFO : 2023-05-31 15:42:50,603 ] - Epoch 120/150 - time: 0.06 - training_loss: -8.7384\n",
      "[ INFO : 2023-05-31 15:42:50,663 ] - Epoch 121/150 - time: 0.06 - training_loss: -8.7394\n",
      "[ INFO : 2023-05-31 15:42:50,722 ] - Epoch 122/150 - time: 0.06 - training_loss: -8.7404\n",
      "[ INFO : 2023-05-31 15:42:50,788 ] - Epoch 123/150 - time: 0.06 - training_loss: -8.7413\n",
      "[ INFO : 2023-05-31 15:42:50,847 ] - Epoch 124/150 - time: 0.06 - training_loss: -8.7423\n",
      "[ INFO : 2023-05-31 15:42:50,905 ] - Epoch 125/150 - time: 0.06 - training_loss: -8.7432\n",
      "[ INFO : 2023-05-31 15:42:50,964 ] - Epoch 126/150 - time: 0.06 - training_loss: -8.7442\n",
      "[ INFO : 2023-05-31 15:42:51,022 ] - Epoch 127/150 - time: 0.06 - training_loss: -8.7451\n",
      "[ INFO : 2023-05-31 15:42:51,080 ] - Epoch 128/150 - time: 0.06 - training_loss: -8.7460\n",
      "[ INFO : 2023-05-31 15:42:51,139 ] - Epoch 129/150 - time: 0.06 - training_loss: -8.7469\n",
      "[ INFO : 2023-05-31 15:42:51,199 ] - Epoch 130/150 - time: 0.06 - training_loss: -8.7477\n",
      "[ INFO : 2023-05-31 15:42:51,258 ] - Epoch 131/150 - time: 0.06 - training_loss: -8.7486\n",
      "[ INFO : 2023-05-31 15:42:51,320 ] - Epoch 132/150 - time: 0.06 - training_loss: -8.7495\n",
      "[ INFO : 2023-05-31 15:42:51,379 ] - Epoch 133/150 - time: 0.06 - training_loss: -8.7503\n",
      "[ INFO : 2023-05-31 15:42:51,439 ] - Epoch 134/150 - time: 0.06 - training_loss: -8.7511\n",
      "[ INFO : 2023-05-31 15:42:51,502 ] - Epoch 135/150 - time: 0.06 - training_loss: -8.7519\n",
      "[ INFO : 2023-05-31 15:42:51,563 ] - Epoch 136/150 - time: 0.06 - training_loss: -8.7527\n",
      "[ INFO : 2023-05-31 15:42:51,625 ] - Epoch 137/150 - time: 0.06 - training_loss: -8.7535\n",
      "[ INFO : 2023-05-31 15:42:51,690 ] - Epoch 138/150 - time: 0.06 - training_loss: -8.7543\n",
      "[ INFO : 2023-05-31 15:42:51,751 ] - Epoch 139/150 - time: 0.06 - training_loss: -8.7550\n",
      "[ INFO : 2023-05-31 15:42:51,811 ] - Epoch 140/150 - time: 0.06 - training_loss: -8.7558\n",
      "[ INFO : 2023-05-31 15:42:51,887 ] - Epoch 141/150 - time: 0.08 - training_loss: -8.7565\n",
      "[ INFO : 2023-05-31 15:42:51,945 ] - Epoch 142/150 - time: 0.06 - training_loss: -8.7573\n",
      "[ INFO : 2023-05-31 15:42:52,003 ] - Epoch 143/150 - time: 0.06 - training_loss: -8.7580\n",
      "[ INFO : 2023-05-31 15:42:52,062 ] - Epoch 144/150 - time: 0.06 - training_loss: -8.7587\n",
      "[ INFO : 2023-05-31 15:42:52,120 ] - Epoch 145/150 - time: 0.06 - training_loss: -8.7594\n",
      "[ INFO : 2023-05-31 15:42:52,178 ] - Epoch 146/150 - time: 0.06 - training_loss: -8.7601\n",
      "[ INFO : 2023-05-31 15:42:52,240 ] - Epoch 147/150 - time: 0.06 - training_loss: -8.7608\n",
      "[ INFO : 2023-05-31 15:42:52,300 ] - Epoch 148/150 - time: 0.06 - training_loss: -8.7614\n",
      "[ INFO : 2023-05-31 15:42:52,359 ] - Epoch 149/150 - time: 0.06 - training_loss: -8.7621\n",
      "[ INFO : 2023-05-31 15:42:52,421 ] - Epoch 150/150 - time: 0.06 - training_loss: -8.7628\n",
      "[ INFO : 2023-05-31 15:42:52,466 ] - DeepGCCA(\n",
      "  (model_list): ModuleList(\n",
      "    (0-2): 3 x MlpNet(\n",
      "      (layers): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
      "          (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[ INFO : 2023-05-31 15:42:52,471 ] - Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.05\n",
      "    lr: 0.05\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "[ INFO : 2023-05-31 15:42:52,537 ] - Epoch 1/150 - time: 0.06 - training_loss: -3.1359\n",
      "[ INFO : 2023-05-31 15:42:52,596 ] - Epoch 2/150 - time: 0.06 - training_loss: -4.1811\n",
      "[ INFO : 2023-05-31 15:42:52,656 ] - Epoch 3/150 - time: 0.06 - training_loss: -5.3615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear CCA started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2023-05-31 15:42:52,716 ] - Epoch 4/150 - time: 0.06 - training_loss: -6.1516\n",
      "[ INFO : 2023-05-31 15:42:52,778 ] - Epoch 5/150 - time: 0.06 - training_loss: -6.6446\n",
      "[ INFO : 2023-05-31 15:42:52,837 ] - Epoch 6/150 - time: 0.06 - training_loss: -6.9881\n",
      "[ INFO : 2023-05-31 15:42:52,895 ] - Epoch 7/150 - time: 0.06 - training_loss: -7.2352\n",
      "[ INFO : 2023-05-31 15:42:52,954 ] - Epoch 8/150 - time: 0.06 - training_loss: -7.4198\n",
      "[ INFO : 2023-05-31 15:42:53,012 ] - Epoch 9/150 - time: 0.06 - training_loss: -7.5645\n",
      "[ INFO : 2023-05-31 15:42:53,069 ] - Epoch 10/150 - time: 0.06 - training_loss: -7.6830\n",
      "[ INFO : 2023-05-31 15:42:53,128 ] - Epoch 11/150 - time: 0.06 - training_loss: -7.7827\n",
      "[ INFO : 2023-05-31 15:42:53,187 ] - Epoch 12/150 - time: 0.06 - training_loss: -7.8669\n",
      "[ INFO : 2023-05-31 15:42:53,246 ] - Epoch 13/150 - time: 0.06 - training_loss: -7.9381\n",
      "[ INFO : 2023-05-31 15:42:53,308 ] - Epoch 14/150 - time: 0.06 - training_loss: -7.9992\n",
      "[ INFO : 2023-05-31 15:42:53,367 ] - Epoch 15/150 - time: 0.06 - training_loss: -8.0529\n",
      "[ INFO : 2023-05-31 15:42:53,426 ] - Epoch 16/150 - time: 0.06 - training_loss: -8.1008\n",
      "[ INFO : 2023-05-31 15:42:53,487 ] - Epoch 17/150 - time: 0.06 - training_loss: -8.1435\n",
      "[ INFO : 2023-05-31 15:42:53,546 ] - Epoch 18/150 - time: 0.06 - training_loss: -8.1817\n",
      "[ INFO : 2023-05-31 15:42:53,605 ] - Epoch 19/150 - time: 0.06 - training_loss: -8.2157\n",
      "[ INFO : 2023-05-31 15:42:53,679 ] - Epoch 20/150 - time: 0.07 - training_loss: -8.2463\n",
      "[ INFO : 2023-05-31 15:42:53,739 ] - Epoch 21/150 - time: 0.06 - training_loss: -8.2742\n",
      "[ INFO : 2023-05-31 15:42:53,797 ] - Epoch 22/150 - time: 0.06 - training_loss: -8.2999\n",
      "[ INFO : 2023-05-31 15:42:53,859 ] - Epoch 23/150 - time: 0.06 - training_loss: -8.3236\n",
      "[ INFO : 2023-05-31 15:42:53,916 ] - Epoch 24/150 - time: 0.06 - training_loss: -8.3454\n",
      "[ INFO : 2023-05-31 15:42:53,973 ] - Epoch 25/150 - time: 0.06 - training_loss: -8.3654\n",
      "[ INFO : 2023-05-31 15:42:54,033 ] - Epoch 26/150 - time: 0.06 - training_loss: -8.3838\n",
      "[ INFO : 2023-05-31 15:42:54,090 ] - Epoch 27/150 - time: 0.06 - training_loss: -8.4010\n",
      "[ INFO : 2023-05-31 15:42:54,148 ] - Epoch 28/150 - time: 0.06 - training_loss: -8.4170\n",
      "[ INFO : 2023-05-31 15:42:54,233 ] - Epoch 29/150 - time: 0.08 - training_loss: -8.4320\n",
      "[ INFO : 2023-05-31 15:42:54,292 ] - Epoch 30/150 - time: 0.06 - training_loss: -8.4460\n",
      "[ INFO : 2023-05-31 15:42:54,351 ] - Epoch 31/150 - time: 0.06 - training_loss: -8.4591\n",
      "[ INFO : 2023-05-31 15:42:54,412 ] - Epoch 32/150 - time: 0.06 - training_loss: -8.4714\n",
      "[ INFO : 2023-05-31 15:42:54,471 ] - Epoch 33/150 - time: 0.06 - training_loss: -8.4830\n",
      "[ INFO : 2023-05-31 15:42:54,530 ] - Epoch 34/150 - time: 0.06 - training_loss: -8.4939\n",
      "[ INFO : 2023-05-31 15:42:54,592 ] - Epoch 35/150 - time: 0.06 - training_loss: -8.5043\n",
      "[ INFO : 2023-05-31 15:42:54,651 ] - Epoch 36/150 - time: 0.06 - training_loss: -8.5140\n",
      "[ INFO : 2023-05-31 15:42:54,711 ] - Epoch 37/150 - time: 0.06 - training_loss: -8.5232\n",
      "[ INFO : 2023-05-31 15:42:54,772 ] - Epoch 38/150 - time: 0.06 - training_loss: -8.5320\n",
      "[ INFO : 2023-05-31 15:42:54,833 ] - Epoch 39/150 - time: 0.06 - training_loss: -8.5403\n",
      "[ INFO : 2023-05-31 15:42:54,890 ] - Epoch 40/150 - time: 0.06 - training_loss: -8.5483\n",
      "[ INFO : 2023-05-31 15:42:54,951 ] - Epoch 41/150 - time: 0.06 - training_loss: -8.5558\n",
      "[ INFO : 2023-05-31 15:42:55,008 ] - Epoch 42/150 - time: 0.06 - training_loss: -8.5630\n",
      "[ INFO : 2023-05-31 15:42:55,066 ] - Epoch 43/150 - time: 0.06 - training_loss: -8.5699\n",
      "[ INFO : 2023-05-31 15:42:55,124 ] - Epoch 44/150 - time: 0.06 - training_loss: -8.5764\n",
      "[ INFO : 2023-05-31 15:42:55,183 ] - Epoch 45/150 - time: 0.06 - training_loss: -8.5827\n",
      "[ INFO : 2023-05-31 15:42:55,242 ] - Epoch 46/150 - time: 0.06 - training_loss: -8.5887\n",
      "[ INFO : 2023-05-31 15:42:55,304 ] - Epoch 47/150 - time: 0.06 - training_loss: -8.5944\n",
      "[ INFO : 2023-05-31 15:42:55,363 ] - Epoch 48/150 - time: 0.06 - training_loss: -8.6000\n",
      "[ INFO : 2023-05-31 15:42:55,422 ] - Epoch 49/150 - time: 0.06 - training_loss: -8.6053\n",
      "[ INFO : 2023-05-31 15:42:55,484 ] - Epoch 50/150 - time: 0.06 - training_loss: -8.6103\n",
      "[ INFO : 2023-05-31 15:42:55,543 ] - Epoch 51/150 - time: 0.06 - training_loss: -8.6152\n",
      "[ INFO : 2023-05-31 15:42:55,601 ] - Epoch 52/150 - time: 0.06 - training_loss: -8.6199\n",
      "[ INFO : 2023-05-31 15:42:55,663 ] - Epoch 53/150 - time: 0.06 - training_loss: -8.6244\n",
      "[ INFO : 2023-05-31 15:42:55,722 ] - Epoch 54/150 - time: 0.06 - training_loss: -8.6288\n",
      "[ INFO : 2023-05-31 15:42:55,781 ] - Epoch 55/150 - time: 0.06 - training_loss: -8.6330\n",
      "[ INFO : 2023-05-31 15:42:55,842 ] - Epoch 56/150 - time: 0.06 - training_loss: -8.6370\n",
      "[ INFO : 2023-05-31 15:42:55,899 ] - Epoch 57/150 - time: 0.06 - training_loss: -8.6410\n",
      "[ INFO : 2023-05-31 15:42:55,957 ] - Epoch 58/150 - time: 0.06 - training_loss: -8.6447\n",
      "[ INFO : 2023-05-31 15:42:56,016 ] - Epoch 59/150 - time: 0.06 - training_loss: -8.6484\n",
      "[ INFO : 2023-05-31 15:42:56,074 ] - Epoch 60/150 - time: 0.06 - training_loss: -8.6519\n",
      "[ INFO : 2023-05-31 15:42:56,131 ] - Epoch 61/150 - time: 0.06 - training_loss: -8.6553\n",
      "[ INFO : 2023-05-31 15:42:56,192 ] - Epoch 62/150 - time: 0.06 - training_loss: -8.6586\n",
      "[ INFO : 2023-05-31 15:42:56,251 ] - Epoch 63/150 - time: 0.06 - training_loss: -8.6618\n",
      "[ INFO : 2023-05-31 15:42:56,313 ] - Epoch 64/150 - time: 0.06 - training_loss: -8.6649\n",
      "[ INFO : 2023-05-31 15:42:56,375 ] - Epoch 65/150 - time: 0.06 - training_loss: -8.6679\n",
      "[ INFO : 2023-05-31 15:42:56,434 ] - Epoch 66/150 - time: 0.06 - training_loss: -8.6708\n",
      "[ INFO : 2023-05-31 15:42:56,496 ] - Epoch 67/150 - time: 0.06 - training_loss: -8.6737\n",
      "[ INFO : 2023-05-31 15:42:56,558 ] - Epoch 68/150 - time: 0.06 - training_loss: -8.6764\n",
      "[ INFO : 2023-05-31 15:42:56,617 ] - Epoch 69/150 - time: 0.06 - training_loss: -8.6791\n",
      "[ INFO : 2023-05-31 15:42:56,676 ] - Epoch 70/150 - time: 0.06 - training_loss: -8.6817\n",
      "[ INFO : 2023-05-31 15:42:56,738 ] - Epoch 71/150 - time: 0.06 - training_loss: -8.6842\n",
      "[ INFO : 2023-05-31 15:42:56,797 ] - Epoch 72/150 - time: 0.06 - training_loss: -8.6866\n",
      "[ INFO : 2023-05-31 15:42:56,855 ] - Epoch 73/150 - time: 0.06 - training_loss: -8.6890\n",
      "[ INFO : 2023-05-31 15:42:56,914 ] - Epoch 74/150 - time: 0.06 - training_loss: -8.6913\n",
      "[ INFO : 2023-05-31 15:42:56,972 ] - Epoch 75/150 - time: 0.06 - training_loss: -8.6936\n",
      "[ INFO : 2023-05-31 15:42:57,029 ] - Epoch 76/150 - time: 0.06 - training_loss: -8.6958\n",
      "[ INFO : 2023-05-31 15:42:57,089 ] - Epoch 77/150 - time: 0.06 - training_loss: -8.6979\n",
      "[ INFO : 2023-05-31 15:42:57,146 ] - Epoch 78/150 - time: 0.06 - training_loss: -8.7000\n",
      "[ INFO : 2023-05-31 15:42:57,210 ] - Epoch 79/150 - time: 0.06 - training_loss: -8.7020\n",
      "[ INFO : 2023-05-31 15:42:57,273 ] - Epoch 80/150 - time: 0.06 - training_loss: -8.7040\n",
      "[ INFO : 2023-05-31 15:42:57,334 ] - Epoch 81/150 - time: 0.06 - training_loss: -8.7059\n",
      "[ INFO : 2023-05-31 15:42:57,394 ] - Epoch 82/150 - time: 0.06 - training_loss: -8.7078\n",
      "[ INFO : 2023-05-31 15:42:57,458 ] - Epoch 83/150 - time: 0.06 - training_loss: -8.7096\n",
      "[ INFO : 2023-05-31 15:42:57,518 ] - Epoch 84/150 - time: 0.06 - training_loss: -8.7114\n",
      "[ INFO : 2023-05-31 15:42:57,578 ] - Epoch 85/150 - time: 0.06 - training_loss: -8.7132\n",
      "[ INFO : 2023-05-31 15:42:57,640 ] - Epoch 86/150 - time: 0.06 - training_loss: -8.7149\n",
      "[ INFO : 2023-05-31 15:42:57,698 ] - Epoch 87/150 - time: 0.06 - training_loss: -8.7166\n",
      "[ INFO : 2023-05-31 15:42:57,758 ] - Epoch 88/150 - time: 0.06 - training_loss: -8.7182\n",
      "[ INFO : 2023-05-31 15:42:57,819 ] - Epoch 89/150 - time: 0.06 - training_loss: -8.7198\n",
      "[ INFO : 2023-05-31 15:42:57,878 ] - Epoch 90/150 - time: 0.06 - training_loss: -8.7214\n",
      "[ INFO : 2023-05-31 15:42:57,935 ] - Epoch 91/150 - time: 0.06 - training_loss: -8.7229\n",
      "[ INFO : 2023-05-31 15:42:57,995 ] - Epoch 92/150 - time: 0.06 - training_loss: -8.7244\n",
      "[ INFO : 2023-05-31 15:42:58,053 ] - Epoch 93/150 - time: 0.06 - training_loss: -8.7259\n",
      "[ INFO : 2023-05-31 15:42:58,111 ] - Epoch 94/150 - time: 0.06 - training_loss: -8.7273\n",
      "[ INFO : 2023-05-31 15:42:58,171 ] - Epoch 95/150 - time: 0.06 - training_loss: -8.7287\n",
      "[ INFO : 2023-05-31 15:42:58,229 ] - Epoch 96/150 - time: 0.06 - training_loss: -8.7301\n",
      "[ INFO : 2023-05-31 15:42:58,288 ] - Epoch 97/150 - time: 0.06 - training_loss: -8.7314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2023-05-31 15:42:58,350 ] - Epoch 98/150 - time: 0.06 - training_loss: -8.7327\n",
      "[ INFO : 2023-05-31 15:42:58,409 ] - Epoch 99/150 - time: 0.06 - training_loss: -8.7340\n",
      "[ INFO : 2023-05-31 15:42:58,468 ] - Epoch 100/150 - time: 0.06 - training_loss: -8.7353\n",
      "[ INFO : 2023-05-31 15:42:58,530 ] - Epoch 101/150 - time: 0.06 - training_loss: -8.7365\n",
      "[ INFO : 2023-05-31 15:42:58,590 ] - Epoch 102/150 - time: 0.06 - training_loss: -8.7377\n",
      "[ INFO : 2023-05-31 15:42:58,649 ] - Epoch 103/150 - time: 0.06 - training_loss: -8.7389\n",
      "[ INFO : 2023-05-31 15:42:58,711 ] - Epoch 104/150 - time: 0.06 - training_loss: -8.7401\n",
      "[ INFO : 2023-05-31 15:42:58,771 ] - Epoch 105/150 - time: 0.06 - training_loss: -8.7412\n",
      "[ INFO : 2023-05-31 15:42:58,830 ] - Epoch 106/150 - time: 0.06 - training_loss: -8.7424\n",
      "[ INFO : 2023-05-31 15:42:58,890 ] - Epoch 107/150 - time: 0.06 - training_loss: -8.7435\n",
      "[ INFO : 2023-05-31 15:42:58,948 ] - Epoch 108/150 - time: 0.06 - training_loss: -8.7445\n",
      "[ INFO : 2023-05-31 15:42:59,006 ] - Epoch 109/150 - time: 0.06 - training_loss: -8.7456\n",
      "[ INFO : 2023-05-31 15:42:59,065 ] - Epoch 110/150 - time: 0.06 - training_loss: -8.7466\n",
      "[ INFO : 2023-05-31 15:42:59,123 ] - Epoch 111/150 - time: 0.06 - training_loss: -8.7477\n",
      "[ INFO : 2023-05-31 15:42:59,181 ] - Epoch 112/150 - time: 0.06 - training_loss: -8.7487\n",
      "[ INFO : 2023-05-31 15:42:59,243 ] - Epoch 113/150 - time: 0.06 - training_loss: -8.7497\n",
      "[ INFO : 2023-05-31 15:42:59,302 ] - Epoch 114/150 - time: 0.06 - training_loss: -8.7506\n",
      "[ INFO : 2023-05-31 15:42:59,362 ] - Epoch 115/150 - time: 0.06 - training_loss: -8.7516\n",
      "[ INFO : 2023-05-31 15:42:59,423 ] - Epoch 116/150 - time: 0.06 - training_loss: -8.7525\n",
      "[ INFO : 2023-05-31 15:42:59,483 ] - Epoch 117/150 - time: 0.06 - training_loss: -8.7535\n",
      "[ INFO : 2023-05-31 15:42:59,542 ] - Epoch 118/150 - time: 0.06 - training_loss: -8.7544\n",
      "[ INFO : 2023-05-31 15:42:59,605 ] - Epoch 119/150 - time: 0.06 - training_loss: -8.7552\n",
      "[ INFO : 2023-05-31 15:42:59,664 ] - Epoch 120/150 - time: 0.06 - training_loss: -8.7561\n",
      "[ INFO : 2023-05-31 15:42:59,724 ] - Epoch 121/150 - time: 0.06 - training_loss: -8.7570\n",
      "[ INFO : 2023-05-31 15:42:59,799 ] - Epoch 122/150 - time: 0.07 - training_loss: -8.7578\n",
      "[ INFO : 2023-05-31 15:42:59,857 ] - Epoch 123/150 - time: 0.06 - training_loss: -8.7587\n",
      "[ INFO : 2023-05-31 15:42:59,915 ] - Epoch 124/150 - time: 0.06 - training_loss: -8.7595\n",
      "[ INFO : 2023-05-31 15:42:59,975 ] - Epoch 125/150 - time: 0.06 - training_loss: -8.7603\n",
      "[ INFO : 2023-05-31 15:43:00,031 ] - Epoch 126/150 - time: 0.05 - training_loss: -8.7611\n",
      "[ INFO : 2023-05-31 15:43:00,090 ] - Epoch 127/150 - time: 0.06 - training_loss: -8.7619\n",
      "[ INFO : 2023-05-31 15:43:00,150 ] - Epoch 128/150 - time: 0.06 - training_loss: -8.7626\n",
      "[ INFO : 2023-05-31 15:43:00,209 ] - Epoch 129/150 - time: 0.06 - training_loss: -8.7634\n",
      "[ INFO : 2023-05-31 15:43:00,269 ] - Epoch 130/150 - time: 0.06 - training_loss: -8.7641\n",
      "[ INFO : 2023-05-31 15:43:00,331 ] - Epoch 131/150 - time: 0.06 - training_loss: -8.7649\n",
      "[ INFO : 2023-05-31 15:43:00,390 ] - Epoch 132/150 - time: 0.06 - training_loss: -8.7656\n",
      "[ INFO : 2023-05-31 15:43:00,450 ] - Epoch 133/150 - time: 0.06 - training_loss: -8.7663\n",
      "[ INFO : 2023-05-31 15:43:00,511 ] - Epoch 134/150 - time: 0.06 - training_loss: -8.7670\n",
      "[ INFO : 2023-05-31 15:43:00,571 ] - Epoch 135/150 - time: 0.06 - training_loss: -8.7677\n",
      "[ INFO : 2023-05-31 15:43:00,630 ] - Epoch 136/150 - time: 0.06 - training_loss: -8.7684\n",
      "[ INFO : 2023-05-31 15:43:00,692 ] - Epoch 137/150 - time: 0.06 - training_loss: -8.7691\n",
      "[ INFO : 2023-05-31 15:43:00,752 ] - Epoch 138/150 - time: 0.06 - training_loss: -8.7697\n",
      "[ INFO : 2023-05-31 15:43:00,811 ] - Epoch 139/150 - time: 0.06 - training_loss: -8.7704\n",
      "[ INFO : 2023-05-31 15:43:00,873 ] - Epoch 140/150 - time: 0.06 - training_loss: -8.7710\n",
      "[ INFO : 2023-05-31 15:43:00,931 ] - Epoch 141/150 - time: 0.06 - training_loss: -8.7717\n",
      "[ INFO : 2023-05-31 15:43:00,989 ] - Epoch 142/150 - time: 0.06 - training_loss: -8.7723\n",
      "[ INFO : 2023-05-31 15:43:01,050 ] - Epoch 143/150 - time: 0.06 - training_loss: -8.7729\n",
      "[ INFO : 2023-05-31 15:43:01,107 ] - Epoch 144/150 - time: 0.06 - training_loss: -8.7735\n",
      "[ INFO : 2023-05-31 15:43:01,166 ] - Epoch 145/150 - time: 0.06 - training_loss: -8.7741\n",
      "[ INFO : 2023-05-31 15:43:01,228 ] - Epoch 146/150 - time: 0.06 - training_loss: -8.7747\n",
      "[ INFO : 2023-05-31 15:43:01,287 ] - Epoch 147/150 - time: 0.06 - training_loss: -8.7753\n",
      "[ INFO : 2023-05-31 15:43:01,346 ] - Epoch 148/150 - time: 0.06 - training_loss: -8.7759\n",
      "[ INFO : 2023-05-31 15:43:01,408 ] - Epoch 149/150 - time: 0.06 - training_loss: -8.7764\n",
      "[ INFO : 2023-05-31 15:43:01,467 ] - Epoch 150/150 - time: 0.06 - training_loss: -8.7770\n",
      "[ INFO : 2023-05-31 15:43:01,512 ] - DeepGCCA(\n",
      "  (model_list): ModuleList(\n",
      "    (0-2): 3 x MlpNet(\n",
      "      (layers): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
      "          (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[ INFO : 2023-05-31 15:43:01,518 ] - Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.05\n",
      "    lr: 0.05\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "[ INFO : 2023-05-31 15:43:01,585 ] - Epoch 1/150 - time: 0.06 - training_loss: -3.2698\n",
      "[ INFO : 2023-05-31 15:43:01,645 ] - Epoch 2/150 - time: 0.06 - training_loss: -3.7138\n",
      "[ INFO : 2023-05-31 15:43:01,705 ] - Epoch 3/150 - time: 0.06 - training_loss: -4.8708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear CCA started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2023-05-31 15:43:01,768 ] - Epoch 4/150 - time: 0.06 - training_loss: -5.6813\n",
      "[ INFO : 2023-05-31 15:43:01,828 ] - Epoch 5/150 - time: 0.06 - training_loss: -6.2173\n",
      "[ INFO : 2023-05-31 15:43:01,886 ] - Epoch 6/150 - time: 0.06 - training_loss: -6.6128\n",
      "[ INFO : 2023-05-31 15:43:01,943 ] - Epoch 7/150 - time: 0.06 - training_loss: -6.9154\n",
      "[ INFO : 2023-05-31 15:43:02,002 ] - Epoch 8/150 - time: 0.06 - training_loss: -7.1466\n",
      "[ INFO : 2023-05-31 15:43:02,060 ] - Epoch 9/150 - time: 0.06 - training_loss: -7.3259\n",
      "[ INFO : 2023-05-31 15:43:02,119 ] - Epoch 10/150 - time: 0.06 - training_loss: -7.4681\n",
      "[ INFO : 2023-05-31 15:43:02,179 ] - Epoch 11/150 - time: 0.06 - training_loss: -7.5841\n",
      "[ INFO : 2023-05-31 15:43:02,239 ] - Epoch 12/150 - time: 0.06 - training_loss: -7.6821\n",
      "[ INFO : 2023-05-31 15:43:02,299 ] - Epoch 13/150 - time: 0.06 - training_loss: -7.7671\n",
      "[ INFO : 2023-05-31 15:43:02,361 ] - Epoch 14/150 - time: 0.06 - training_loss: -7.8414\n",
      "[ INFO : 2023-05-31 15:43:02,426 ] - Epoch 15/150 - time: 0.06 - training_loss: -7.9065\n",
      "[ INFO : 2023-05-31 15:43:02,489 ] - Epoch 16/150 - time: 0.06 - training_loss: -7.9637\n",
      "[ INFO : 2023-05-31 15:43:02,549 ] - Epoch 17/150 - time: 0.06 - training_loss: -8.0145\n",
      "[ INFO : 2023-05-31 15:43:02,609 ] - Epoch 18/150 - time: 0.06 - training_loss: -8.0597\n",
      "[ INFO : 2023-05-31 15:43:02,670 ] - Epoch 19/150 - time: 0.06 - training_loss: -8.1004\n",
      "[ INFO : 2023-05-31 15:43:02,733 ] - Epoch 20/150 - time: 0.06 - training_loss: -8.1370\n",
      "[ INFO : 2023-05-31 15:43:02,793 ] - Epoch 21/150 - time: 0.06 - training_loss: -8.1702\n",
      "[ INFO : 2023-05-31 15:43:02,855 ] - Epoch 22/150 - time: 0.06 - training_loss: -8.2005\n",
      "[ INFO : 2023-05-31 15:43:02,912 ] - Epoch 23/150 - time: 0.06 - training_loss: -8.2283\n",
      "[ INFO : 2023-05-31 15:43:02,970 ] - Epoch 24/150 - time: 0.06 - training_loss: -8.2539\n",
      "[ INFO : 2023-05-31 15:43:03,028 ] - Epoch 25/150 - time: 0.06 - training_loss: -8.2775\n",
      "[ INFO : 2023-05-31 15:43:03,089 ] - Epoch 26/150 - time: 0.06 - training_loss: -8.2994\n",
      "[ INFO : 2023-05-31 15:43:03,150 ] - Epoch 27/150 - time: 0.06 - training_loss: -8.3197\n",
      "[ INFO : 2023-05-31 15:43:03,215 ] - Epoch 28/150 - time: 0.06 - training_loss: -8.3387\n",
      "[ INFO : 2023-05-31 15:43:03,278 ] - Epoch 29/150 - time: 0.06 - training_loss: -8.3563\n",
      "[ INFO : 2023-05-31 15:43:03,339 ] - Epoch 30/150 - time: 0.06 - training_loss: -8.3728\n",
      "[ INFO : 2023-05-31 15:43:03,399 ] - Epoch 31/150 - time: 0.06 - training_loss: -8.3883\n",
      "[ INFO : 2023-05-31 15:43:03,461 ] - Epoch 32/150 - time: 0.06 - training_loss: -8.4028\n",
      "[ INFO : 2023-05-31 15:43:03,520 ] - Epoch 33/150 - time: 0.06 - training_loss: -8.4165\n",
      "[ INFO : 2023-05-31 15:43:03,583 ] - Epoch 34/150 - time: 0.06 - training_loss: -8.4294\n",
      "[ INFO : 2023-05-31 15:43:03,650 ] - Epoch 35/150 - time: 0.07 - training_loss: -8.4415\n",
      "[ INFO : 2023-05-31 15:43:03,710 ] - Epoch 36/150 - time: 0.06 - training_loss: -8.4530\n",
      "[ INFO : 2023-05-31 15:43:03,770 ] - Epoch 37/150 - time: 0.06 - training_loss: -8.4639\n",
      "[ INFO : 2023-05-31 15:43:03,833 ] - Epoch 38/150 - time: 0.06 - training_loss: -8.4743\n",
      "[ INFO : 2023-05-31 15:43:03,891 ] - Epoch 39/150 - time: 0.06 - training_loss: -8.4841\n",
      "[ INFO : 2023-05-31 15:43:03,949 ] - Epoch 40/150 - time: 0.06 - training_loss: -8.4934\n",
      "[ INFO : 2023-05-31 15:43:04,007 ] - Epoch 41/150 - time: 0.06 - training_loss: -8.5023\n",
      "[ INFO : 2023-05-31 15:43:04,064 ] - Epoch 42/150 - time: 0.06 - training_loss: -8.5108\n",
      "[ INFO : 2023-05-31 15:43:04,123 ] - Epoch 43/150 - time: 0.06 - training_loss: -8.5189\n",
      "[ INFO : 2023-05-31 15:43:04,183 ] - Epoch 44/150 - time: 0.06 - training_loss: -8.5266\n",
      "[ INFO : 2023-05-31 15:43:04,245 ] - Epoch 45/150 - time: 0.06 - training_loss: -8.5340\n",
      "[ INFO : 2023-05-31 15:43:04,306 ] - Epoch 46/150 - time: 0.06 - training_loss: -8.5410\n",
      "[ INFO : 2023-05-31 15:43:04,367 ] - Epoch 47/150 - time: 0.06 - training_loss: -8.5478\n",
      "[ INFO : 2023-05-31 15:43:04,426 ] - Epoch 48/150 - time: 0.06 - training_loss: -8.5543\n",
      "[ INFO : 2023-05-31 15:43:04,486 ] - Epoch 49/150 - time: 0.06 - training_loss: -8.5605\n",
      "[ INFO : 2023-05-31 15:43:04,548 ] - Epoch 50/150 - time: 0.06 - training_loss: -8.5664\n",
      "[ INFO : 2023-05-31 15:43:04,608 ] - Epoch 51/150 - time: 0.06 - training_loss: -8.5722\n",
      "[ INFO : 2023-05-31 15:43:04,670 ] - Epoch 52/150 - time: 0.06 - training_loss: -8.5777\n",
      "[ INFO : 2023-05-31 15:43:04,730 ] - Epoch 53/150 - time: 0.06 - training_loss: -8.5830\n",
      "[ INFO : 2023-05-31 15:43:04,789 ] - Epoch 54/150 - time: 0.06 - training_loss: -8.5882\n",
      "[ INFO : 2023-05-31 15:43:04,849 ] - Epoch 55/150 - time: 0.06 - training_loss: -8.5931\n",
      "[ INFO : 2023-05-31 15:43:04,909 ] - Epoch 56/150 - time: 0.06 - training_loss: -8.5979\n",
      "[ INFO : 2023-05-31 15:43:04,966 ] - Epoch 57/150 - time: 0.06 - training_loss: -8.6025\n",
      "[ INFO : 2023-05-31 15:43:05,024 ] - Epoch 58/150 - time: 0.06 - training_loss: -8.6069\n",
      "[ INFO : 2023-05-31 15:43:05,082 ] - Epoch 59/150 - time: 0.06 - training_loss: -8.6112\n",
      "[ INFO : 2023-05-31 15:43:05,145 ] - Epoch 60/150 - time: 0.06 - training_loss: -8.6153\n",
      "[ INFO : 2023-05-31 15:43:05,206 ] - Epoch 61/150 - time: 0.06 - training_loss: -8.6193\n",
      "[ INFO : 2023-05-31 15:43:05,269 ] - Epoch 62/150 - time: 0.06 - training_loss: -8.6232\n",
      "[ INFO : 2023-05-31 15:43:05,328 ] - Epoch 63/150 - time: 0.06 - training_loss: -8.6270\n",
      "[ INFO : 2023-05-31 15:43:05,389 ] - Epoch 64/150 - time: 0.06 - training_loss: -8.6306\n",
      "[ INFO : 2023-05-31 15:43:05,464 ] - Epoch 65/150 - time: 0.07 - training_loss: -8.6342\n",
      "[ INFO : 2023-05-31 15:43:05,523 ] - Epoch 66/150 - time: 0.06 - training_loss: -8.6376\n",
      "[ INFO : 2023-05-31 15:43:05,583 ] - Epoch 67/150 - time: 0.06 - training_loss: -8.6409\n",
      "[ INFO : 2023-05-31 15:43:05,645 ] - Epoch 68/150 - time: 0.06 - training_loss: -8.6441\n",
      "[ INFO : 2023-05-31 15:43:05,704 ] - Epoch 69/150 - time: 0.06 - training_loss: -8.6473\n",
      "[ INFO : 2023-05-31 15:43:05,766 ] - Epoch 70/150 - time: 0.06 - training_loss: -8.6503\n",
      "[ INFO : 2023-05-31 15:43:05,826 ] - Epoch 71/150 - time: 0.06 - training_loss: -8.6533\n",
      "[ INFO : 2023-05-31 15:43:05,884 ] - Epoch 72/150 - time: 0.06 - training_loss: -8.6561\n",
      "[ INFO : 2023-05-31 15:43:05,943 ] - Epoch 73/150 - time: 0.06 - training_loss: -8.6589\n",
      "[ INFO : 2023-05-31 15:43:06,003 ] - Epoch 74/150 - time: 0.06 - training_loss: -8.6617\n",
      "[ INFO : 2023-05-31 15:43:06,061 ] - Epoch 75/150 - time: 0.06 - training_loss: -8.6643\n",
      "[ INFO : 2023-05-31 15:43:06,121 ] - Epoch 76/150 - time: 0.06 - training_loss: -8.6669\n",
      "[ INFO : 2023-05-31 15:43:06,180 ] - Epoch 77/150 - time: 0.06 - training_loss: -8.6694\n",
      "[ INFO : 2023-05-31 15:43:06,240 ] - Epoch 78/150 - time: 0.06 - training_loss: -8.6719\n",
      "[ INFO : 2023-05-31 15:43:06,301 ] - Epoch 79/150 - time: 0.06 - training_loss: -8.6742\n",
      "[ INFO : 2023-05-31 15:43:06,364 ] - Epoch 80/150 - time: 0.06 - training_loss: -8.6766\n",
      "[ INFO : 2023-05-31 15:43:06,424 ] - Epoch 81/150 - time: 0.06 - training_loss: -8.6788\n",
      "[ INFO : 2023-05-31 15:43:06,487 ] - Epoch 82/150 - time: 0.06 - training_loss: -8.6810\n",
      "[ INFO : 2023-05-31 15:43:06,547 ] - Epoch 83/150 - time: 0.06 - training_loss: -8.6832\n",
      "[ INFO : 2023-05-31 15:43:06,607 ] - Epoch 84/150 - time: 0.06 - training_loss: -8.6853\n",
      "[ INFO : 2023-05-31 15:43:06,667 ] - Epoch 85/150 - time: 0.06 - training_loss: -8.6874\n",
      "[ INFO : 2023-05-31 15:43:06,729 ] - Epoch 86/150 - time: 0.06 - training_loss: -8.6894\n",
      "[ INFO : 2023-05-31 15:43:06,789 ] - Epoch 87/150 - time: 0.06 - training_loss: -8.6914\n",
      "[ INFO : 2023-05-31 15:43:06,850 ] - Epoch 88/150 - time: 0.06 - training_loss: -8.6933\n",
      "[ INFO : 2023-05-31 15:43:06,907 ] - Epoch 89/150 - time: 0.06 - training_loss: -8.6952\n",
      "[ INFO : 2023-05-31 15:43:06,964 ] - Epoch 90/150 - time: 0.06 - training_loss: -8.6970\n",
      "[ INFO : 2023-05-31 15:43:07,022 ] - Epoch 91/150 - time: 0.06 - training_loss: -8.6988\n",
      "[ INFO : 2023-05-31 15:43:07,081 ] - Epoch 92/150 - time: 0.06 - training_loss: -8.7005\n",
      "[ INFO : 2023-05-31 15:43:07,140 ] - Epoch 93/150 - time: 0.06 - training_loss: -8.7023\n",
      "[ INFO : 2023-05-31 15:43:07,202 ] - Epoch 94/150 - time: 0.06 - training_loss: -8.7039\n",
      "[ INFO : 2023-05-31 15:43:07,262 ] - Epoch 95/150 - time: 0.06 - training_loss: -8.7056\n",
      "[ INFO : 2023-05-31 15:43:07,322 ] - Epoch 96/150 - time: 0.06 - training_loss: -8.7072\n",
      "[ INFO : 2023-05-31 15:43:07,388 ] - Epoch 97/150 - time: 0.06 - training_loss: -8.7088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2023-05-31 15:43:07,450 ] - Epoch 98/150 - time: 0.06 - training_loss: -8.7103\n",
      "[ INFO : 2023-05-31 15:43:07,510 ] - Epoch 99/150 - time: 0.06 - training_loss: -8.7118\n",
      "[ INFO : 2023-05-31 15:43:07,573 ] - Epoch 100/150 - time: 0.06 - training_loss: -8.7133\n",
      "[ INFO : 2023-05-31 15:43:07,633 ] - Epoch 101/150 - time: 0.06 - training_loss: -8.7148\n",
      "[ INFO : 2023-05-31 15:43:07,693 ] - Epoch 102/150 - time: 0.06 - training_loss: -8.7162\n",
      "[ INFO : 2023-05-31 15:43:07,753 ] - Epoch 103/150 - time: 0.06 - training_loss: -8.7176\n",
      "[ INFO : 2023-05-31 15:43:07,815 ] - Epoch 104/150 - time: 0.06 - training_loss: -8.7190\n",
      "[ INFO : 2023-05-31 15:43:07,888 ] - Epoch 105/150 - time: 0.07 - training_loss: -8.7203\n",
      "[ INFO : 2023-05-31 15:43:07,947 ] - Epoch 106/150 - time: 0.06 - training_loss: -8.7217\n",
      "[ INFO : 2023-05-31 15:43:08,004 ] - Epoch 107/150 - time: 0.06 - training_loss: -8.7230\n",
      "[ INFO : 2023-05-31 15:43:08,062 ] - Epoch 108/150 - time: 0.06 - training_loss: -8.7242\n",
      "[ INFO : 2023-05-31 15:43:08,119 ] - Epoch 109/150 - time: 0.06 - training_loss: -8.7255\n",
      "[ INFO : 2023-05-31 15:43:08,180 ] - Epoch 110/150 - time: 0.06 - training_loss: -8.7267\n",
      "[ INFO : 2023-05-31 15:43:08,239 ] - Epoch 111/150 - time: 0.06 - training_loss: -8.7279\n",
      "[ INFO : 2023-05-31 15:43:08,301 ] - Epoch 112/150 - time: 0.06 - training_loss: -8.7291\n",
      "[ INFO : 2023-05-31 15:43:08,363 ] - Epoch 113/150 - time: 0.06 - training_loss: -8.7302\n",
      "[ INFO : 2023-05-31 15:43:08,422 ] - Epoch 114/150 - time: 0.06 - training_loss: -8.7314\n",
      "[ INFO : 2023-05-31 15:43:08,481 ] - Epoch 115/150 - time: 0.06 - training_loss: -8.7325\n",
      "[ INFO : 2023-05-31 15:43:08,542 ] - Epoch 116/150 - time: 0.06 - training_loss: -8.7336\n",
      "[ INFO : 2023-05-31 15:43:08,602 ] - Epoch 117/150 - time: 0.06 - training_loss: -8.7347\n",
      "[ INFO : 2023-05-31 15:43:08,664 ] - Epoch 118/150 - time: 0.06 - training_loss: -8.7358\n",
      "[ INFO : 2023-05-31 15:43:08,725 ] - Epoch 119/150 - time: 0.06 - training_loss: -8.7368\n",
      "[ INFO : 2023-05-31 15:43:08,785 ] - Epoch 120/150 - time: 0.06 - training_loss: -8.7378\n",
      "[ INFO : 2023-05-31 15:43:08,844 ] - Epoch 121/150 - time: 0.06 - training_loss: -8.7389\n",
      "[ INFO : 2023-05-31 15:43:08,903 ] - Epoch 122/150 - time: 0.06 - training_loss: -8.7399\n",
      "[ INFO : 2023-05-31 15:43:08,960 ] - Epoch 123/150 - time: 0.06 - training_loss: -8.7408\n",
      "[ INFO : 2023-05-31 15:43:09,019 ] - Epoch 124/150 - time: 0.06 - training_loss: -8.7418\n",
      "[ INFO : 2023-05-31 15:43:09,077 ] - Epoch 125/150 - time: 0.06 - training_loss: -8.7427\n",
      "[ INFO : 2023-05-31 15:43:09,135 ] - Epoch 126/150 - time: 0.06 - training_loss: -8.7437\n",
      "[ INFO : 2023-05-31 15:43:09,194 ] - Epoch 127/150 - time: 0.06 - training_loss: -8.7446\n",
      "[ INFO : 2023-05-31 15:43:09,257 ] - Epoch 128/150 - time: 0.06 - training_loss: -8.7455\n",
      "[ INFO : 2023-05-31 15:43:09,316 ] - Epoch 129/150 - time: 0.06 - training_loss: -8.7464\n",
      "[ INFO : 2023-05-31 15:43:09,379 ] - Epoch 130/150 - time: 0.06 - training_loss: -8.7473\n",
      "[ INFO : 2023-05-31 15:43:09,439 ] - Epoch 131/150 - time: 0.06 - training_loss: -8.7481\n",
      "[ INFO : 2023-05-31 15:43:09,499 ] - Epoch 132/150 - time: 0.06 - training_loss: -8.7490\n",
      "[ INFO : 2023-05-31 15:43:09,558 ] - Epoch 133/150 - time: 0.06 - training_loss: -8.7498\n",
      "[ INFO : 2023-05-31 15:43:09,621 ] - Epoch 134/150 - time: 0.06 - training_loss: -8.7506\n",
      "[ INFO : 2023-05-31 15:43:09,681 ] - Epoch 135/150 - time: 0.06 - training_loss: -8.7515\n",
      "[ INFO : 2023-05-31 15:43:09,743 ] - Epoch 136/150 - time: 0.06 - training_loss: -8.7523\n",
      "[ INFO : 2023-05-31 15:43:09,803 ] - Epoch 137/150 - time: 0.06 - training_loss: -8.7531\n",
      "[ INFO : 2023-05-31 15:43:09,862 ] - Epoch 138/150 - time: 0.06 - training_loss: -8.7538\n",
      "[ INFO : 2023-05-31 15:43:09,920 ] - Epoch 139/150 - time: 0.06 - training_loss: -8.7546\n",
      "[ INFO : 2023-05-31 15:43:09,979 ] - Epoch 140/150 - time: 0.06 - training_loss: -8.7554\n",
      "[ INFO : 2023-05-31 15:43:10,036 ] - Epoch 141/150 - time: 0.06 - training_loss: -8.7561\n",
      "[ INFO : 2023-05-31 15:43:10,095 ] - Epoch 142/150 - time: 0.06 - training_loss: -8.7568\n",
      "[ INFO : 2023-05-31 15:43:10,154 ] - Epoch 143/150 - time: 0.06 - training_loss: -8.7576\n",
      "[ INFO : 2023-05-31 15:43:10,213 ] - Epoch 144/150 - time: 0.06 - training_loss: -8.7583\n",
      "[ INFO : 2023-05-31 15:43:10,275 ] - Epoch 145/150 - time: 0.06 - training_loss: -8.7590\n",
      "[ INFO : 2023-05-31 15:43:10,337 ] - Epoch 146/150 - time: 0.06 - training_loss: -8.7597\n",
      "[ INFO : 2023-05-31 15:43:10,397 ] - Epoch 147/150 - time: 0.06 - training_loss: -8.7604\n",
      "[ INFO : 2023-05-31 15:43:10,460 ] - Epoch 148/150 - time: 0.06 - training_loss: -8.7610\n",
      "[ INFO : 2023-05-31 15:43:10,520 ] - Epoch 149/150 - time: 0.06 - training_loss: -8.7617\n",
      "[ INFO : 2023-05-31 15:43:10,578 ] - Epoch 150/150 - time: 0.06 - training_loss: -8.7624\n",
      "[ INFO : 2023-05-31 15:43:10,623 ] - DeepGCCA(\n",
      "  (model_list): ModuleList(\n",
      "    (0-2): 3 x MlpNet(\n",
      "      (layers): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
      "          (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[ INFO : 2023-05-31 15:43:10,629 ] - Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.05\n",
      "    lr: 0.05\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "[ INFO : 2023-05-31 15:43:10,695 ] - Epoch 1/150 - time: 0.06 - training_loss: -3.4656\n",
      "[ INFO : 2023-05-31 15:43:10,756 ] - Epoch 2/150 - time: 0.06 - training_loss: -4.8480\n",
      "[ INFO : 2023-05-31 15:43:10,817 ] - Epoch 3/150 - time: 0.06 - training_loss: -5.5232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear CCA started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2023-05-31 15:43:10,876 ] - Epoch 4/150 - time: 0.06 - training_loss: -6.2189\n",
      "[ INFO : 2023-05-31 15:43:10,934 ] - Epoch 5/150 - time: 0.06 - training_loss: -6.6850\n",
      "[ INFO : 2023-05-31 15:43:10,991 ] - Epoch 6/150 - time: 0.06 - training_loss: -7.0142\n",
      "[ INFO : 2023-05-31 15:43:11,050 ] - Epoch 7/150 - time: 0.06 - training_loss: -7.2620\n",
      "[ INFO : 2023-05-31 15:43:11,107 ] - Epoch 8/150 - time: 0.06 - training_loss: -7.4467\n",
      "[ INFO : 2023-05-31 15:43:11,169 ] - Epoch 9/150 - time: 0.06 - training_loss: -7.5880\n",
      "[ INFO : 2023-05-31 15:43:11,229 ] - Epoch 10/150 - time: 0.06 - training_loss: -7.7029\n",
      "[ INFO : 2023-05-31 15:43:11,288 ] - Epoch 11/150 - time: 0.06 - training_loss: -7.8003\n",
      "[ INFO : 2023-05-31 15:43:11,350 ] - Epoch 12/150 - time: 0.06 - training_loss: -7.8841\n",
      "[ INFO : 2023-05-31 15:43:11,410 ] - Epoch 13/150 - time: 0.06 - training_loss: -7.9561\n",
      "[ INFO : 2023-05-31 15:43:11,469 ] - Epoch 14/150 - time: 0.06 - training_loss: -8.0178\n",
      "[ INFO : 2023-05-31 15:43:11,531 ] - Epoch 15/150 - time: 0.06 - training_loss: -8.0708\n",
      "[ INFO : 2023-05-31 15:43:11,591 ] - Epoch 16/150 - time: 0.06 - training_loss: -8.1172\n",
      "[ INFO : 2023-05-31 15:43:11,650 ] - Epoch 17/150 - time: 0.06 - training_loss: -8.1585\n",
      "[ INFO : 2023-05-31 15:43:11,711 ] - Epoch 18/150 - time: 0.06 - training_loss: -8.1957\n",
      "[ INFO : 2023-05-31 15:43:11,772 ] - Epoch 19/150 - time: 0.06 - training_loss: -8.2293\n",
      "[ INFO : 2023-05-31 15:43:11,847 ] - Epoch 20/150 - time: 0.07 - training_loss: -8.2597\n",
      "[ INFO : 2023-05-31 15:43:11,907 ] - Epoch 21/150 - time: 0.06 - training_loss: -8.2871\n",
      "[ INFO : 2023-05-31 15:43:11,965 ] - Epoch 22/150 - time: 0.06 - training_loss: -8.3121\n",
      "[ INFO : 2023-05-31 15:43:12,024 ] - Epoch 23/150 - time: 0.06 - training_loss: -8.3351\n",
      "[ INFO : 2023-05-31 15:43:12,083 ] - Epoch 24/150 - time: 0.06 - training_loss: -8.3564\n",
      "[ INFO : 2023-05-31 15:43:12,141 ] - Epoch 25/150 - time: 0.06 - training_loss: -8.3762\n",
      "[ INFO : 2023-05-31 15:43:12,199 ] - Epoch 26/150 - time: 0.06 - training_loss: -8.3945\n",
      "[ INFO : 2023-05-31 15:43:12,261 ] - Epoch 27/150 - time: 0.06 - training_loss: -8.4115\n",
      "[ INFO : 2023-05-31 15:43:12,321 ] - Epoch 28/150 - time: 0.06 - training_loss: -8.4273\n",
      "[ INFO : 2023-05-31 15:43:12,381 ] - Epoch 29/150 - time: 0.06 - training_loss: -8.4419\n",
      "[ INFO : 2023-05-31 15:43:12,443 ] - Epoch 30/150 - time: 0.06 - training_loss: -8.4555\n",
      "[ INFO : 2023-05-31 15:43:12,503 ] - Epoch 31/150 - time: 0.06 - training_loss: -8.4682\n",
      "[ INFO : 2023-05-31 15:43:12,564 ] - Epoch 32/150 - time: 0.06 - training_loss: -8.4803\n",
      "[ INFO : 2023-05-31 15:43:12,625 ] - Epoch 33/150 - time: 0.06 - training_loss: -8.4916\n",
      "[ INFO : 2023-05-31 15:43:12,685 ] - Epoch 34/150 - time: 0.06 - training_loss: -8.5023\n",
      "[ INFO : 2023-05-31 15:43:12,744 ] - Epoch 35/150 - time: 0.06 - training_loss: -8.5124\n",
      "[ INFO : 2023-05-31 15:43:12,807 ] - Epoch 36/150 - time: 0.06 - training_loss: -8.5220\n",
      "[ INFO : 2023-05-31 15:43:12,866 ] - Epoch 37/150 - time: 0.06 - training_loss: -8.5310\n",
      "[ INFO : 2023-05-31 15:43:12,924 ] - Epoch 38/150 - time: 0.06 - training_loss: -8.5396\n",
      "[ INFO : 2023-05-31 15:43:12,983 ] - Epoch 39/150 - time: 0.06 - training_loss: -8.5478\n",
      "[ INFO : 2023-05-31 15:43:13,042 ] - Epoch 40/150 - time: 0.06 - training_loss: -8.5555\n",
      "[ INFO : 2023-05-31 15:43:13,099 ] - Epoch 41/150 - time: 0.06 - training_loss: -8.5629\n",
      "[ INFO : 2023-05-31 15:43:13,159 ] - Epoch 42/150 - time: 0.06 - training_loss: -8.5699\n",
      "[ INFO : 2023-05-31 15:43:13,218 ] - Epoch 43/150 - time: 0.06 - training_loss: -8.5766\n",
      "[ INFO : 2023-05-31 15:43:13,278 ] - Epoch 44/150 - time: 0.06 - training_loss: -8.5830\n",
      "[ INFO : 2023-05-31 15:43:13,339 ] - Epoch 45/150 - time: 0.06 - training_loss: -8.5892\n",
      "[ INFO : 2023-05-31 15:43:13,399 ] - Epoch 46/150 - time: 0.06 - training_loss: -8.5950\n",
      "[ INFO : 2023-05-31 15:43:13,459 ] - Epoch 47/150 - time: 0.06 - training_loss: -8.6006\n",
      "[ INFO : 2023-05-31 15:43:13,521 ] - Epoch 48/150 - time: 0.06 - training_loss: -8.6060\n",
      "[ INFO : 2023-05-31 15:43:13,580 ] - Epoch 49/150 - time: 0.06 - training_loss: -8.6112\n",
      "[ INFO : 2023-05-31 15:43:13,640 ] - Epoch 50/150 - time: 0.06 - training_loss: -8.6162\n",
      "[ INFO : 2023-05-31 15:43:13,703 ] - Epoch 51/150 - time: 0.06 - training_loss: -8.6209\n",
      "[ INFO : 2023-05-31 15:43:13,763 ] - Epoch 52/150 - time: 0.06 - training_loss: -8.6255\n",
      "[ INFO : 2023-05-31 15:43:13,822 ] - Epoch 53/150 - time: 0.06 - training_loss: -8.6300\n",
      "[ INFO : 2023-05-31 15:43:13,883 ] - Epoch 54/150 - time: 0.06 - training_loss: -8.6342\n",
      "[ INFO : 2023-05-31 15:43:13,941 ] - Epoch 55/150 - time: 0.06 - training_loss: -8.6383\n",
      "[ INFO : 2023-05-31 15:43:13,999 ] - Epoch 56/150 - time: 0.06 - training_loss: -8.6423\n",
      "[ INFO : 2023-05-31 15:43:14,057 ] - Epoch 57/150 - time: 0.06 - training_loss: -8.6461\n",
      "[ INFO : 2023-05-31 15:43:14,115 ] - Epoch 58/150 - time: 0.06 - training_loss: -8.6498\n",
      "[ INFO : 2023-05-31 15:43:14,177 ] - Epoch 59/150 - time: 0.06 - training_loss: -8.6533\n",
      "[ INFO : 2023-05-31 15:43:14,239 ] - Epoch 60/150 - time: 0.06 - training_loss: -8.6568\n",
      "[ INFO : 2023-05-31 15:43:14,297 ] - Epoch 61/150 - time: 0.06 - training_loss: -8.6601\n",
      "[ INFO : 2023-05-31 15:43:14,357 ] - Epoch 62/150 - time: 0.06 - training_loss: -8.6633\n",
      "[ INFO : 2023-05-31 15:43:14,419 ] - Epoch 63/150 - time: 0.06 - training_loss: -8.6665\n",
      "[ INFO : 2023-05-31 15:43:14,479 ] - Epoch 64/150 - time: 0.06 - training_loss: -8.6695\n",
      "[ INFO : 2023-05-31 15:43:14,540 ] - Epoch 65/150 - time: 0.06 - training_loss: -8.6724\n",
      "[ INFO : 2023-05-31 15:43:14,602 ] - Epoch 66/150 - time: 0.06 - training_loss: -8.6753\n",
      "[ INFO : 2023-05-31 15:43:14,661 ] - Epoch 67/150 - time: 0.06 - training_loss: -8.6780\n",
      "[ INFO : 2023-05-31 15:43:14,721 ] - Epoch 68/150 - time: 0.06 - training_loss: -8.6807\n",
      "[ INFO : 2023-05-31 15:43:14,783 ] - Epoch 69/150 - time: 0.06 - training_loss: -8.6833\n",
      "[ INFO : 2023-05-31 15:43:14,842 ] - Epoch 70/150 - time: 0.06 - training_loss: -8.6858\n",
      "[ INFO : 2023-05-31 15:43:14,901 ] - Epoch 71/150 - time: 0.06 - training_loss: -8.6883\n",
      "[ INFO : 2023-05-31 15:43:14,961 ] - Epoch 72/150 - time: 0.06 - training_loss: -8.6907\n",
      "[ INFO : 2023-05-31 15:43:15,019 ] - Epoch 73/150 - time: 0.06 - training_loss: -8.6930\n",
      "[ INFO : 2023-05-31 15:43:15,076 ] - Epoch 74/150 - time: 0.06 - training_loss: -8.6953\n",
      "[ INFO : 2023-05-31 15:43:15,135 ] - Epoch 75/150 - time: 0.06 - training_loss: -8.6975\n",
      "[ INFO : 2023-05-31 15:43:15,193 ] - Epoch 76/150 - time: 0.06 - training_loss: -8.6996\n",
      "[ INFO : 2023-05-31 15:43:15,253 ] - Epoch 77/150 - time: 0.06 - training_loss: -8.7017\n",
      "[ INFO : 2023-05-31 15:43:15,318 ] - Epoch 78/150 - time: 0.06 - training_loss: -8.7037\n",
      "[ INFO : 2023-05-31 15:43:15,377 ] - Epoch 79/150 - time: 0.06 - training_loss: -8.7057\n",
      "[ INFO : 2023-05-31 15:43:15,437 ] - Epoch 80/150 - time: 0.06 - training_loss: -8.7077\n",
      "[ INFO : 2023-05-31 15:43:15,500 ] - Epoch 81/150 - time: 0.06 - training_loss: -8.7095\n",
      "[ INFO : 2023-05-31 15:43:15,560 ] - Epoch 82/150 - time: 0.06 - training_loss: -8.7114\n",
      "[ INFO : 2023-05-31 15:43:15,619 ] - Epoch 83/150 - time: 0.06 - training_loss: -8.7132\n",
      "[ INFO : 2023-05-31 15:43:15,681 ] - Epoch 84/150 - time: 0.06 - training_loss: -8.7149\n",
      "[ INFO : 2023-05-31 15:43:15,740 ] - Epoch 85/150 - time: 0.06 - training_loss: -8.7166\n",
      "[ INFO : 2023-05-31 15:43:15,801 ] - Epoch 86/150 - time: 0.06 - training_loss: -8.7183\n",
      "[ INFO : 2023-05-31 15:43:15,862 ] - Epoch 87/150 - time: 0.06 - training_loss: -8.7199\n",
      "[ INFO : 2023-05-31 15:43:15,919 ] - Epoch 88/150 - time: 0.06 - training_loss: -8.7215\n",
      "[ INFO : 2023-05-31 15:43:15,976 ] - Epoch 89/150 - time: 0.06 - training_loss: -8.7231\n",
      "[ INFO : 2023-05-31 15:43:16,036 ] - Epoch 90/150 - time: 0.06 - training_loss: -8.7246\n",
      "[ INFO : 2023-05-31 15:43:16,094 ] - Epoch 91/150 - time: 0.06 - training_loss: -8.7261\n",
      "[ INFO : 2023-05-31 15:43:16,152 ] - Epoch 92/150 - time: 0.06 - training_loss: -8.7276\n",
      "[ INFO : 2023-05-31 15:43:16,212 ] - Epoch 93/150 - time: 0.06 - training_loss: -8.7290\n",
      "[ INFO : 2023-05-31 15:43:16,272 ] - Epoch 94/150 - time: 0.06 - training_loss: -8.7304\n",
      "[ INFO : 2023-05-31 15:43:16,332 ] - Epoch 95/150 - time: 0.06 - training_loss: -8.7318\n",
      "[ INFO : 2023-05-31 15:43:16,394 ] - Epoch 96/150 - time: 0.06 - training_loss: -8.7331\n",
      "[ INFO : 2023-05-31 15:43:16,455 ] - Epoch 97/150 - time: 0.06 - training_loss: -8.7344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2023-05-31 15:43:16,515 ] - Epoch 98/150 - time: 0.06 - training_loss: -8.7357\n",
      "[ INFO : 2023-05-31 15:43:16,576 ] - Epoch 99/150 - time: 0.06 - training_loss: -8.7370\n",
      "[ INFO : 2023-05-31 15:43:16,636 ] - Epoch 100/150 - time: 0.06 - training_loss: -8.7382\n",
      "[ INFO : 2023-05-31 15:43:16,695 ] - Epoch 101/150 - time: 0.06 - training_loss: -8.7394\n",
      "[ INFO : 2023-05-31 15:43:16,758 ] - Epoch 102/150 - time: 0.06 - training_loss: -8.7406\n",
      "[ INFO : 2023-05-31 15:43:16,817 ] - Epoch 103/150 - time: 0.06 - training_loss: -8.7418\n",
      "[ INFO : 2023-05-31 15:43:16,875 ] - Epoch 104/150 - time: 0.06 - training_loss: -8.7429\n",
      "[ INFO : 2023-05-31 15:43:16,934 ] - Epoch 105/150 - time: 0.06 - training_loss: -8.7440\n",
      "[ INFO : 2023-05-31 15:43:16,992 ] - Epoch 106/150 - time: 0.06 - training_loss: -8.7451\n",
      "[ INFO : 2023-05-31 15:43:17,051 ] - Epoch 107/150 - time: 0.06 - training_loss: -8.7462\n",
      "[ INFO : 2023-05-31 15:43:17,110 ] - Epoch 108/150 - time: 0.06 - training_loss: -8.7473\n",
      "[ INFO : 2023-05-31 15:43:17,168 ] - Epoch 109/150 - time: 0.06 - training_loss: -8.7483\n",
      "[ INFO : 2023-05-31 15:43:17,228 ] - Epoch 110/150 - time: 0.06 - training_loss: -8.7493\n",
      "[ INFO : 2023-05-31 15:43:17,290 ] - Epoch 111/150 - time: 0.06 - training_loss: -8.7503\n",
      "[ INFO : 2023-05-31 15:43:17,350 ] - Epoch 112/150 - time: 0.06 - training_loss: -8.7513\n",
      "[ INFO : 2023-05-31 15:43:17,409 ] - Epoch 113/150 - time: 0.06 - training_loss: -8.7523\n",
      "[ INFO : 2023-05-31 15:43:17,470 ] - Epoch 114/150 - time: 0.06 - training_loss: -8.7532\n",
      "[ INFO : 2023-05-31 15:43:17,531 ] - Epoch 115/150 - time: 0.06 - training_loss: -8.7541\n",
      "[ INFO : 2023-05-31 15:43:17,591 ] - Epoch 116/150 - time: 0.06 - training_loss: -8.7551\n",
      "[ INFO : 2023-05-31 15:43:17,653 ] - Epoch 117/150 - time: 0.06 - training_loss: -8.7560\n",
      "[ INFO : 2023-05-31 15:43:17,718 ] - Epoch 118/150 - time: 0.06 - training_loss: -8.7568\n",
      "[ INFO : 2023-05-31 15:43:17,777 ] - Epoch 119/150 - time: 0.06 - training_loss: -8.7577\n",
      "[ INFO : 2023-05-31 15:43:17,840 ] - Epoch 120/150 - time: 0.06 - training_loss: -8.7586\n",
      "[ INFO : 2023-05-31 15:43:17,899 ] - Epoch 121/150 - time: 0.06 - training_loss: -8.7594\n",
      "[ INFO : 2023-05-31 15:43:17,956 ] - Epoch 122/150 - time: 0.05 - training_loss: -8.7602\n",
      "[ INFO : 2023-05-31 15:43:18,015 ] - Epoch 123/150 - time: 0.06 - training_loss: -8.7611\n",
      "[ INFO : 2023-05-31 15:43:18,073 ] - Epoch 124/150 - time: 0.06 - training_loss: -8.7619\n",
      "[ INFO : 2023-05-31 15:43:18,131 ] - Epoch 125/150 - time: 0.06 - training_loss: -8.7626\n",
      "[ INFO : 2023-05-31 15:43:18,192 ] - Epoch 126/150 - time: 0.06 - training_loss: -8.7634\n",
      "[ INFO : 2023-05-31 15:43:18,252 ] - Epoch 127/150 - time: 0.06 - training_loss: -8.7642\n",
      "[ INFO : 2023-05-31 15:43:18,311 ] - Epoch 128/150 - time: 0.06 - training_loss: -8.7649\n",
      "[ INFO : 2023-05-31 15:43:18,373 ] - Epoch 129/150 - time: 0.06 - training_loss: -8.7657\n",
      "[ INFO : 2023-05-31 15:43:18,432 ] - Epoch 130/150 - time: 0.06 - training_loss: -8.7664\n",
      "[ INFO : 2023-05-31 15:43:18,491 ] - Epoch 131/150 - time: 0.06 - training_loss: -8.7671\n",
      "[ INFO : 2023-05-31 15:43:18,554 ] - Epoch 132/150 - time: 0.06 - training_loss: -8.7678\n",
      "[ INFO : 2023-05-31 15:43:18,614 ] - Epoch 133/150 - time: 0.06 - training_loss: -8.7685\n",
      "[ INFO : 2023-05-31 15:43:18,675 ] - Epoch 134/150 - time: 0.06 - training_loss: -8.7692\n",
      "[ INFO : 2023-05-31 15:43:18,737 ] - Epoch 135/150 - time: 0.06 - training_loss: -8.7699\n",
      "[ INFO : 2023-05-31 15:43:18,796 ] - Epoch 136/150 - time: 0.06 - training_loss: -8.7706\n",
      "[ INFO : 2023-05-31 15:43:18,856 ] - Epoch 137/150 - time: 0.06 - training_loss: -8.7712\n",
      "[ INFO : 2023-05-31 15:43:18,915 ] - Epoch 138/150 - time: 0.06 - training_loss: -8.7719\n",
      "[ INFO : 2023-05-31 15:43:18,973 ] - Epoch 139/150 - time: 0.06 - training_loss: -8.7725\n",
      "[ INFO : 2023-05-31 15:43:19,031 ] - Epoch 140/150 - time: 0.06 - training_loss: -8.7731\n",
      "[ INFO : 2023-05-31 15:43:19,090 ] - Epoch 141/150 - time: 0.06 - training_loss: -8.7737\n",
      "[ INFO : 2023-05-31 15:43:19,148 ] - Epoch 142/150 - time: 0.06 - training_loss: -8.7743\n",
      "[ INFO : 2023-05-31 15:43:19,207 ] - Epoch 143/150 - time: 0.06 - training_loss: -8.7749\n",
      "[ INFO : 2023-05-31 15:43:19,269 ] - Epoch 144/150 - time: 0.06 - training_loss: -8.7755\n",
      "[ INFO : 2023-05-31 15:43:19,328 ] - Epoch 145/150 - time: 0.06 - training_loss: -8.7761\n",
      "[ INFO : 2023-05-31 15:43:19,387 ] - Epoch 146/150 - time: 0.06 - training_loss: -8.7767\n",
      "[ INFO : 2023-05-31 15:43:19,449 ] - Epoch 147/150 - time: 0.06 - training_loss: -8.7773\n",
      "[ INFO : 2023-05-31 15:43:19,509 ] - Epoch 148/150 - time: 0.06 - training_loss: -8.7778\n",
      "[ INFO : 2023-05-31 15:43:19,569 ] - Epoch 149/150 - time: 0.06 - training_loss: -8.7784\n",
      "[ INFO : 2023-05-31 15:43:19,630 ] - Epoch 150/150 - time: 0.06 - training_loss: -8.7789\n",
      "[ INFO : 2023-05-31 15:43:19,688 ] - DeepGCCA(\n",
      "  (model_list): ModuleList(\n",
      "    (0-2): 3 x MlpNet(\n",
      "      (layers): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
      "          (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[ INFO : 2023-05-31 15:43:19,694 ] - Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.05\n",
      "    lr: 0.05\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "[ INFO : 2023-05-31 15:43:19,761 ] - Epoch 1/150 - time: 0.06 - training_loss: -2.7625\n",
      "[ INFO : 2023-05-31 15:43:19,821 ] - Epoch 2/150 - time: 0.06 - training_loss: -3.0695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear CCA started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2023-05-31 15:43:19,881 ] - Epoch 3/150 - time: 0.06 - training_loss: -4.3649\n",
      "[ INFO : 2023-05-31 15:43:19,939 ] - Epoch 4/150 - time: 0.06 - training_loss: -5.2730\n",
      "[ INFO : 2023-05-31 15:43:19,997 ] - Epoch 5/150 - time: 0.06 - training_loss: -5.9106\n",
      "[ INFO : 2023-05-31 15:43:20,057 ] - Epoch 6/150 - time: 0.06 - training_loss: -6.3594\n",
      "[ INFO : 2023-05-31 15:43:20,114 ] - Epoch 7/150 - time: 0.06 - training_loss: -6.6866\n",
      "[ INFO : 2023-05-31 15:43:20,175 ] - Epoch 8/150 - time: 0.06 - training_loss: -6.9319\n",
      "[ INFO : 2023-05-31 15:43:20,235 ] - Epoch 9/150 - time: 0.06 - training_loss: -7.1251\n",
      "[ INFO : 2023-05-31 15:43:20,295 ] - Epoch 10/150 - time: 0.06 - training_loss: -7.2856\n",
      "[ INFO : 2023-05-31 15:43:20,356 ] - Epoch 11/150 - time: 0.06 - training_loss: -7.4212\n",
      "[ INFO : 2023-05-31 15:43:20,419 ] - Epoch 12/150 - time: 0.06 - training_loss: -7.5358\n",
      "[ INFO : 2023-05-31 15:43:20,479 ] - Epoch 13/150 - time: 0.06 - training_loss: -7.6335\n",
      "[ INFO : 2023-05-31 15:43:20,541 ] - Epoch 14/150 - time: 0.06 - training_loss: -7.7176\n",
      "[ INFO : 2023-05-31 15:43:20,601 ] - Epoch 15/150 - time: 0.06 - training_loss: -7.7909\n",
      "[ INFO : 2023-05-31 15:43:20,662 ] - Epoch 16/150 - time: 0.06 - training_loss: -7.8553\n",
      "[ INFO : 2023-05-31 15:43:20,722 ] - Epoch 17/150 - time: 0.06 - training_loss: -7.9124\n",
      "[ INFO : 2023-05-31 15:43:20,785 ] - Epoch 18/150 - time: 0.06 - training_loss: -7.9634\n",
      "[ INFO : 2023-05-31 15:43:20,846 ] - Epoch 19/150 - time: 0.06 - training_loss: -8.0092\n",
      "[ INFO : 2023-05-31 15:43:20,905 ] - Epoch 20/150 - time: 0.06 - training_loss: -8.0507\n",
      "[ INFO : 2023-05-31 15:43:20,963 ] - Epoch 21/150 - time: 0.06 - training_loss: -8.0882\n",
      "[ INFO : 2023-05-31 15:43:21,020 ] - Epoch 22/150 - time: 0.06 - training_loss: -8.1224\n",
      "[ INFO : 2023-05-31 15:43:21,079 ] - Epoch 23/150 - time: 0.06 - training_loss: -8.1536\n",
      "[ INFO : 2023-05-31 15:43:21,139 ] - Epoch 24/150 - time: 0.06 - training_loss: -8.1823\n",
      "[ INFO : 2023-05-31 15:43:21,198 ] - Epoch 25/150 - time: 0.06 - training_loss: -8.2088\n",
      "[ INFO : 2023-05-31 15:43:21,260 ] - Epoch 26/150 - time: 0.06 - training_loss: -8.2334\n",
      "[ INFO : 2023-05-31 15:43:21,322 ] - Epoch 27/150 - time: 0.06 - training_loss: -8.2562\n",
      "[ INFO : 2023-05-31 15:43:21,382 ] - Epoch 28/150 - time: 0.06 - training_loss: -8.2775\n",
      "[ INFO : 2023-05-31 15:43:21,442 ] - Epoch 29/150 - time: 0.06 - training_loss: -8.2973\n",
      "[ INFO : 2023-05-31 15:43:21,505 ] - Epoch 30/150 - time: 0.06 - training_loss: -8.3159\n",
      "[ INFO : 2023-05-31 15:43:21,566 ] - Epoch 31/150 - time: 0.06 - training_loss: -8.3333\n",
      "[ INFO : 2023-05-31 15:43:21,629 ] - Epoch 32/150 - time: 0.06 - training_loss: -8.3495\n",
      "[ INFO : 2023-05-31 15:43:21,689 ] - Epoch 33/150 - time: 0.06 - training_loss: -8.3648\n",
      "[ INFO : 2023-05-31 15:43:21,750 ] - Epoch 34/150 - time: 0.06 - training_loss: -8.3792\n",
      "[ INFO : 2023-05-31 15:43:21,813 ] - Epoch 35/150 - time: 0.06 - training_loss: -8.3928\n",
      "[ INFO : 2023-05-31 15:43:21,876 ] - Epoch 36/150 - time: 0.06 - training_loss: -8.4057\n",
      "[ INFO : 2023-05-31 15:43:21,933 ] - Epoch 37/150 - time: 0.06 - training_loss: -8.4179\n",
      "[ INFO : 2023-05-31 15:43:21,992 ] - Epoch 38/150 - time: 0.06 - training_loss: -8.4294\n",
      "[ INFO : 2023-05-31 15:43:22,050 ] - Epoch 39/150 - time: 0.06 - training_loss: -8.4404\n",
      "[ INFO : 2023-05-31 15:43:22,108 ] - Epoch 40/150 - time: 0.06 - training_loss: -8.4508\n",
      "[ INFO : 2023-05-31 15:43:22,167 ] - Epoch 41/150 - time: 0.06 - training_loss: -8.4608\n",
      "[ INFO : 2023-05-31 15:43:22,230 ] - Epoch 42/150 - time: 0.06 - training_loss: -8.4702\n",
      "[ INFO : 2023-05-31 15:43:22,291 ] - Epoch 43/150 - time: 0.06 - training_loss: -8.4792\n",
      "[ INFO : 2023-05-31 15:43:22,353 ] - Epoch 44/150 - time: 0.06 - training_loss: -8.4879\n",
      "[ INFO : 2023-05-31 15:43:22,413 ] - Epoch 45/150 - time: 0.06 - training_loss: -8.4961\n",
      "[ INFO : 2023-05-31 15:43:22,473 ] - Epoch 46/150 - time: 0.06 - training_loss: -8.5040\n",
      "[ INFO : 2023-05-31 15:43:22,534 ] - Epoch 47/150 - time: 0.06 - training_loss: -8.5116\n",
      "[ INFO : 2023-05-31 15:43:22,598 ] - Epoch 48/150 - time: 0.06 - training_loss: -8.5188\n",
      "[ INFO : 2023-05-31 15:43:22,660 ] - Epoch 49/150 - time: 0.06 - training_loss: -8.5257\n",
      "[ INFO : 2023-05-31 15:43:22,722 ] - Epoch 50/150 - time: 0.06 - training_loss: -8.5324\n",
      "[ INFO : 2023-05-31 15:43:22,790 ] - Epoch 51/150 - time: 0.07 - training_loss: -8.5388\n",
      "[ INFO : 2023-05-31 15:43:22,852 ] - Epoch 52/150 - time: 0.06 - training_loss: -8.5450\n",
      "[ INFO : 2023-05-31 15:43:22,911 ] - Epoch 53/150 - time: 0.06 - training_loss: -8.5509\n",
      "[ INFO : 2023-05-31 15:43:22,970 ] - Epoch 54/150 - time: 0.06 - training_loss: -8.5567\n",
      "[ INFO : 2023-05-31 15:43:23,028 ] - Epoch 55/150 - time: 0.06 - training_loss: -8.5622\n",
      "[ INFO : 2023-05-31 15:43:23,088 ] - Epoch 56/150 - time: 0.06 - training_loss: -8.5675\n",
      "[ INFO : 2023-05-31 15:43:23,146 ] - Epoch 57/150 - time: 0.06 - training_loss: -8.5726\n",
      "[ INFO : 2023-05-31 15:43:23,206 ] - Epoch 58/150 - time: 0.06 - training_loss: -8.5776\n",
      "[ INFO : 2023-05-31 15:43:23,266 ] - Epoch 59/150 - time: 0.06 - training_loss: -8.5824\n",
      "[ INFO : 2023-05-31 15:43:23,330 ] - Epoch 60/150 - time: 0.06 - training_loss: -8.5870\n",
      "[ INFO : 2023-05-31 15:43:23,390 ] - Epoch 61/150 - time: 0.06 - training_loss: -8.5915\n",
      "[ INFO : 2023-05-31 15:43:23,452 ] - Epoch 62/150 - time: 0.06 - training_loss: -8.5958\n",
      "[ INFO : 2023-05-31 15:43:23,513 ] - Epoch 63/150 - time: 0.06 - training_loss: -8.6000\n",
      "[ INFO : 2023-05-31 15:43:23,572 ] - Epoch 64/150 - time: 0.06 - training_loss: -8.6041\n",
      "[ INFO : 2023-05-31 15:43:23,632 ] - Epoch 65/150 - time: 0.06 - training_loss: -8.6080\n",
      "[ INFO : 2023-05-31 15:43:23,696 ] - Epoch 66/150 - time: 0.06 - training_loss: -8.6118\n",
      "[ INFO : 2023-05-31 15:43:23,756 ] - Epoch 67/150 - time: 0.06 - training_loss: -8.6155\n",
      "[ INFO : 2023-05-31 15:43:23,819 ] - Epoch 68/150 - time: 0.06 - training_loss: -8.6191\n",
      "[ INFO : 2023-05-31 15:43:23,882 ] - Epoch 69/150 - time: 0.06 - training_loss: -8.6226\n",
      "[ INFO : 2023-05-31 15:43:23,940 ] - Epoch 70/150 - time: 0.06 - training_loss: -8.6260\n",
      "[ INFO : 2023-05-31 15:43:23,998 ] - Epoch 71/150 - time: 0.06 - training_loss: -8.6293\n",
      "[ INFO : 2023-05-31 15:43:24,057 ] - Epoch 72/150 - time: 0.06 - training_loss: -8.6325\n",
      "[ INFO : 2023-05-31 15:43:24,115 ] - Epoch 73/150 - time: 0.06 - training_loss: -8.6356\n",
      "[ INFO : 2023-05-31 15:43:24,176 ] - Epoch 74/150 - time: 0.06 - training_loss: -8.6387\n",
      "[ INFO : 2023-05-31 15:43:24,236 ] - Epoch 75/150 - time: 0.06 - training_loss: -8.6416\n",
      "[ INFO : 2023-05-31 15:43:24,295 ] - Epoch 76/150 - time: 0.06 - training_loss: -8.6445\n",
      "[ INFO : 2023-05-31 15:43:24,355 ] - Epoch 77/150 - time: 0.06 - training_loss: -8.6473\n",
      "[ INFO : 2023-05-31 15:43:24,417 ] - Epoch 78/150 - time: 0.06 - training_loss: -8.6500\n",
      "[ INFO : 2023-05-31 15:43:24,478 ] - Epoch 79/150 - time: 0.06 - training_loss: -8.6527\n",
      "[ INFO : 2023-05-31 15:43:24,540 ] - Epoch 80/150 - time: 0.06 - training_loss: -8.6553\n",
      "[ INFO : 2023-05-31 15:43:24,601 ] - Epoch 81/150 - time: 0.06 - training_loss: -8.6578\n",
      "[ INFO : 2023-05-31 15:43:24,660 ] - Epoch 82/150 - time: 0.06 - training_loss: -8.6603\n",
      "[ INFO : 2023-05-31 15:43:24,728 ] - Epoch 83/150 - time: 0.07 - training_loss: -8.6627\n",
      "[ INFO : 2023-05-31 15:43:24,792 ] - Epoch 84/150 - time: 0.06 - training_loss: -8.6651\n",
      "[ INFO : 2023-05-31 15:43:24,852 ] - Epoch 85/150 - time: 0.06 - training_loss: -8.6674\n",
      "[ INFO : 2023-05-31 15:43:24,911 ] - Epoch 86/150 - time: 0.06 - training_loss: -8.6696\n",
      "[ INFO : 2023-05-31 15:43:24,969 ] - Epoch 87/150 - time: 0.06 - training_loss: -8.6718\n",
      "[ INFO : 2023-05-31 15:43:25,027 ] - Epoch 88/150 - time: 0.06 - training_loss: -8.6739\n",
      "[ INFO : 2023-05-31 15:43:25,086 ] - Epoch 89/150 - time: 0.06 - training_loss: -8.6760\n",
      "[ INFO : 2023-05-31 15:43:25,146 ] - Epoch 90/150 - time: 0.06 - training_loss: -8.6781\n",
      "[ INFO : 2023-05-31 15:43:25,206 ] - Epoch 91/150 - time: 0.06 - training_loss: -8.6801\n",
      "[ INFO : 2023-05-31 15:43:25,269 ] - Epoch 92/150 - time: 0.06 - training_loss: -8.6821\n",
      "[ INFO : 2023-05-31 15:43:25,329 ] - Epoch 93/150 - time: 0.06 - training_loss: -8.6840\n",
      "[ INFO : 2023-05-31 15:43:25,389 ] - Epoch 94/150 - time: 0.06 - training_loss: -8.6858\n",
      "[ INFO : 2023-05-31 15:43:25,449 ] - Epoch 95/150 - time: 0.06 - training_loss: -8.6877\n",
      "[ INFO : 2023-05-31 15:43:25,511 ] - Epoch 96/150 - time: 0.06 - training_loss: -8.6895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2023-05-31 15:43:25,571 ] - Epoch 97/150 - time: 0.06 - training_loss: -8.6912\n",
      "[ INFO : 2023-05-31 15:43:25,633 ] - Epoch 98/150 - time: 0.06 - training_loss: -8.6930\n",
      "[ INFO : 2023-05-31 15:43:25,692 ] - Epoch 99/150 - time: 0.06 - training_loss: -8.6947\n",
      "[ INFO : 2023-05-31 15:43:25,753 ] - Epoch 100/150 - time: 0.06 - training_loss: -8.6963\n",
      "[ INFO : 2023-05-31 15:43:25,814 ] - Epoch 101/150 - time: 0.06 - training_loss: -8.6979\n",
      "[ INFO : 2023-05-31 15:43:25,875 ] - Epoch 102/150 - time: 0.06 - training_loss: -8.6995\n",
      "[ INFO : 2023-05-31 15:43:25,933 ] - Epoch 103/150 - time: 0.06 - training_loss: -8.7011\n",
      "[ INFO : 2023-05-31 15:43:25,992 ] - Epoch 104/150 - time: 0.06 - training_loss: -8.7026\n",
      "[ INFO : 2023-05-31 15:43:26,066 ] - Epoch 105/150 - time: 0.07 - training_loss: -8.7041\n",
      "[ INFO : 2023-05-31 15:43:26,124 ] - Epoch 106/150 - time: 0.06 - training_loss: -8.7056\n",
      "[ INFO : 2023-05-31 15:43:26,183 ] - Epoch 107/150 - time: 0.06 - training_loss: -8.7071\n",
      "[ INFO : 2023-05-31 15:43:26,245 ] - Epoch 108/150 - time: 0.06 - training_loss: -8.7085\n",
      "[ INFO : 2023-05-31 15:43:26,305 ] - Epoch 109/150 - time: 0.06 - training_loss: -8.7099\n",
      "[ INFO : 2023-05-31 15:43:26,368 ] - Epoch 110/150 - time: 0.06 - training_loss: -8.7112\n",
      "[ INFO : 2023-05-31 15:43:26,428 ] - Epoch 111/150 - time: 0.06 - training_loss: -8.7126\n",
      "[ INFO : 2023-05-31 15:43:26,487 ] - Epoch 112/150 - time: 0.06 - training_loss: -8.7139\n",
      "[ INFO : 2023-05-31 15:43:26,547 ] - Epoch 113/150 - time: 0.06 - training_loss: -8.7152\n",
      "[ INFO : 2023-05-31 15:43:26,610 ] - Epoch 114/150 - time: 0.06 - training_loss: -8.7165\n",
      "[ INFO : 2023-05-31 15:43:26,670 ] - Epoch 115/150 - time: 0.06 - training_loss: -8.7177\n",
      "[ INFO : 2023-05-31 15:43:26,750 ] - Epoch 116/150 - time: 0.08 - training_loss: -8.7190\n",
      "[ INFO : 2023-05-31 15:43:26,810 ] - Epoch 117/150 - time: 0.06 - training_loss: -8.7202\n",
      "[ INFO : 2023-05-31 15:43:26,871 ] - Epoch 118/150 - time: 0.06 - training_loss: -8.7214\n",
      "[ INFO : 2023-05-31 15:43:26,929 ] - Epoch 119/150 - time: 0.06 - training_loss: -8.7225\n",
      "[ INFO : 2023-05-31 15:43:26,988 ] - Epoch 120/150 - time: 0.06 - training_loss: -8.7237\n",
      "[ INFO : 2023-05-31 15:43:27,046 ] - Epoch 121/150 - time: 0.06 - training_loss: -8.7248\n",
      "[ INFO : 2023-05-31 15:43:27,108 ] - Epoch 122/150 - time: 0.06 - training_loss: -8.7259\n",
      "[ INFO : 2023-05-31 15:43:27,167 ] - Epoch 123/150 - time: 0.06 - training_loss: -8.7270\n",
      "[ INFO : 2023-05-31 15:43:27,226 ] - Epoch 124/150 - time: 0.06 - training_loss: -8.7281\n",
      "[ INFO : 2023-05-31 15:43:27,286 ] - Epoch 125/150 - time: 0.06 - training_loss: -8.7291\n",
      "[ INFO : 2023-05-31 15:43:27,348 ] - Epoch 126/150 - time: 0.06 - training_loss: -8.7302\n",
      "[ INFO : 2023-05-31 15:43:27,409 ] - Epoch 127/150 - time: 0.06 - training_loss: -8.7312\n",
      "[ INFO : 2023-05-31 15:43:27,472 ] - Epoch 128/150 - time: 0.06 - training_loss: -8.7322\n",
      "[ INFO : 2023-05-31 15:43:27,531 ] - Epoch 129/150 - time: 0.06 - training_loss: -8.7332\n",
      "[ INFO : 2023-05-31 15:43:27,592 ] - Epoch 130/150 - time: 0.06 - training_loss: -8.7342\n",
      "[ INFO : 2023-05-31 15:43:27,652 ] - Epoch 131/150 - time: 0.06 - training_loss: -8.7352\n",
      "[ INFO : 2023-05-31 15:43:27,715 ] - Epoch 132/150 - time: 0.06 - training_loss: -8.7361\n",
      "[ INFO : 2023-05-31 15:43:27,777 ] - Epoch 133/150 - time: 0.06 - training_loss: -8.7370\n",
      "[ INFO : 2023-05-31 15:43:27,839 ] - Epoch 134/150 - time: 0.06 - training_loss: -8.7380\n",
      "[ INFO : 2023-05-31 15:43:27,897 ] - Epoch 135/150 - time: 0.06 - training_loss: -8.7389\n",
      "[ INFO : 2023-05-31 15:43:27,955 ] - Epoch 136/150 - time: 0.06 - training_loss: -8.7398\n",
      "[ INFO : 2023-05-31 15:43:28,013 ] - Epoch 137/150 - time: 0.06 - training_loss: -8.7406\n",
      "[ INFO : 2023-05-31 15:43:28,072 ] - Epoch 138/150 - time: 0.06 - training_loss: -8.7415\n",
      "[ INFO : 2023-05-31 15:43:28,131 ] - Epoch 139/150 - time: 0.06 - training_loss: -8.7424\n",
      "[ INFO : 2023-05-31 15:43:28,192 ] - Epoch 140/150 - time: 0.06 - training_loss: -8.7432\n",
      "[ INFO : 2023-05-31 15:43:28,253 ] - Epoch 141/150 - time: 0.06 - training_loss: -8.7440\n",
      "[ INFO : 2023-05-31 15:43:28,313 ] - Epoch 142/150 - time: 0.06 - training_loss: -8.7449\n",
      "[ INFO : 2023-05-31 15:43:28,374 ] - Epoch 143/150 - time: 0.06 - training_loss: -8.7457\n",
      "[ INFO : 2023-05-31 15:43:28,436 ] - Epoch 144/150 - time: 0.06 - training_loss: -8.7465\n",
      "[ INFO : 2023-05-31 15:43:28,496 ] - Epoch 145/150 - time: 0.06 - training_loss: -8.7472\n",
      "[ INFO : 2023-05-31 15:43:28,558 ] - Epoch 146/150 - time: 0.06 - training_loss: -8.7480\n",
      "[ INFO : 2023-05-31 15:43:28,619 ] - Epoch 147/150 - time: 0.06 - training_loss: -8.7488\n",
      "[ INFO : 2023-05-31 15:43:28,680 ] - Epoch 148/150 - time: 0.06 - training_loss: -8.7495\n",
      "[ INFO : 2023-05-31 15:43:28,742 ] - Epoch 149/150 - time: 0.06 - training_loss: -8.7503\n",
      "[ INFO : 2023-05-31 15:43:28,805 ] - Epoch 150/150 - time: 0.06 - training_loss: -8.7510\n",
      "[ INFO : 2023-05-31 15:43:28,850 ] - DeepGCCA(\n",
      "  (model_list): ModuleList(\n",
      "    (0-2): 3 x MlpNet(\n",
      "      (layers): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
      "          (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[ INFO : 2023-05-31 15:43:28,857 ] - Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.05\n",
      "    lr: 0.05\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "[ INFO : 2023-05-31 15:43:28,920 ] - Epoch 1/150 - time: 0.06 - training_loss: -2.7990\n",
      "[ INFO : 2023-05-31 15:43:28,978 ] - Epoch 2/150 - time: 0.06 - training_loss: -3.4109\n",
      "[ INFO : 2023-05-31 15:43:29,036 ] - Epoch 3/150 - time: 0.06 - training_loss: -4.5844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear CCA started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2023-05-31 15:43:29,096 ] - Epoch 4/150 - time: 0.06 - training_loss: -5.4546\n",
      "[ INFO : 2023-05-31 15:43:29,154 ] - Epoch 5/150 - time: 0.06 - training_loss: -6.0504\n",
      "[ INFO : 2023-05-31 15:43:29,213 ] - Epoch 6/150 - time: 0.06 - training_loss: -6.4751\n",
      "[ INFO : 2023-05-31 15:43:29,275 ] - Epoch 7/150 - time: 0.06 - training_loss: -6.7805\n",
      "[ INFO : 2023-05-31 15:43:29,335 ] - Epoch 8/150 - time: 0.06 - training_loss: -7.0158\n",
      "[ INFO : 2023-05-31 15:43:29,395 ] - Epoch 9/150 - time: 0.06 - training_loss: -7.2064\n",
      "[ INFO : 2023-05-31 15:43:29,458 ] - Epoch 10/150 - time: 0.06 - training_loss: -7.3619\n",
      "[ INFO : 2023-05-31 15:43:29,517 ] - Epoch 11/150 - time: 0.06 - training_loss: -7.4897\n",
      "[ INFO : 2023-05-31 15:43:29,577 ] - Epoch 12/150 - time: 0.06 - training_loss: -7.5966\n",
      "[ INFO : 2023-05-31 15:43:29,639 ] - Epoch 13/150 - time: 0.06 - training_loss: -7.6880\n",
      "[ INFO : 2023-05-31 15:43:29,700 ] - Epoch 14/150 - time: 0.06 - training_loss: -7.7669\n",
      "[ INFO : 2023-05-31 15:43:29,759 ] - Epoch 15/150 - time: 0.06 - training_loss: -7.8357\n",
      "[ INFO : 2023-05-31 15:43:29,821 ] - Epoch 16/150 - time: 0.06 - training_loss: -7.8963\n",
      "[ INFO : 2023-05-31 15:43:29,880 ] - Epoch 17/150 - time: 0.06 - training_loss: -7.9501\n",
      "[ INFO : 2023-05-31 15:43:29,938 ] - Epoch 18/150 - time: 0.06 - training_loss: -7.9984\n",
      "[ INFO : 2023-05-31 15:43:29,998 ] - Epoch 19/150 - time: 0.06 - training_loss: -8.0421\n",
      "[ INFO : 2023-05-31 15:43:30,055 ] - Epoch 20/150 - time: 0.06 - training_loss: -8.0816\n",
      "[ INFO : 2023-05-31 15:43:30,113 ] - Epoch 21/150 - time: 0.06 - training_loss: -8.1176\n",
      "[ INFO : 2023-05-31 15:43:30,173 ] - Epoch 22/150 - time: 0.06 - training_loss: -8.1505\n",
      "[ INFO : 2023-05-31 15:43:30,232 ] - Epoch 23/150 - time: 0.06 - training_loss: -8.1807\n",
      "[ INFO : 2023-05-31 15:43:30,292 ] - Epoch 24/150 - time: 0.06 - training_loss: -8.2084\n",
      "[ INFO : 2023-05-31 15:43:30,356 ] - Epoch 25/150 - time: 0.06 - training_loss: -8.2340\n",
      "[ INFO : 2023-05-31 15:43:30,415 ] - Epoch 26/150 - time: 0.06 - training_loss: -8.2575\n",
      "[ INFO : 2023-05-31 15:43:30,475 ] - Epoch 27/150 - time: 0.06 - training_loss: -8.2792\n",
      "[ INFO : 2023-05-31 15:43:30,536 ] - Epoch 28/150 - time: 0.06 - training_loss: -8.2994\n",
      "[ INFO : 2023-05-31 15:43:30,596 ] - Epoch 29/150 - time: 0.06 - training_loss: -8.3183\n",
      "[ INFO : 2023-05-31 15:43:30,655 ] - Epoch 30/150 - time: 0.06 - training_loss: -8.3360\n",
      "[ INFO : 2023-05-31 15:43:30,717 ] - Epoch 31/150 - time: 0.06 - training_loss: -8.3526\n",
      "[ INFO : 2023-05-31 15:43:30,777 ] - Epoch 32/150 - time: 0.06 - training_loss: -8.3683\n",
      "[ INFO : 2023-05-31 15:43:30,840 ] - Epoch 33/150 - time: 0.06 - training_loss: -8.3830\n",
      "[ INFO : 2023-05-31 15:43:30,899 ] - Epoch 34/150 - time: 0.06 - training_loss: -8.3969\n",
      "[ INFO : 2023-05-31 15:43:30,956 ] - Epoch 35/150 - time: 0.06 - training_loss: -8.4100\n",
      "[ INFO : 2023-05-31 15:43:31,015 ] - Epoch 36/150 - time: 0.06 - training_loss: -8.4223\n",
      "[ INFO : 2023-05-31 15:43:31,074 ] - Epoch 37/150 - time: 0.06 - training_loss: -8.4341\n",
      "[ INFO : 2023-05-31 15:43:31,132 ] - Epoch 38/150 - time: 0.06 - training_loss: -8.4452\n",
      "[ INFO : 2023-05-31 15:43:31,191 ] - Epoch 39/150 - time: 0.06 - training_loss: -8.4558\n",
      "[ INFO : 2023-05-31 15:43:31,252 ] - Epoch 40/150 - time: 0.06 - training_loss: -8.4658\n",
      "[ INFO : 2023-05-31 15:43:31,312 ] - Epoch 41/150 - time: 0.06 - training_loss: -8.4753\n",
      "[ INFO : 2023-05-31 15:43:31,372 ] - Epoch 42/150 - time: 0.06 - training_loss: -8.4844\n",
      "[ INFO : 2023-05-31 15:43:31,433 ] - Epoch 43/150 - time: 0.06 - training_loss: -8.4931\n",
      "[ INFO : 2023-05-31 15:43:31,493 ] - Epoch 44/150 - time: 0.06 - training_loss: -8.5014\n",
      "[ INFO : 2023-05-31 15:43:31,552 ] - Epoch 45/150 - time: 0.06 - training_loss: -8.5093\n",
      "[ INFO : 2023-05-31 15:43:31,615 ] - Epoch 46/150 - time: 0.06 - training_loss: -8.5169\n",
      "[ INFO : 2023-05-31 15:43:31,678 ] - Epoch 47/150 - time: 0.06 - training_loss: -8.5242\n",
      "[ INFO : 2023-05-31 15:43:31,737 ] - Epoch 48/150 - time: 0.06 - training_loss: -8.5312\n",
      "[ INFO : 2023-05-31 15:43:31,800 ] - Epoch 49/150 - time: 0.06 - training_loss: -8.5379\n",
      "[ INFO : 2023-05-31 15:43:31,861 ] - Epoch 50/150 - time: 0.06 - training_loss: -8.5443\n",
      "[ INFO : 2023-05-31 15:43:31,920 ] - Epoch 51/150 - time: 0.06 - training_loss: -8.5505\n",
      "[ INFO : 2023-05-31 15:43:31,979 ] - Epoch 52/150 - time: 0.06 - training_loss: -8.5564\n",
      "[ INFO : 2023-05-31 15:43:32,036 ] - Epoch 53/150 - time: 0.06 - training_loss: -8.5621\n",
      "[ INFO : 2023-05-31 15:43:32,094 ] - Epoch 54/150 - time: 0.06 - training_loss: -8.5677\n",
      "[ INFO : 2023-05-31 15:43:32,154 ] - Epoch 55/150 - time: 0.06 - training_loss: -8.5730\n",
      "[ INFO : 2023-05-31 15:43:32,214 ] - Epoch 56/150 - time: 0.06 - training_loss: -8.5781\n",
      "[ INFO : 2023-05-31 15:43:32,273 ] - Epoch 57/150 - time: 0.06 - training_loss: -8.5830\n",
      "[ INFO : 2023-05-31 15:43:32,335 ] - Epoch 58/150 - time: 0.06 - training_loss: -8.5878\n",
      "[ INFO : 2023-05-31 15:43:32,395 ] - Epoch 59/150 - time: 0.06 - training_loss: -8.5924\n",
      "[ INFO : 2023-05-31 15:43:32,455 ] - Epoch 60/150 - time: 0.06 - training_loss: -8.5969\n",
      "[ INFO : 2023-05-31 15:43:32,516 ] - Epoch 61/150 - time: 0.06 - training_loss: -8.6012\n",
      "[ INFO : 2023-05-31 15:43:32,576 ] - Epoch 62/150 - time: 0.06 - training_loss: -8.6054\n",
      "[ INFO : 2023-05-31 15:43:32,635 ] - Epoch 63/150 - time: 0.06 - training_loss: -8.6094\n",
      "[ INFO : 2023-05-31 15:43:32,697 ] - Epoch 64/150 - time: 0.06 - training_loss: -8.6133\n",
      "[ INFO : 2023-05-31 15:43:32,756 ] - Epoch 65/150 - time: 0.06 - training_loss: -8.6171\n",
      "[ INFO : 2023-05-31 15:43:32,816 ] - Epoch 66/150 - time: 0.06 - training_loss: -8.6208\n",
      "[ INFO : 2023-05-31 15:43:32,879 ] - Epoch 67/150 - time: 0.06 - training_loss: -8.6244\n",
      "[ INFO : 2023-05-31 15:43:32,939 ] - Epoch 68/150 - time: 0.06 - training_loss: -8.6278\n",
      "[ INFO : 2023-05-31 15:43:32,997 ] - Epoch 69/150 - time: 0.06 - training_loss: -8.6312\n",
      "[ INFO : 2023-05-31 15:43:33,056 ] - Epoch 70/150 - time: 0.06 - training_loss: -8.6345\n",
      "[ INFO : 2023-05-31 15:43:33,116 ] - Epoch 71/150 - time: 0.06 - training_loss: -8.6377\n",
      "[ INFO : 2023-05-31 15:43:33,174 ] - Epoch 72/150 - time: 0.06 - training_loss: -8.6408\n",
      "[ INFO : 2023-05-31 15:43:33,236 ] - Epoch 73/150 - time: 0.06 - training_loss: -8.6438\n",
      "[ INFO : 2023-05-31 15:43:33,296 ] - Epoch 74/150 - time: 0.06 - training_loss: -8.6467\n",
      "[ INFO : 2023-05-31 15:43:33,357 ] - Epoch 75/150 - time: 0.06 - training_loss: -8.6495\n",
      "[ INFO : 2023-05-31 15:43:33,419 ] - Epoch 76/150 - time: 0.06 - training_loss: -8.6523\n",
      "[ INFO : 2023-05-31 15:43:33,479 ] - Epoch 77/150 - time: 0.06 - training_loss: -8.6550\n",
      "[ INFO : 2023-05-31 15:43:33,539 ] - Epoch 78/150 - time: 0.06 - training_loss: -8.6576\n",
      "[ INFO : 2023-05-31 15:43:33,602 ] - Epoch 79/150 - time: 0.06 - training_loss: -8.6602\n",
      "[ INFO : 2023-05-31 15:43:33,661 ] - Epoch 80/150 - time: 0.06 - training_loss: -8.6627\n",
      "[ INFO : 2023-05-31 15:43:33,721 ] - Epoch 81/150 - time: 0.06 - training_loss: -8.6652\n",
      "[ INFO : 2023-05-31 15:43:33,784 ] - Epoch 82/150 - time: 0.06 - training_loss: -8.6675\n",
      "[ INFO : 2023-05-31 15:43:33,844 ] - Epoch 83/150 - time: 0.06 - training_loss: -8.6699\n",
      "[ INFO : 2023-05-31 15:43:33,901 ] - Epoch 84/150 - time: 0.06 - training_loss: -8.6721\n",
      "[ INFO : 2023-05-31 15:43:33,962 ] - Epoch 85/150 - time: 0.06 - training_loss: -8.6743\n",
      "[ INFO : 2023-05-31 15:43:34,020 ] - Epoch 86/150 - time: 0.06 - training_loss: -8.6765\n",
      "[ INFO : 2023-05-31 15:43:34,084 ] - Epoch 87/150 - time: 0.06 - training_loss: -8.6786\n",
      "[ INFO : 2023-05-31 15:43:34,144 ] - Epoch 88/150 - time: 0.06 - training_loss: -8.6807\n",
      "[ INFO : 2023-05-31 15:43:34,202 ] - Epoch 89/150 - time: 0.06 - training_loss: -8.6827\n",
      "[ INFO : 2023-05-31 15:43:34,262 ] - Epoch 90/150 - time: 0.06 - training_loss: -8.6847\n",
      "[ INFO : 2023-05-31 15:43:34,324 ] - Epoch 91/150 - time: 0.06 - training_loss: -8.6866\n",
      "[ INFO : 2023-05-31 15:43:34,384 ] - Epoch 92/150 - time: 0.06 - training_loss: -8.6885\n",
      "[ INFO : 2023-05-31 15:43:34,444 ] - Epoch 93/150 - time: 0.06 - training_loss: -8.6903\n",
      "[ INFO : 2023-05-31 15:43:34,506 ] - Epoch 94/150 - time: 0.06 - training_loss: -8.6922\n",
      "[ INFO : 2023-05-31 15:43:34,566 ] - Epoch 95/150 - time: 0.06 - training_loss: -8.6939\n",
      "[ INFO : 2023-05-31 15:43:34,626 ] - Epoch 96/150 - time: 0.06 - training_loss: -8.6957\n",
      "[ INFO : 2023-05-31 15:43:34,688 ] - Epoch 97/150 - time: 0.06 - training_loss: -8.6974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2023-05-31 15:43:34,748 ] - Epoch 98/150 - time: 0.06 - training_loss: -8.6990\n",
      "[ INFO : 2023-05-31 15:43:34,807 ] - Epoch 99/150 - time: 0.06 - training_loss: -8.7007\n",
      "[ INFO : 2023-05-31 15:43:34,871 ] - Epoch 100/150 - time: 0.06 - training_loss: -8.7022\n",
      "[ INFO : 2023-05-31 15:43:34,930 ] - Epoch 101/150 - time: 0.06 - training_loss: -8.7038\n",
      "[ INFO : 2023-05-31 15:43:34,989 ] - Epoch 102/150 - time: 0.06 - training_loss: -8.7054\n",
      "[ INFO : 2023-05-31 15:43:35,048 ] - Epoch 103/150 - time: 0.06 - training_loss: -8.7069\n",
      "[ INFO : 2023-05-31 15:43:35,107 ] - Epoch 104/150 - time: 0.06 - training_loss: -8.7083\n",
      "[ INFO : 2023-05-31 15:43:35,166 ] - Epoch 105/150 - time: 0.06 - training_loss: -8.7098\n",
      "[ INFO : 2023-05-31 15:43:35,229 ] - Epoch 106/150 - time: 0.06 - training_loss: -8.7112\n",
      "[ INFO : 2023-05-31 15:43:35,289 ] - Epoch 107/150 - time: 0.06 - training_loss: -8.7126\n",
      "[ INFO : 2023-05-31 15:43:35,349 ] - Epoch 108/150 - time: 0.06 - training_loss: -8.7140\n",
      "[ INFO : 2023-05-31 15:43:35,410 ] - Epoch 109/150 - time: 0.06 - training_loss: -8.7153\n",
      "[ INFO : 2023-05-31 15:43:35,470 ] - Epoch 110/150 - time: 0.06 - training_loss: -8.7166\n",
      "[ INFO : 2023-05-31 15:43:35,529 ] - Epoch 111/150 - time: 0.06 - training_loss: -8.7179\n",
      "[ INFO : 2023-05-31 15:43:35,591 ] - Epoch 112/150 - time: 0.06 - training_loss: -8.7192\n",
      "[ INFO : 2023-05-31 15:43:35,651 ] - Epoch 113/150 - time: 0.06 - training_loss: -8.7204\n",
      "[ INFO : 2023-05-31 15:43:35,711 ] - Epoch 114/150 - time: 0.06 - training_loss: -8.7217\n",
      "[ INFO : 2023-05-31 15:43:35,772 ] - Epoch 115/150 - time: 0.06 - training_loss: -8.7229\n",
      "[ INFO : 2023-05-31 15:43:35,834 ] - Epoch 116/150 - time: 0.06 - training_loss: -8.7241\n",
      "[ INFO : 2023-05-31 15:43:35,895 ] - Epoch 117/150 - time: 0.06 - training_loss: -8.7252\n",
      "[ INFO : 2023-05-31 15:43:35,954 ] - Epoch 118/150 - time: 0.06 - training_loss: -8.7264\n",
      "[ INFO : 2023-05-31 15:43:36,013 ] - Epoch 119/150 - time: 0.06 - training_loss: -8.7275\n",
      "[ INFO : 2023-05-31 15:43:36,071 ] - Epoch 120/150 - time: 0.06 - training_loss: -8.7286\n",
      "[ INFO : 2023-05-31 15:43:36,131 ] - Epoch 121/150 - time: 0.06 - training_loss: -8.7297\n",
      "[ INFO : 2023-05-31 15:43:36,191 ] - Epoch 122/150 - time: 0.06 - training_loss: -8.7308\n",
      "[ INFO : 2023-05-31 15:43:36,250 ] - Epoch 123/150 - time: 0.06 - training_loss: -8.7318\n",
      "[ INFO : 2023-05-31 15:43:36,313 ] - Epoch 124/150 - time: 0.06 - training_loss: -8.7329\n",
      "[ INFO : 2023-05-31 15:43:36,373 ] - Epoch 125/150 - time: 0.06 - training_loss: -8.7339\n",
      "[ INFO : 2023-05-31 15:43:36,433 ] - Epoch 126/150 - time: 0.06 - training_loss: -8.7349\n",
      "[ INFO : 2023-05-31 15:43:36,495 ] - Epoch 127/150 - time: 0.06 - training_loss: -8.7359\n",
      "[ INFO : 2023-05-31 15:43:36,554 ] - Epoch 128/150 - time: 0.06 - training_loss: -8.7368\n",
      "[ INFO : 2023-05-31 15:43:36,615 ] - Epoch 129/150 - time: 0.06 - training_loss: -8.7378\n",
      "[ INFO : 2023-05-31 15:43:36,677 ] - Epoch 130/150 - time: 0.06 - training_loss: -8.7387\n",
      "[ INFO : 2023-05-31 15:43:36,737 ] - Epoch 131/150 - time: 0.06 - training_loss: -8.7397\n",
      "[ INFO : 2023-05-31 15:43:36,796 ] - Epoch 132/150 - time: 0.06 - training_loss: -8.7406\n",
      "[ INFO : 2023-05-31 15:43:36,860 ] - Epoch 133/150 - time: 0.06 - training_loss: -8.7415\n",
      "[ INFO : 2023-05-31 15:43:36,918 ] - Epoch 134/150 - time: 0.06 - training_loss: -8.7424\n",
      "[ INFO : 2023-05-31 15:43:36,976 ] - Epoch 135/150 - time: 0.06 - training_loss: -8.7433\n",
      "[ INFO : 2023-05-31 15:43:37,036 ] - Epoch 136/150 - time: 0.06 - training_loss: -8.7441\n",
      "[ INFO : 2023-05-31 15:43:37,093 ] - Epoch 137/150 - time: 0.06 - training_loss: -8.7450\n",
      "[ INFO : 2023-05-31 15:43:37,150 ] - Epoch 138/150 - time: 0.06 - training_loss: -8.7458\n",
      "[ INFO : 2023-05-31 15:43:37,212 ] - Epoch 139/150 - time: 0.06 - training_loss: -8.7466\n",
      "[ INFO : 2023-05-31 15:43:37,271 ] - Epoch 140/150 - time: 0.06 - training_loss: -8.7474\n",
      "[ INFO : 2023-05-31 15:43:37,331 ] - Epoch 141/150 - time: 0.06 - training_loss: -8.7482\n",
      "[ INFO : 2023-05-31 15:43:37,394 ] - Epoch 142/150 - time: 0.06 - training_loss: -8.7490\n",
      "[ INFO : 2023-05-31 15:43:37,454 ] - Epoch 143/150 - time: 0.06 - training_loss: -8.7498\n",
      "[ INFO : 2023-05-31 15:43:37,513 ] - Epoch 144/150 - time: 0.06 - training_loss: -8.7506\n",
      "[ INFO : 2023-05-31 15:43:37,575 ] - Epoch 145/150 - time: 0.06 - training_loss: -8.7513\n",
      "[ INFO : 2023-05-31 15:43:37,635 ] - Epoch 146/150 - time: 0.06 - training_loss: -8.7521\n",
      "[ INFO : 2023-05-31 15:43:37,696 ] - Epoch 147/150 - time: 0.06 - training_loss: -8.7528\n",
      "[ INFO : 2023-05-31 15:43:37,758 ] - Epoch 148/150 - time: 0.06 - training_loss: -8.7535\n",
      "[ INFO : 2023-05-31 15:43:37,824 ] - Epoch 149/150 - time: 0.06 - training_loss: -8.7543\n",
      "[ INFO : 2023-05-31 15:43:37,884 ] - Epoch 150/150 - time: 0.06 - training_loss: -8.7550\n",
      "[ INFO : 2023-05-31 15:43:37,930 ] - DeepGCCA(\n",
      "  (model_list): ModuleList(\n",
      "    (0-2): 3 x MlpNet(\n",
      "      (layers): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
      "          (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[ INFO : 2023-05-31 15:43:37,937 ] - Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.05\n",
      "    lr: 0.05\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "[ INFO : 2023-05-31 15:43:37,999 ] - Epoch 1/150 - time: 0.06 - training_loss: -3.3347\n",
      "[ INFO : 2023-05-31 15:43:38,057 ] - Epoch 2/150 - time: 0.06 - training_loss: -2.7842\n",
      "[ INFO : 2023-05-31 15:43:38,115 ] - Epoch 3/150 - time: 0.06 - training_loss: -3.3547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear CCA started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2023-05-31 15:43:38,176 ] - Epoch 4/150 - time: 0.06 - training_loss: -4.0371\n",
      "[ INFO : 2023-05-31 15:43:38,236 ] - Epoch 5/150 - time: 0.06 - training_loss: -4.6730\n",
      "[ INFO : 2023-05-31 15:43:38,298 ] - Epoch 6/150 - time: 0.06 - training_loss: -5.1986\n",
      "[ INFO : 2023-05-31 15:43:38,358 ] - Epoch 7/150 - time: 0.06 - training_loss: -5.6093\n",
      "[ INFO : 2023-05-31 15:43:38,418 ] - Epoch 8/150 - time: 0.06 - training_loss: -5.9496\n",
      "[ INFO : 2023-05-31 15:43:38,478 ] - Epoch 9/150 - time: 0.06 - training_loss: -6.2397\n",
      "[ INFO : 2023-05-31 15:43:38,539 ] - Epoch 10/150 - time: 0.06 - training_loss: -6.4831\n",
      "[ INFO : 2023-05-31 15:43:38,600 ] - Epoch 11/150 - time: 0.06 - training_loss: -6.6843\n",
      "[ INFO : 2023-05-31 15:43:38,662 ] - Epoch 12/150 - time: 0.06 - training_loss: -6.8513\n",
      "[ INFO : 2023-05-31 15:43:38,722 ] - Epoch 13/150 - time: 0.06 - training_loss: -6.9927\n",
      "[ INFO : 2023-05-31 15:43:38,782 ] - Epoch 14/150 - time: 0.06 - training_loss: -7.1149\n",
      "[ INFO : 2023-05-31 15:43:38,844 ] - Epoch 15/150 - time: 0.06 - training_loss: -7.2225\n",
      "[ INFO : 2023-05-31 15:43:38,904 ] - Epoch 16/150 - time: 0.06 - training_loss: -7.3181\n",
      "[ INFO : 2023-05-31 15:43:38,962 ] - Epoch 17/150 - time: 0.06 - training_loss: -7.4037\n",
      "[ INFO : 2023-05-31 15:43:39,022 ] - Epoch 18/150 - time: 0.06 - training_loss: -7.4807\n",
      "[ INFO : 2023-05-31 15:43:39,081 ] - Epoch 19/150 - time: 0.06 - training_loss: -7.5501\n",
      "[ INFO : 2023-05-31 15:43:39,140 ] - Epoch 20/150 - time: 0.06 - training_loss: -7.6131\n",
      "[ INFO : 2023-05-31 15:43:39,199 ] - Epoch 21/150 - time: 0.06 - training_loss: -7.6705\n",
      "[ INFO : 2023-05-31 15:43:39,261 ] - Epoch 22/150 - time: 0.06 - training_loss: -7.7231\n",
      "[ INFO : 2023-05-31 15:43:39,322 ] - Epoch 23/150 - time: 0.06 - training_loss: -7.7713\n",
      "[ INFO : 2023-05-31 15:43:39,386 ] - Epoch 24/150 - time: 0.06 - training_loss: -7.8156\n",
      "[ INFO : 2023-05-31 15:43:39,447 ] - Epoch 25/150 - time: 0.06 - training_loss: -7.8565\n",
      "[ INFO : 2023-05-31 15:43:39,506 ] - Epoch 26/150 - time: 0.06 - training_loss: -7.8941\n",
      "[ INFO : 2023-05-31 15:43:39,567 ] - Epoch 27/150 - time: 0.06 - training_loss: -7.9289\n",
      "[ INFO : 2023-05-31 15:43:39,630 ] - Epoch 28/150 - time: 0.06 - training_loss: -7.9612\n",
      "[ INFO : 2023-05-31 15:43:39,690 ] - Epoch 29/150 - time: 0.06 - training_loss: -7.9912\n",
      "[ INFO : 2023-05-31 15:43:39,753 ] - Epoch 30/150 - time: 0.06 - training_loss: -8.0194\n",
      "[ INFO : 2023-05-31 15:43:39,813 ] - Epoch 31/150 - time: 0.06 - training_loss: -8.0458\n",
      "[ INFO : 2023-05-31 15:43:39,876 ] - Epoch 32/150 - time: 0.06 - training_loss: -8.0707\n",
      "[ INFO : 2023-05-31 15:43:39,934 ] - Epoch 33/150 - time: 0.06 - training_loss: -8.0943\n",
      "[ INFO : 2023-05-31 15:43:39,994 ] - Epoch 34/150 - time: 0.06 - training_loss: -8.1165\n",
      "[ INFO : 2023-05-31 15:43:40,052 ] - Epoch 35/150 - time: 0.06 - training_loss: -8.1375\n",
      "[ INFO : 2023-05-31 15:43:40,112 ] - Epoch 36/150 - time: 0.06 - training_loss: -8.1574\n",
      "[ INFO : 2023-05-31 15:43:40,171 ] - Epoch 37/150 - time: 0.06 - training_loss: -8.1762\n",
      "[ INFO : 2023-05-31 15:43:40,230 ] - Epoch 38/150 - time: 0.06 - training_loss: -8.1941\n",
      "[ INFO : 2023-05-31 15:43:40,291 ] - Epoch 39/150 - time: 0.06 - training_loss: -8.2110\n",
      "[ INFO : 2023-05-31 15:43:40,353 ] - Epoch 40/150 - time: 0.06 - training_loss: -8.2271\n",
      "[ INFO : 2023-05-31 15:43:40,415 ] - Epoch 41/150 - time: 0.06 - training_loss: -8.2424\n",
      "[ INFO : 2023-05-31 15:43:40,477 ] - Epoch 42/150 - time: 0.06 - training_loss: -8.2570\n",
      "[ INFO : 2023-05-31 15:43:40,537 ] - Epoch 43/150 - time: 0.06 - training_loss: -8.2710\n",
      "[ INFO : 2023-05-31 15:43:40,597 ] - Epoch 44/150 - time: 0.06 - training_loss: -8.2843\n",
      "[ INFO : 2023-05-31 15:43:40,657 ] - Epoch 45/150 - time: 0.06 - training_loss: -8.2970\n",
      "[ INFO : 2023-05-31 15:43:40,722 ] - Epoch 46/150 - time: 0.06 - training_loss: -8.3091\n",
      "[ INFO : 2023-05-31 15:43:40,781 ] - Epoch 47/150 - time: 0.06 - training_loss: -8.3208\n",
      "[ INFO : 2023-05-31 15:43:40,844 ] - Epoch 48/150 - time: 0.06 - training_loss: -8.3320\n",
      "[ INFO : 2023-05-31 15:43:40,902 ] - Epoch 49/150 - time: 0.06 - training_loss: -8.3427\n",
      "[ INFO : 2023-05-31 15:43:40,962 ] - Epoch 50/150 - time: 0.06 - training_loss: -8.3530\n",
      "[ INFO : 2023-05-31 15:43:41,024 ] - Epoch 51/150 - time: 0.06 - training_loss: -8.3629\n",
      "[ INFO : 2023-05-31 15:43:41,084 ] - Epoch 52/150 - time: 0.06 - training_loss: -8.3725\n",
      "[ INFO : 2023-05-31 15:43:41,144 ] - Epoch 53/150 - time: 0.06 - training_loss: -8.3816\n",
      "[ INFO : 2023-05-31 15:43:41,207 ] - Epoch 54/150 - time: 0.06 - training_loss: -8.3905\n",
      "[ INFO : 2023-05-31 15:43:41,281 ] - Epoch 55/150 - time: 0.07 - training_loss: -8.3990\n",
      "[ INFO : 2023-05-31 15:43:41,342 ] - Epoch 56/150 - time: 0.06 - training_loss: -8.4072\n",
      "[ INFO : 2023-05-31 15:43:41,403 ] - Epoch 57/150 - time: 0.06 - training_loss: -8.4152\n",
      "[ INFO : 2023-05-31 15:43:41,465 ] - Epoch 58/150 - time: 0.06 - training_loss: -8.4228\n",
      "[ INFO : 2023-05-31 15:43:41,525 ] - Epoch 59/150 - time: 0.06 - training_loss: -8.4302\n",
      "[ INFO : 2023-05-31 15:43:41,587 ] - Epoch 60/150 - time: 0.06 - training_loss: -8.4374\n",
      "[ INFO : 2023-05-31 15:43:41,647 ] - Epoch 61/150 - time: 0.06 - training_loss: -8.4443\n",
      "[ INFO : 2023-05-31 15:43:41,707 ] - Epoch 62/150 - time: 0.06 - training_loss: -8.4510\n",
      "[ INFO : 2023-05-31 15:43:41,767 ] - Epoch 63/150 - time: 0.06 - training_loss: -8.4575\n",
      "[ INFO : 2023-05-31 15:43:41,830 ] - Epoch 64/150 - time: 0.06 - training_loss: -8.4638\n",
      "[ INFO : 2023-05-31 15:43:41,889 ] - Epoch 65/150 - time: 0.06 - training_loss: -8.4699\n",
      "[ INFO : 2023-05-31 15:43:41,948 ] - Epoch 66/150 - time: 0.06 - training_loss: -8.4758\n",
      "[ INFO : 2023-05-31 15:43:42,006 ] - Epoch 67/150 - time: 0.06 - training_loss: -8.4815\n",
      "[ INFO : 2023-05-31 15:43:42,064 ] - Epoch 68/150 - time: 0.06 - training_loss: -8.4871\n",
      "[ INFO : 2023-05-31 15:43:42,122 ] - Epoch 69/150 - time: 0.06 - training_loss: -8.4925\n",
      "[ INFO : 2023-05-31 15:43:42,183 ] - Epoch 70/150 - time: 0.06 - training_loss: -8.4977\n",
      "[ INFO : 2023-05-31 15:43:42,243 ] - Epoch 71/150 - time: 0.06 - training_loss: -8.5028\n",
      "[ INFO : 2023-05-31 15:43:42,305 ] - Epoch 72/150 - time: 0.06 - training_loss: -8.5078\n",
      "[ INFO : 2023-05-31 15:43:42,365 ] - Epoch 73/150 - time: 0.06 - training_loss: -8.5126\n",
      "[ INFO : 2023-05-31 15:43:42,426 ] - Epoch 74/150 - time: 0.06 - training_loss: -8.5173\n",
      "[ INFO : 2023-05-31 15:43:42,486 ] - Epoch 75/150 - time: 0.06 - training_loss: -8.5219\n",
      "[ INFO : 2023-05-31 15:43:42,549 ] - Epoch 76/150 - time: 0.06 - training_loss: -8.5264\n",
      "[ INFO : 2023-05-31 15:43:42,610 ] - Epoch 77/150 - time: 0.06 - training_loss: -8.5307\n",
      "[ INFO : 2023-05-31 15:43:42,672 ] - Epoch 78/150 - time: 0.06 - training_loss: -8.5349\n",
      "[ INFO : 2023-05-31 15:43:42,732 ] - Epoch 79/150 - time: 0.06 - training_loss: -8.5390\n",
      "[ INFO : 2023-05-31 15:43:42,791 ] - Epoch 80/150 - time: 0.06 - training_loss: -8.5431\n",
      "[ INFO : 2023-05-31 15:43:42,851 ] - Epoch 81/150 - time: 0.06 - training_loss: -8.5470\n",
      "[ INFO : 2023-05-31 15:43:42,910 ] - Epoch 82/150 - time: 0.06 - training_loss: -8.5508\n",
      "[ INFO : 2023-05-31 15:43:42,968 ] - Epoch 83/150 - time: 0.06 - training_loss: -8.5545\n",
      "[ INFO : 2023-05-31 15:43:43,028 ] - Epoch 84/150 - time: 0.06 - training_loss: -8.5582\n",
      "[ INFO : 2023-05-31 15:43:43,085 ] - Epoch 85/150 - time: 0.06 - training_loss: -8.5617\n",
      "[ INFO : 2023-05-31 15:43:43,144 ] - Epoch 86/150 - time: 0.06 - training_loss: -8.5652\n",
      "[ INFO : 2023-05-31 15:43:43,205 ] - Epoch 87/150 - time: 0.06 - training_loss: -8.5686\n",
      "[ INFO : 2023-05-31 15:43:43,267 ] - Epoch 88/150 - time: 0.06 - training_loss: -8.5719\n",
      "[ INFO : 2023-05-31 15:43:43,327 ] - Epoch 89/150 - time: 0.06 - training_loss: -8.5751\n",
      "[ INFO : 2023-05-31 15:43:43,390 ] - Epoch 90/150 - time: 0.06 - training_loss: -8.5783\n",
      "[ INFO : 2023-05-31 15:43:43,451 ] - Epoch 91/150 - time: 0.06 - training_loss: -8.5814\n",
      "[ INFO : 2023-05-31 15:43:43,510 ] - Epoch 92/150 - time: 0.06 - training_loss: -8.5844\n",
      "[ INFO : 2023-05-31 15:43:43,570 ] - Epoch 93/150 - time: 0.06 - training_loss: -8.5874\n",
      "[ INFO : 2023-05-31 15:43:43,632 ] - Epoch 94/150 - time: 0.06 - training_loss: -8.5903\n",
      "[ INFO : 2023-05-31 15:43:43,693 ] - Epoch 95/150 - time: 0.06 - training_loss: -8.5932\n",
      "[ INFO : 2023-05-31 15:43:43,754 ] - Epoch 96/150 - time: 0.06 - training_loss: -8.5959\n",
      "[ INFO : 2023-05-31 15:43:43,815 ] - Epoch 97/150 - time: 0.06 - training_loss: -8.5987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2023-05-31 15:43:43,874 ] - Epoch 98/150 - time: 0.06 - training_loss: -8.6013\n",
      "[ INFO : 2023-05-31 15:43:43,931 ] - Epoch 99/150 - time: 0.06 - training_loss: -8.6040\n",
      "[ INFO : 2023-05-31 15:43:43,990 ] - Epoch 100/150 - time: 0.06 - training_loss: -8.6065\n",
      "[ INFO : 2023-05-31 15:43:44,048 ] - Epoch 101/150 - time: 0.06 - training_loss: -8.6090\n",
      "[ INFO : 2023-05-31 15:43:44,108 ] - Epoch 102/150 - time: 0.06 - training_loss: -8.6115\n",
      "[ INFO : 2023-05-31 15:43:44,167 ] - Epoch 103/150 - time: 0.06 - training_loss: -8.6139\n",
      "[ INFO : 2023-05-31 15:43:44,227 ] - Epoch 104/150 - time: 0.06 - training_loss: -8.6163\n",
      "[ INFO : 2023-05-31 15:43:44,287 ] - Epoch 105/150 - time: 0.06 - training_loss: -8.6186\n",
      "[ INFO : 2023-05-31 15:43:44,349 ] - Epoch 106/150 - time: 0.06 - training_loss: -8.6209\n",
      "[ INFO : 2023-05-31 15:43:44,410 ] - Epoch 107/150 - time: 0.06 - training_loss: -8.6231\n",
      "[ INFO : 2023-05-31 15:43:44,472 ] - Epoch 108/150 - time: 0.06 - training_loss: -8.6253\n",
      "[ INFO : 2023-05-31 15:43:44,532 ] - Epoch 109/150 - time: 0.06 - training_loss: -8.6275\n",
      "[ INFO : 2023-05-31 15:43:44,592 ] - Epoch 110/150 - time: 0.06 - training_loss: -8.6296\n",
      "[ INFO : 2023-05-31 15:43:44,653 ] - Epoch 111/150 - time: 0.06 - training_loss: -8.6317\n",
      "[ INFO : 2023-05-31 15:43:44,715 ] - Epoch 112/150 - time: 0.06 - training_loss: -8.6337\n",
      "[ INFO : 2023-05-31 15:43:44,775 ] - Epoch 113/150 - time: 0.06 - training_loss: -8.6357\n",
      "[ INFO : 2023-05-31 15:43:44,837 ] - Epoch 114/150 - time: 0.06 - training_loss: -8.6377\n",
      "[ INFO : 2023-05-31 15:43:44,895 ] - Epoch 115/150 - time: 0.06 - training_loss: -8.6396\n",
      "[ INFO : 2023-05-31 15:43:44,953 ] - Epoch 116/150 - time: 0.06 - training_loss: -8.6415\n",
      "[ INFO : 2023-05-31 15:43:45,011 ] - Epoch 117/150 - time: 0.06 - training_loss: -8.6434\n",
      "[ INFO : 2023-05-31 15:43:45,071 ] - Epoch 118/150 - time: 0.06 - training_loss: -8.6452\n",
      "[ INFO : 2023-05-31 15:43:45,132 ] - Epoch 119/150 - time: 0.06 - training_loss: -8.6471\n",
      "[ INFO : 2023-05-31 15:43:45,195 ] - Epoch 120/150 - time: 0.06 - training_loss: -8.6488\n",
      "[ INFO : 2023-05-31 15:43:45,256 ] - Epoch 121/150 - time: 0.06 - training_loss: -8.6506\n",
      "[ INFO : 2023-05-31 15:43:45,315 ] - Epoch 122/150 - time: 0.06 - training_loss: -8.6523\n",
      "[ INFO : 2023-05-31 15:43:45,375 ] - Epoch 123/150 - time: 0.06 - training_loss: -8.6540\n",
      "[ INFO : 2023-05-31 15:43:45,437 ] - Epoch 124/150 - time: 0.06 - training_loss: -8.6557\n",
      "[ INFO : 2023-05-31 15:43:45,497 ] - Epoch 125/150 - time: 0.06 - training_loss: -8.6573\n",
      "[ INFO : 2023-05-31 15:43:45,559 ] - Epoch 126/150 - time: 0.06 - training_loss: -8.6589\n",
      "[ INFO : 2023-05-31 15:43:45,619 ] - Epoch 127/150 - time: 0.06 - training_loss: -8.6605\n",
      "[ INFO : 2023-05-31 15:43:45,679 ] - Epoch 128/150 - time: 0.06 - training_loss: -8.6621\n",
      "[ INFO : 2023-05-31 15:43:45,739 ] - Epoch 129/150 - time: 0.06 - training_loss: -8.6636\n",
      "[ INFO : 2023-05-31 15:43:45,801 ] - Epoch 130/150 - time: 0.06 - training_loss: -8.6651\n",
      "[ INFO : 2023-05-31 15:43:45,860 ] - Epoch 131/150 - time: 0.06 - training_loss: -8.6666\n",
      "[ INFO : 2023-05-31 15:43:45,920 ] - Epoch 132/150 - time: 0.06 - training_loss: -8.6681\n",
      "[ INFO : 2023-05-31 15:43:45,986 ] - Epoch 133/150 - time: 0.06 - training_loss: -8.6695\n",
      "[ INFO : 2023-05-31 15:43:46,043 ] - Epoch 134/150 - time: 0.06 - training_loss: -8.6709\n",
      "[ INFO : 2023-05-31 15:43:46,101 ] - Epoch 135/150 - time: 0.06 - training_loss: -8.6723\n",
      "[ INFO : 2023-05-31 15:43:46,161 ] - Epoch 136/150 - time: 0.06 - training_loss: -8.6737\n",
      "[ INFO : 2023-05-31 15:43:46,221 ] - Epoch 137/150 - time: 0.06 - training_loss: -8.6751\n",
      "[ INFO : 2023-05-31 15:43:46,283 ] - Epoch 138/150 - time: 0.06 - training_loss: -8.6764\n",
      "[ INFO : 2023-05-31 15:43:46,343 ] - Epoch 139/150 - time: 0.06 - training_loss: -8.6778\n",
      "[ INFO : 2023-05-31 15:43:46,403 ] - Epoch 140/150 - time: 0.06 - training_loss: -8.6791\n",
      "[ INFO : 2023-05-31 15:43:46,463 ] - Epoch 141/150 - time: 0.06 - training_loss: -8.6803\n",
      "[ INFO : 2023-05-31 15:43:46,525 ] - Epoch 142/150 - time: 0.06 - training_loss: -8.6816\n",
      "[ INFO : 2023-05-31 15:43:46,586 ] - Epoch 143/150 - time: 0.06 - training_loss: -8.6829\n",
      "[ INFO : 2023-05-31 15:43:46,647 ] - Epoch 144/150 - time: 0.06 - training_loss: -8.6841\n",
      "[ INFO : 2023-05-31 15:43:46,707 ] - Epoch 145/150 - time: 0.06 - training_loss: -8.6853\n",
      "[ INFO : 2023-05-31 15:43:46,767 ] - Epoch 146/150 - time: 0.06 - training_loss: -8.6865\n",
      "[ INFO : 2023-05-31 15:43:46,827 ] - Epoch 147/150 - time: 0.06 - training_loss: -8.6877\n",
      "[ INFO : 2023-05-31 15:43:46,887 ] - Epoch 148/150 - time: 0.06 - training_loss: -8.6889\n",
      "[ INFO : 2023-05-31 15:43:46,946 ] - Epoch 149/150 - time: 0.06 - training_loss: -8.6900\n",
      "[ INFO : 2023-05-31 15:43:47,004 ] - Epoch 150/150 - time: 0.06 - training_loss: -8.6911\n",
      "[ INFO : 2023-05-31 15:43:47,048 ] - DeepGCCA(\n",
      "  (model_list): ModuleList(\n",
      "    (0-2): 3 x MlpNet(\n",
      "      (layers): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Linear(in_features=20, out_features=256, bias=True)\n",
      "          (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=1, bias=True)\n",
      "          (1): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[ INFO : 2023-05-31 15:43:47,054 ] - Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.05\n",
      "    lr: 0.05\n",
      "    maximize: False\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "[ INFO : 2023-05-31 15:43:47,118 ] - Epoch 1/150 - time: 0.06 - training_loss: -3.7238\n",
      "[ INFO : 2023-05-31 15:43:47,178 ] - Epoch 2/150 - time: 0.06 - training_loss: -4.8248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear CCA started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2023-05-31 15:43:47,238 ] - Epoch 3/150 - time: 0.06 - training_loss: -5.8928\n",
      "[ INFO : 2023-05-31 15:43:47,298 ] - Epoch 4/150 - time: 0.06 - training_loss: -6.5685\n",
      "[ INFO : 2023-05-31 15:43:47,369 ] - Epoch 5/150 - time: 0.07 - training_loss: -6.9897\n",
      "[ INFO : 2023-05-31 15:43:47,429 ] - Epoch 6/150 - time: 0.06 - training_loss: -7.2830\n",
      "[ INFO : 2023-05-31 15:43:47,489 ] - Epoch 7/150 - time: 0.06 - training_loss: -7.4951\n",
      "[ INFO : 2023-05-31 15:43:47,551 ] - Epoch 8/150 - time: 0.06 - training_loss: -7.6528\n",
      "[ INFO : 2023-05-31 15:43:47,611 ] - Epoch 9/150 - time: 0.06 - training_loss: -7.7753\n",
      "[ INFO : 2023-05-31 15:43:47,670 ] - Epoch 10/150 - time: 0.06 - training_loss: -7.8750\n",
      "[ INFO : 2023-05-31 15:43:47,732 ] - Epoch 11/150 - time: 0.06 - training_loss: -7.9588\n",
      "[ INFO : 2023-05-31 15:43:47,792 ] - Epoch 12/150 - time: 0.06 - training_loss: -8.0300\n",
      "[ INFO : 2023-05-31 15:43:47,851 ] - Epoch 13/150 - time: 0.06 - training_loss: -8.0908\n",
      "[ INFO : 2023-05-31 15:43:47,910 ] - Epoch 14/150 - time: 0.06 - training_loss: -8.1431\n",
      "[ INFO : 2023-05-31 15:43:47,968 ] - Epoch 15/150 - time: 0.06 - training_loss: -8.1886\n",
      "[ INFO : 2023-05-31 15:43:48,025 ] - Epoch 16/150 - time: 0.06 - training_loss: -8.2285\n",
      "[ INFO : 2023-05-31 15:43:48,084 ] - Epoch 17/150 - time: 0.06 - training_loss: -8.2639\n",
      "[ INFO : 2023-05-31 15:43:48,142 ] - Epoch 18/150 - time: 0.06 - training_loss: -8.2955\n",
      "[ INFO : 2023-05-31 15:43:48,202 ] - Epoch 19/150 - time: 0.06 - training_loss: -8.3240\n",
      "[ INFO : 2023-05-31 15:43:48,264 ] - Epoch 20/150 - time: 0.06 - training_loss: -8.3500\n",
      "[ INFO : 2023-05-31 15:43:48,324 ] - Epoch 21/150 - time: 0.06 - training_loss: -8.3735\n",
      "[ INFO : 2023-05-31 15:43:48,386 ] - Epoch 22/150 - time: 0.06 - training_loss: -8.3951\n",
      "[ INFO : 2023-05-31 15:43:48,448 ] - Epoch 23/150 - time: 0.06 - training_loss: -8.4148\n",
      "[ INFO : 2023-05-31 15:43:48,508 ] - Epoch 24/150 - time: 0.06 - training_loss: -8.4329\n",
      "[ INFO : 2023-05-31 15:43:48,568 ] - Epoch 25/150 - time: 0.06 - training_loss: -8.4495\n",
      "[ INFO : 2023-05-31 15:43:48,630 ] - Epoch 26/150 - time: 0.06 - training_loss: -8.4649\n",
      "[ INFO : 2023-05-31 15:43:48,690 ] - Epoch 27/150 - time: 0.06 - training_loss: -8.4792\n",
      "[ INFO : 2023-05-31 15:43:48,750 ] - Epoch 28/150 - time: 0.06 - training_loss: -8.4925\n",
      "[ INFO : 2023-05-31 15:43:48,814 ] - Epoch 29/150 - time: 0.06 - training_loss: -8.5050\n",
      "[ INFO : 2023-05-31 15:43:48,874 ] - Epoch 30/150 - time: 0.06 - training_loss: -8.5166\n",
      "[ INFO : 2023-05-31 15:43:48,932 ] - Epoch 31/150 - time: 0.06 - training_loss: -8.5276\n",
      "[ INFO : 2023-05-31 15:43:48,991 ] - Epoch 32/150 - time: 0.06 - training_loss: -8.5378\n",
      "[ INFO : 2023-05-31 15:43:49,049 ] - Epoch 33/150 - time: 0.06 - training_loss: -8.5475\n",
      "[ INFO : 2023-05-31 15:43:49,106 ] - Epoch 34/150 - time: 0.06 - training_loss: -8.5566\n",
      "[ INFO : 2023-05-31 15:43:49,166 ] - Epoch 35/150 - time: 0.06 - training_loss: -8.5651\n",
      "[ INFO : 2023-05-31 15:43:49,226 ] - Epoch 36/150 - time: 0.06 - training_loss: -8.5732\n",
      "[ INFO : 2023-05-31 15:43:49,285 ] - Epoch 37/150 - time: 0.06 - training_loss: -8.5809\n",
      "[ INFO : 2023-05-31 15:43:49,347 ] - Epoch 38/150 - time: 0.06 - training_loss: -8.5882\n",
      "[ INFO : 2023-05-31 15:43:49,408 ] - Epoch 39/150 - time: 0.06 - training_loss: -8.5951\n",
      "[ INFO : 2023-05-31 15:43:49,467 ] - Epoch 40/150 - time: 0.06 - training_loss: -8.6017\n",
      "[ INFO : 2023-05-31 15:43:49,532 ] - Epoch 41/150 - time: 0.06 - training_loss: -8.6080\n",
      "[ INFO : 2023-05-31 15:43:49,604 ] - Epoch 42/150 - time: 0.07 - training_loss: -8.6139\n",
      "[ INFO : 2023-05-31 15:43:49,665 ] - Epoch 43/150 - time: 0.06 - training_loss: -8.6196\n",
      "[ INFO : 2023-05-31 15:43:49,728 ] - Epoch 44/150 - time: 0.06 - training_loss: -8.6251\n",
      "[ INFO : 2023-05-31 15:43:49,790 ] - Epoch 45/150 - time: 0.06 - training_loss: -8.6303\n",
      "[ INFO : 2023-05-31 15:43:49,851 ] - Epoch 46/150 - time: 0.06 - training_loss: -8.6352\n",
      "[ INFO : 2023-05-31 15:43:49,912 ] - Epoch 47/150 - time: 0.06 - training_loss: -8.6400\n",
      "[ INFO : 2023-05-31 15:43:49,969 ] - Epoch 48/150 - time: 0.06 - training_loss: -8.6446\n",
      "[ INFO : 2023-05-31 15:43:50,027 ] - Epoch 49/150 - time: 0.06 - training_loss: -8.6490\n",
      "[ INFO : 2023-05-31 15:43:50,085 ] - Epoch 50/150 - time: 0.06 - training_loss: -8.6532\n",
      "[ INFO : 2023-05-31 15:43:50,143 ] - Epoch 51/150 - time: 0.06 - training_loss: -8.6572\n",
      "[ INFO : 2023-05-31 15:43:50,202 ] - Epoch 52/150 - time: 0.06 - training_loss: -8.6611\n",
      "[ INFO : 2023-05-31 15:43:50,264 ] - Epoch 53/150 - time: 0.06 - training_loss: -8.6649\n",
      "[ INFO : 2023-05-31 15:43:50,323 ] - Epoch 54/150 - time: 0.06 - training_loss: -8.6685\n",
      "[ INFO : 2023-05-31 15:43:50,383 ] - Epoch 55/150 - time: 0.06 - training_loss: -8.6720\n",
      "[ INFO : 2023-05-31 15:43:50,446 ] - Epoch 56/150 - time: 0.06 - training_loss: -8.6753\n",
      "[ INFO : 2023-05-31 15:43:50,507 ] - Epoch 57/150 - time: 0.06 - training_loss: -8.6786\n",
      "[ INFO : 2023-05-31 15:43:50,570 ] - Epoch 58/150 - time: 0.06 - training_loss: -8.6817\n",
      "[ INFO : 2023-05-31 15:43:50,633 ] - Epoch 59/150 - time: 0.06 - training_loss: -8.6847\n",
      "[ INFO : 2023-05-31 15:43:50,694 ] - Epoch 60/150 - time: 0.06 - training_loss: -8.6876\n",
      "[ INFO : 2023-05-31 15:43:50,753 ] - Epoch 61/150 - time: 0.06 - training_loss: -8.6905\n",
      "[ INFO : 2023-05-31 15:43:50,815 ] - Epoch 62/150 - time: 0.06 - training_loss: -8.6932\n",
      "[ INFO : 2023-05-31 15:43:50,873 ] - Epoch 63/150 - time: 0.06 - training_loss: -8.6959\n",
      "[ INFO : 2023-05-31 15:43:50,931 ] - Epoch 64/150 - time: 0.06 - training_loss: -8.6984\n",
      "[ INFO : 2023-05-31 15:43:50,991 ] - Epoch 65/150 - time: 0.06 - training_loss: -8.7009\n",
      "[ INFO : 2023-05-31 15:43:51,049 ] - Epoch 66/150 - time: 0.06 - training_loss: -8.7033\n",
      "[ INFO : 2023-05-31 15:43:51,106 ] - Epoch 67/150 - time: 0.06 - training_loss: -8.7057\n",
      "[ INFO : 2023-05-31 15:43:51,166 ] - Epoch 68/150 - time: 0.06 - training_loss: -8.7079\n",
      "[ INFO : 2023-05-31 15:43:51,225 ] - Epoch 69/150 - time: 0.06 - training_loss: -8.7102\n",
      "[ INFO : 2023-05-31 15:43:51,286 ] - Epoch 70/150 - time: 0.06 - training_loss: -8.7123\n",
      "[ INFO : 2023-05-31 15:43:51,348 ] - Epoch 71/150 - time: 0.06 - training_loss: -8.7144\n",
      "[ INFO : 2023-05-31 15:43:51,407 ] - Epoch 72/150 - time: 0.06 - training_loss: -8.7164\n",
      "[ INFO : 2023-05-31 15:43:51,467 ] - Epoch 73/150 - time: 0.06 - training_loss: -8.7184\n",
      "[ INFO : 2023-05-31 15:43:51,529 ] - Epoch 74/150 - time: 0.06 - training_loss: -8.7203\n",
      "[ INFO : 2023-05-31 15:43:51,589 ] - Epoch 75/150 - time: 0.06 - training_loss: -8.7222\n",
      "[ INFO : 2023-05-31 15:43:51,648 ] - Epoch 76/150 - time: 0.06 - training_loss: -8.7240\n",
      "[ INFO : 2023-05-31 15:43:51,710 ] - Epoch 77/150 - time: 0.06 - training_loss: -8.7258\n",
      "[ INFO : 2023-05-31 15:43:51,770 ] - Epoch 78/150 - time: 0.06 - training_loss: -8.7275\n",
      "[ INFO : 2023-05-31 15:43:51,829 ] - Epoch 79/150 - time: 0.06 - training_loss: -8.7292\n",
      "[ INFO : 2023-05-31 15:43:51,889 ] - Epoch 80/150 - time: 0.06 - training_loss: -8.7308\n",
      "[ INFO : 2023-05-31 15:43:51,946 ] - Epoch 81/150 - time: 0.06 - training_loss: -8.7324\n",
      "[ INFO : 2023-05-31 15:43:52,005 ] - Epoch 82/150 - time: 0.06 - training_loss: -8.7340\n",
      "[ INFO : 2023-05-31 15:43:52,064 ] - Epoch 83/150 - time: 0.06 - training_loss: -8.7355\n",
      "[ INFO : 2023-05-31 15:43:52,122 ] - Epoch 84/150 - time: 0.06 - training_loss: -8.7370\n",
      "[ INFO : 2023-05-31 15:43:52,181 ] - Epoch 85/150 - time: 0.06 - training_loss: -8.7384\n",
      "[ INFO : 2023-05-31 15:43:52,243 ] - Epoch 86/150 - time: 0.06 - training_loss: -8.7398\n",
      "[ INFO : 2023-05-31 15:43:52,302 ] - Epoch 87/150 - time: 0.06 - training_loss: -8.7412\n",
      "[ INFO : 2023-05-31 15:43:52,362 ] - Epoch 88/150 - time: 0.06 - training_loss: -8.7426\n",
      "[ INFO : 2023-05-31 15:43:52,424 ] - Epoch 89/150 - time: 0.06 - training_loss: -8.7439\n",
      "[ INFO : 2023-05-31 15:43:52,483 ] - Epoch 90/150 - time: 0.06 - training_loss: -8.7452\n",
      "[ INFO : 2023-05-31 15:43:52,544 ] - Epoch 91/150 - time: 0.06 - training_loss: -8.7465\n",
      "[ INFO : 2023-05-31 15:43:52,621 ] - Epoch 92/150 - time: 0.08 - training_loss: -8.7477\n",
      "[ INFO : 2023-05-31 15:43:52,680 ] - Epoch 93/150 - time: 0.06 - training_loss: -8.7489\n",
      "[ INFO : 2023-05-31 15:43:52,740 ] - Epoch 94/150 - time: 0.06 - training_loss: -8.7501\n",
      "[ INFO : 2023-05-31 15:43:52,802 ] - Epoch 95/150 - time: 0.06 - training_loss: -8.7513\n",
      "[ INFO : 2023-05-31 15:43:52,861 ] - Epoch 96/150 - time: 0.06 - training_loss: -8.7524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ INFO : 2023-05-31 15:43:52,919 ] - Epoch 97/150 - time: 0.06 - training_loss: -8.7535\n",
      "[ INFO : 2023-05-31 15:43:52,978 ] - Epoch 98/150 - time: 0.06 - training_loss: -8.7546\n",
      "[ INFO : 2023-05-31 15:43:53,036 ] - Epoch 99/150 - time: 0.06 - training_loss: -8.7557\n",
      "[ INFO : 2023-05-31 15:43:53,095 ] - Epoch 100/150 - time: 0.06 - training_loss: -8.7567\n",
      "[ INFO : 2023-05-31 15:43:53,155 ] - Epoch 101/150 - time: 0.06 - training_loss: -8.7578\n",
      "[ INFO : 2023-05-31 15:43:53,214 ] - Epoch 102/150 - time: 0.06 - training_loss: -8.7588\n",
      "[ INFO : 2023-05-31 15:43:53,273 ] - Epoch 103/150 - time: 0.06 - training_loss: -8.7597\n",
      "[ INFO : 2023-05-31 15:43:53,336 ] - Epoch 104/150 - time: 0.06 - training_loss: -8.7607\n",
      "[ INFO : 2023-05-31 15:43:53,395 ] - Epoch 105/150 - time: 0.06 - training_loss: -8.7617\n",
      "[ INFO : 2023-05-31 15:43:53,455 ] - Epoch 106/150 - time: 0.06 - training_loss: -8.7626\n",
      "[ INFO : 2023-05-31 15:43:53,517 ] - Epoch 107/150 - time: 0.06 - training_loss: -8.7635\n",
      "[ INFO : 2023-05-31 15:43:53,578 ] - Epoch 108/150 - time: 0.06 - training_loss: -8.7644\n",
      "[ INFO : 2023-05-31 15:43:53,639 ] - Epoch 109/150 - time: 0.06 - training_loss: -8.7653\n",
      "[ INFO : 2023-05-31 15:43:53,702 ] - Epoch 110/150 - time: 0.06 - training_loss: -8.7662\n",
      "[ INFO : 2023-05-31 15:43:53,762 ] - Epoch 111/150 - time: 0.06 - training_loss: -8.7670\n",
      "[ INFO : 2023-05-31 15:43:53,822 ] - Epoch 112/150 - time: 0.06 - training_loss: -8.7678\n",
      "[ INFO : 2023-05-31 15:43:53,882 ] - Epoch 113/150 - time: 0.06 - training_loss: -8.7687\n",
      "[ INFO : 2023-05-31 15:43:53,940 ] - Epoch 114/150 - time: 0.06 - training_loss: -8.7695\n",
      "[ INFO : 2023-05-31 15:43:53,998 ] - Epoch 115/150 - time: 0.06 - training_loss: -8.7702\n",
      "[ INFO : 2023-05-31 15:43:54,058 ] - Epoch 116/150 - time: 0.06 - training_loss: -8.7710\n",
      "[ INFO : 2023-05-31 15:43:54,115 ] - Epoch 117/150 - time: 0.06 - training_loss: -8.7718\n",
      "[ INFO : 2023-05-31 15:43:54,174 ] - Epoch 118/150 - time: 0.06 - training_loss: -8.7725\n",
      "[ INFO : 2023-05-31 15:43:54,236 ] - Epoch 119/150 - time: 0.06 - training_loss: -8.7733\n",
      "[ INFO : 2023-05-31 15:43:54,295 ] - Epoch 120/150 - time: 0.06 - training_loss: -8.7740\n",
      "[ INFO : 2023-05-31 15:43:54,355 ] - Epoch 121/150 - time: 0.06 - training_loss: -8.7747\n",
      "[ INFO : 2023-05-31 15:43:54,417 ] - Epoch 122/150 - time: 0.06 - training_loss: -8.7754\n",
      "[ INFO : 2023-05-31 15:43:54,477 ] - Epoch 123/150 - time: 0.06 - training_loss: -8.7761\n",
      "[ INFO : 2023-05-31 15:43:54,537 ] - Epoch 124/150 - time: 0.06 - training_loss: -8.7768\n",
      "[ INFO : 2023-05-31 15:43:54,599 ] - Epoch 125/150 - time: 0.06 - training_loss: -8.7775\n",
      "[ INFO : 2023-05-31 15:43:54,659 ] - Epoch 126/150 - time: 0.06 - training_loss: -8.7781\n",
      "[ INFO : 2023-05-31 15:43:54,724 ] - Epoch 127/150 - time: 0.06 - training_loss: -8.7788\n",
      "[ INFO : 2023-05-31 15:43:54,786 ] - Epoch 128/150 - time: 0.06 - training_loss: -8.7794\n",
      "[ INFO : 2023-05-31 15:43:54,845 ] - Epoch 129/150 - time: 0.06 - training_loss: -8.7800\n",
      "[ INFO : 2023-05-31 15:43:54,904 ] - Epoch 130/150 - time: 0.06 - training_loss: -8.7807\n",
      "[ INFO : 2023-05-31 15:43:54,963 ] - Epoch 131/150 - time: 0.06 - training_loss: -8.7813\n",
      "[ INFO : 2023-05-31 15:43:55,021 ] - Epoch 132/150 - time: 0.06 - training_loss: -8.7819\n",
      "[ INFO : 2023-05-31 15:43:55,079 ] - Epoch 133/150 - time: 0.06 - training_loss: -8.7825\n",
      "[ INFO : 2023-05-31 15:43:55,139 ] - Epoch 134/150 - time: 0.06 - training_loss: -8.7830\n",
      "[ INFO : 2023-05-31 15:43:55,198 ] - Epoch 135/150 - time: 0.06 - training_loss: -8.7836\n",
      "[ INFO : 2023-05-31 15:43:55,258 ] - Epoch 136/150 - time: 0.06 - training_loss: -8.7842\n",
      "[ INFO : 2023-05-31 15:43:55,320 ] - Epoch 137/150 - time: 0.06 - training_loss: -8.7847\n",
      "[ INFO : 2023-05-31 15:43:55,381 ] - Epoch 138/150 - time: 0.06 - training_loss: -8.7853\n",
      "[ INFO : 2023-05-31 15:43:55,440 ] - Epoch 139/150 - time: 0.06 - training_loss: -8.7858\n",
      "[ INFO : 2023-05-31 15:43:55,502 ] - Epoch 140/150 - time: 0.06 - training_loss: -8.7863\n",
      "[ INFO : 2023-05-31 15:43:55,562 ] - Epoch 141/150 - time: 0.06 - training_loss: -8.7869\n",
      "[ INFO : 2023-05-31 15:43:55,622 ] - Epoch 142/150 - time: 0.06 - training_loss: -8.7874\n",
      "[ INFO : 2023-05-31 15:43:55,684 ] - Epoch 143/150 - time: 0.06 - training_loss: -8.7879\n",
      "[ INFO : 2023-05-31 15:43:55,745 ] - Epoch 144/150 - time: 0.06 - training_loss: -8.7884\n",
      "[ INFO : 2023-05-31 15:43:55,804 ] - Epoch 145/150 - time: 0.06 - training_loss: -8.7889\n",
      "[ INFO : 2023-05-31 15:43:55,865 ] - Epoch 146/150 - time: 0.06 - training_loss: -8.7894\n",
      "[ INFO : 2023-05-31 15:43:55,923 ] - Epoch 147/150 - time: 0.06 - training_loss: -8.7899\n",
      "[ INFO : 2023-05-31 15:43:55,981 ] - Epoch 148/150 - time: 0.06 - training_loss: -8.7904\n",
      "[ INFO : 2023-05-31 15:43:56,040 ] - Epoch 149/150 - time: 0.06 - training_loss: -8.7908\n",
      "[ INFO : 2023-05-31 15:43:56,098 ] - Epoch 150/150 - time: 0.06 - training_loss: -8.7913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear CCA started!\n"
     ]
    }
   ],
   "source": [
    "N = 400\n",
    "views = create_synthData_new(N,mode=1,F=20)\n",
    "print(f'input views shape :')\n",
    "for i, view in enumerate(views):\n",
    "    print(f'view_{i} :  {view.shape}')\n",
    "    view = view.to(device)\n",
    "\n",
    "U_sum = []\n",
    "outputs_sum = []\n",
    "test = []\n",
    "for i in range(3):\n",
    "    testm = torch.eye(20)\n",
    "    test.append(testm)\n",
    "    \n",
    "import pandas as pd\n",
    "for _ in range(10):\n",
    "\n",
    "    # size of the input for view 1 and view 2\n",
    "    input_shape_list = [view.shape[-1] for view in views]\n",
    "\n",
    "    # Building, training, and producing the new features by DCCA\n",
    "    model = DeepGCCA(layer_sizes_list, input_shape_list, outdim_size,\n",
    "                             use_all_singular_values, device=device).double()\n",
    "    l_gcca = None\n",
    "    if apply_linear_gcca:\n",
    "        l_gcca = linear_gcca\n",
    "    solver = Solver(model, l_gcca, outdim_size, epoch_num, batch_size,\n",
    "                    learning_rate, reg_par, device=device)\n",
    "    # train1, train2 = data1[0][0], data2[0][0]\n",
    "    # val1, val2 = data1[1][0], data2[1][0]\n",
    "    # test1, test2 = data1[2][0], data2[2][0]\n",
    "\n",
    "    solver.fit(views, checkpoint=save_name)\n",
    "\n",
    "\n",
    "    # TODO: Save l_gcca model if needed\n",
    "    _ , _, outputs_def = solver.test(test, apply_linear_gcca)\n",
    "\n",
    "    A = outputs_def[0]\n",
    "    B = outputs_def[1]\n",
    "    C = outputs_def[2]\n",
    "    U = [A,B,C]\n",
    "\n",
    "    A = outputs_def[0]\n",
    "    B = outputs_def[1]\n",
    "    C = outputs_def[2]\n",
    "\n",
    "    os = [A,B,C]\n",
    "    U_sum.append(U)\n",
    "    outputs_sum.append(os)\n",
    "    \n",
    "\n",
    "#variables = pd.DataFrame(U_sum)\n",
    "#results = pd.DataFrame(results_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5aec7b8-21f7-4e9c-a28f-616bb5aa5a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0 1.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "from validation_method import FS_MCC\n",
    "\n",
    "Label = torch.cat([torch.ones(2, dtype=torch.bool), torch.zeros(18, dtype=torch.bool)])\n",
    "acc,f1,mcc = FS_MCC(U_sum[i],Label)\n",
    "\n",
    "ACC = []\n",
    "F1 = []\n",
    "MCC = []\n",
    "Label = torch.cat([torch.ones(2, dtype=torch.bool), torch.zeros(18, dtype=torch.bool)])\n",
    "for i in range(10):\n",
    "    acc,f1,mcc = FS_MCC(U_sum[i],Label)\n",
    "    ACC.append(acc)\n",
    "    F1.append(f1)\n",
    "    MCC.append(mcc)\n",
    "\n",
    "print(np.mean(ACC),np.std(ACC))\n",
    "print(np.mean(F1),np.std(F1))\n",
    "print(np.mean(MCC),np.std(MCC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee2f0ee-bae3-4e85-9448-fad68474aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, loss, outputs = solver.test(views, apply_linear_gcca)\n",
    "for i in range(len(outputs)):\n",
    "    outputs[i] = outputs[i]\n",
    "for i in range(3):\n",
    "    for j in range(i+1,3):\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.scatter(outputs[i],outputs[j])\n",
    "        plt.title('X'+str(i+1)+' VS X'+str(j+1))\n",
    "        plt.show()\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax3d = fig.add_subplot(projection='3d')\n",
    "ax3d.scatter(outputs[0],outputs[1],outputs[2]) \n",
    "plt.title('X1 VS X2 VS X3')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92071289-15f7-47d5-991a-9c44f7255466",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 400\n",
    "views = create_synthData_new(N,mode=2,F=20)\n",
    "print(f'input views shape :')\n",
    "for i, view in enumerate(views):\n",
    "    print(f'view_{i} :  {view.shape}')\n",
    "    view = view.to(device)\n",
    "\n",
    "U_sum = []\n",
    "outputs_sum = []\n",
    "test = []\n",
    "for i in range(3):\n",
    "    testm = torch.eye(20)\n",
    "    test.append(testm)\n",
    "    \n",
    "import pandas as pd\n",
    "for _ in range(10):\n",
    "\n",
    "    # size of the input for view 1 and view 2\n",
    "    input_shape_list = [view.shape[-1] for view in views]\n",
    "\n",
    "    # Building, training, and producing the new features by DCCA\n",
    "    model = DeepGCCA(layer_sizes_list, input_shape_list, outdim_size,\n",
    "                             use_all_singular_values, device=device).double()\n",
    "    l_gcca = None\n",
    "    if apply_linear_gcca:\n",
    "        l_gcca = linear_gcca\n",
    "    solver = Solver(model, l_gcca, outdim_size, epoch_num, batch_size,\n",
    "                    learning_rate, reg_par, device=device)\n",
    "    # train1, train2 = data1[0][0], data2[0][0]\n",
    "    # val1, val2 = data1[1][0], data2[1][0]\n",
    "    # test1, test2 = data1[2][0], data2[2][0]\n",
    "\n",
    "    solver.fit(views, checkpoint=save_name)\n",
    "\n",
    "\n",
    "    # TODO: Save l_gcca model if needed\n",
    "    _ , _, outputs_def = solver.test(test, apply_linear_gcca)\n",
    "\n",
    "    A = outputs_def[0]\n",
    "    B = outputs_def[1]\n",
    "    C = outputs_def[2]\n",
    "    U = [A,B,C]\n",
    "\n",
    "    A = outputs_def[0]\n",
    "    B = outputs_def[1]\n",
    "    C = outputs_def[2]\n",
    "\n",
    "    os = [A,B,C]\n",
    "    U_sum.append(U)\n",
    "    outputs_sum.append(os)\n",
    "    \n",
    "\n",
    "#variables = pd.DataFrame(U_sum)\n",
    "#results = pd.DataFrame(results_sum)\n",
    "\n",
    "f1 = np.zeros(10)\n",
    "mcc = np.zeros(10)\n",
    "Label = torch.cat([torch.ones(2, dtype=torch.bool), torch.zeros(18, dtype=torch.bool)])\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    f1[i],mcc[i] = FS_MCC(U_sum[i][0],U_sum[i][1],U_sum[i][2],Label)\n",
    "\n",
    "print(np.mean(f1),np.std(f1))\n",
    "print(np.mean(mcc),np.std(mcc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aa7e8d-03b2-459f-b8ca-db29214e249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, loss, outputs = solver.test(views, apply_linear_gcca)\n",
    "for i in range(len(outputs)):\n",
    "    outputs[i] = outputs[i].cpu()\n",
    "for i in range(3):\n",
    "    for j in range(i+1,3):\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.scatter(outputs[i],outputs[j])\n",
    "        plt.title('X'+str(i+1)+' VS X'+str(j+1))\n",
    "        plt.show()\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax3d = fig.add_subplot(projection='3d')\n",
    "ax3d.scatter(outputs[0],outputs[1],outputs[2]) \n",
    "plt.title('X1 VS X2 VS X3')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bbecbd-48b8-49fe-9525-69cadd069218",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 400\n",
    "views = create_synthData_new(N,mode=3,F=20)\n",
    "print(f'input views shape :')\n",
    "for i, view in enumerate(views):\n",
    "    print(f'view_{i} :  {view.shape}')\n",
    "    view = view.to(device)\n",
    "\n",
    "U_sum = []\n",
    "outputs_sum = []\n",
    "test = []\n",
    "for i in range(3):\n",
    "    testm = torch.eye(20)\n",
    "    test.append(testm)\n",
    "    \n",
    "import pandas as pd\n",
    "for _ in range(10):\n",
    "\n",
    "    # size of the input for view 1 and view 2\n",
    "    input_shape_list = [view.shape[-1] for view in views]\n",
    "\n",
    "    # Building, training, and producing the new features by DCCA\n",
    "    model = DeepGCCA(layer_sizes_list, input_shape_list, outdim_size,\n",
    "                             use_all_singular_values, device=device).double()\n",
    "    l_gcca = None\n",
    "    if apply_linear_gcca:\n",
    "        l_gcca = linear_gcca\n",
    "    solver = Solver(model, l_gcca, outdim_size, epoch_num, batch_size,\n",
    "                    learning_rate, reg_par, device=device)\n",
    "    # train1, train2 = data1[0][0], data2[0][0]\n",
    "    # val1, val2 = data1[1][0], data2[1][0]\n",
    "    # test1, test2 = data1[2][0], data2[2][0]\n",
    "\n",
    "    solver.fit(views, checkpoint=save_name)\n",
    "\n",
    "\n",
    "    # TODO: Save l_gcca model if needed\n",
    "    _ , _, outputs_def = solver.test(test, apply_linear_gcca)\n",
    "\n",
    "    A = outputs_def[0]\n",
    "    B = outputs_def[1]\n",
    "    C = outputs_def[2]\n",
    "    U = [A,B,C]\n",
    "\n",
    "    A = outputs_def[0]\n",
    "    B = outputs_def[1]\n",
    "    C = outputs_def[2]\n",
    "\n",
    "    os = [A,B,C]\n",
    "    U_sum.append(U)\n",
    "    outputs_sum.append(os)\n",
    "    \n",
    "\n",
    "#variables = pd.DataFrame(U_sum)\n",
    "#results = pd.DataFrame(results_sum)\n",
    "\n",
    "f1 = np.zeros(10)\n",
    "mcc = np.zeros(10)\n",
    "Label = torch.cat([torch.ones(2, dtype=torch.bool), torch.zeros(18, dtype=torch.bool)])\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    f1[i],mcc[i] = FS_MCC(U_sum[i][0],U_sum[i][1],U_sum[i][2],Label)\n",
    "\n",
    "print(np.mean(f1),np.std(f1))\n",
    "print(np.mean(mcc),np.std(mcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c6f8a-b484-4c33-89fe-28d242cc6b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, loss, outputs = solver.test(views, apply_linear_gcca)\n",
    "for i in range(len(outputs)):\n",
    "    outputs[i] = outputs[i].cpu()\n",
    "for i in range(3):\n",
    "    for j in range(i+1,3):\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.scatter(outputs[i],outputs[j])\n",
    "        plt.title('X'+str(i+1)+' VS X'+str(j+1))\n",
    "        plt.show()\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax3d = fig.add_subplot(projection='3d')\n",
    "ax3d.scatter(outputs[0],outputs[1],outputs[2]) \n",
    "plt.title('X1 VS X2 VS X3')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "78fafd15",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cca'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mGCCA\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gcca\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\myfile\\GCCA\\gcca\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m __author__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrupy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcca\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CCA\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgcca\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GCCA\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbridged_cca\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BridgedCCA\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cca'"
     ]
    }
   ],
   "source": [
    "from GCCA import gcca\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# set log level\n",
    "logging.root.setLevel(level=logging.INFO)\n",
    "\n",
    "# create data in advance\n",
    "a = np.random.rand(50, 50)\n",
    "b = np.random.rand(50, 60)\n",
    "c = np.random.rand(50, 70)\n",
    "d = np.random.rand(50, 80)\n",
    "e = np.random.rand(50, 90)\n",
    "f = np.random.rand(50, 100)\n",
    "g = np.random.rand(50, 110)\n",
    "h = np.random.rand(50, 120)\n",
    "i = np.random.rand(50, 130)\n",
    "j = np.random.rand(50, 140)\n",
    "k = np.random.rand(50, 150)\n",
    "\n",
    "# create instance of GCCA\n",
    "gcca = GCCA()\n",
    "# calculate GCCA\n",
    "gcca.fit(a, b, c, d, e, f, g, h, i, j, k)\n",
    "# transform\n",
    "gcca.transform(a, b, c, d, e, f, g, h, i, j, k)\n",
    "# save\n",
    "gcca.save_params(\"save/gcca.h5\")\n",
    "# load\n",
    "gcca.load_params(\"save/gcca.h5\")\n",
    "# plot\n",
    "gcca.plot_gcca_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74799772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
